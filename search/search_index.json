{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"citations/","title":"How to cite PEPkit","text":"<p>Thanks for citing us! If you use the PEP tools, or concepts in your research, please cite us!</p> If you use... Please cite ... PEP specification Sheffield et al. (2021) GigaScience <code>eido</code> Sheffield et al. (2021) GigaScience <code>geofetch</code> Sheffield et al. (2021) GigaScience; Khoroshevskyi et al. (2023) <code>looper</code> Sheffield et al. (2021) GigaScience <code>PEPhub</code> Sheffield et al. (2021) GigaScience; LeRoy et al. (2024) GigaScience <code>peppy</code> Sheffield et al. (2021) GigaScience <code>pypiper</code> Sheffield et al. (2021) GigaScience <code>pipestat</code> Sheffield et al. (2021) GigaScience"},{"location":"citations/#full-citation-information","title":"Full citation information","text":"<p>Sheffield NC, Stolarczyk M, Reuter VP, and Rendeiro A. Linking big biomedical datasets to modular analysis with Portable Encapsulated Projects GigaScience (2021) DOI: 10.1093/gigascience/giab077</p> <p>O Khoroshevskyi, N LeRoy, VP Reuter, NC Sheffield. GEOfetch: a command-line tool for downloading data and standardized metadata from GEO and SRA Bioinformatics 39 (3), btad069 (2023) DOI: 10.1093/bioinformatics/btad069</p> <p>NJ LeRoy, O Khoroshevskyi, A O\u2019Brien, R Stepie\u0144, A Arslan, NC Sheffield. PEPhub: a database, web interface, and API for editing, sharing, and validating biological sample metadata GigaScience (2024) DOI: 10.1093/gigascience/giae033</p>"},{"location":"statistics/","title":"PEPkit usage statistics","text":"<p>This page documents usage of PEPkit-related tools:</p> <ol> <li>statistics of downloads of packages from PyPI</li> <li>other software packages that use PEPkit software</li> <li>datasets organized in PEP-compatible formats</li> <li>publications that reference PEP manuscripts</li> </ol>"},{"location":"statistics/#pypi-download-history","title":"PyPI download history","text":""},{"location":"statistics/#software-using-pepkit","title":"Software using PEPkit","text":"<p>Publicly available software that builds on PEP:</p> <ul> <li>PEPATAC - An ATAC-seq pipeline. </li> <li>PEPPRO - An nascent RNA profiling pipeline (PRO-seq, GRO-seq, ChRO-seq).</li> <li>peppy</li> <li>pepr</li> <li>geofetch - Converts GEO or SRA accessions into PEP projects.</li> <li>divcfg</li> <li>pifaces</li> <li>pypiper</li> <li>dnameth_pipelines</li> <li>projectInit</li> <li>ngstoolkit - NGS analysis toolkit</li> <li>BiocProject</li> </ul>"},{"location":"statistics/#demo-data-using-pepkit","title":"Demo data using PEPkit","text":"<ul> <li>example_peps repository - A collection of example PEPs demonstrating various features.</li> <li>microtest</li> <li>hello looper! example</li> </ul>"},{"location":"statistics/#real-datasets-organized-in-pep-format","title":"Real datasets organized in PEP format:","text":"<ul> <li>https://github.com/epigen/crop-seq</li> <li>https://github.com/epigen/baf_complex</li> <li>https://github.com/epigen/mthfd1</li> <li>https://github.com/epigen/cll-ibrutinib_time</li> <li>https://github.com/epigen/cll-ibrutinib</li> <li>https://github.com/epigen/cll-chromatin</li> </ul>"},{"location":"statistics/#publications-that-use-pepkit","title":"Publications that use PEPkit:","text":"<ul> <li>Nash et al. (2023). Maternal diet alters long-term innate immune cell memory in fetal and juvenile hematopoietic stem and progenitor cells in nonhuman primate offspring Cell Reports.  DOI: 10.1016/j.celrep.2023.112393</li> <li>Danko et al. (2023). Evolution of promoter-proximal pausing enabled a new layer of transcription control DOI: 10.21203/rs.3.rs-2679520/v1</li> <li>Singh et al. (2023). Cohesin regulates alternative splicing Science Advances.  DOI: 10.1126/sciadv.ade3876</li> <li>Luo et al. (2023). Epiblast-like stem cells established by Wnt/-catenin signaling manifest distinct features of formative pluripotency and germline competence Cell Reports.  DOI: 10.1016/j.celrep.2023.112021</li> <li>Abadie et al. (2022). Flexible and scalable control of T cell memory by a reversible epigenetic switch DOI: 10.1101/2022.12.31.521782</li> <li>Wolpe et al. (2022). Correction of transposase sequence bias in ATAC-seq data with rule ensemble modeling DOI: 10.1101/2022.12.08.519600</li> <li>Robbe et al. (2022). Whole-genome sequencing of chronic lymphocytic leukemia identifies subgroups with distinct biological and clinical features Nature Genetics.  DOI: 10.1038/s41588-022-01211-y</li> <li>Robey et al. (2022). The methyltransferases METTL7A and METTL7B confer resistance to thiol-based histone deacetylase inhibitors DOI: 10.1101/2022.10.07.511310</li> <li>Callahan et al. (2022). High enhancer activity is an epigenetic feature of HPV negative atypical head and neck squamous cell carcinoma Frontiers in Cell and Developmental Biology.  DOI: 10.3389/fcell.2022.936168</li> <li>Duvall et al. (2022). Single-cell transcriptome and accessible chromatin dynamics during endocrine pancreas development Proceedings of the National Academy of Sciences.  DOI: 10.1073/pnas.2201267119</li> <li>Taklifi et al. (2022). Integrating chromatin accessibility states in the design of targeted sequencing panels for liquid biopsy Scientific Reports.  DOI: 10.1038/s41598-022-14675-z</li> <li>Grandi et al. (2022). Chromatin accessibility profiling by ATAC-seq Nature Protocols.  DOI: 10.1038/s41596-022-00692-9</li> <li>Hunter et al. (2022). HNF4A modulates glucocorticoid action in the liver Cell Reports.  DOI: 10.1016/j.celrep.2022.110697</li> <li>O'Connor et al. (2022). BET Protein Inhibition Regulates Macrophage Chromatin Accessibility and Microbiota-Dependent Colitis Frontiers in Immunology.  DOI: 10.3389/fimmu.2022.856966</li> <li>Wang et al. (2022). Prediction of histone post-translational modification patterns based on nascent transcription data Nature Genetics.  DOI: 10.1038/s41588-022-01026-x</li> <li>Zhang et al. (2022). Extensive evaluation of ATAC-seq protocols for native or formaldehyde-fixed nuclei BMC Genomics.  DOI: 10.1186/s12864-021-08266-x</li> <li>Shahin et al. (2021). Germline biallelic mutation affecting the transcription factor Helios causes pleiotropic defects of immunity Science Immunology.  DOI: 10.1126/sciimmunol.abe3981</li> <li>Ram-Mohan et al. (2021). Profiling chromatin accessibility responses in human neutrophils with sensitive pathogen detection Life Science Alliance.  DOI: 10.26508/lsa.202000976</li> <li>Robertson et al. (2021). Fine-mapping, trans-ancestral and genomic analyses identify causal variants, cells, genes and drug targets for type 1 diabetes Nature Genetics.  DOI: 10.1038/s41588-021-00880-5</li> <li>Cheung et al. (2021). Repression of CTSG, ELANE and PRTN3-mediated histone H3 proteolytic cleavage promotes monocyte-to-macrophage differentiation DOI: 10.1038/s41590-021-00928-y</li> <li>Hasegawa et al. (2021). Clonal inactivation of telomerase promotes accelerated stem cell differentiation bioRxiv.  DOI: 10.1101/2021.04.28.441728</li> <li>Weber et al. (2021). Transient rest restores functionality in exhausted CAR-T cells through epigenetic remodeling Science.  DOI: 10.1126/science.aba1786</li> <li>Gharavi et al. (2021). Embeddings of genomic region sets capture rich biological associations in low dimensions Bioinformatics.  DOI: 10.1093/bioinformatics/btab439</li> <li>Tovar et al. (2021). Integrative phenotypic and genomic analyses reveal strain-dependent responses to acute ozone exposure and their associations with airway macrophage transcriptional activity bioRxiv.  DOI: 10.1101/2021.01.29.428733</li> <li>Granja et al. (2021). ArchR is a scalable software package for integrative single-cell chromatin accessibility analysis Nature Genetics.  DOI: 10.1038/s41588-021-00790-6</li> <li>Gu et al. (2021). Bedshift: perturbation of genomic interval sets Genome Biology.  DOI: 10.1186/s13059-021-02440-w</li> <li>M\u00f6lder et al. (2021). Sustainable data analysis with Snakemake F1000Research.  DOI: 10.12688/f1000research.29032.2</li> <li>Smith et al. (2021). PEPPRO: quality control and processing of nascent RNA profiling data Genome Biology.  DOI: 10.1186/s13059-021-02349-4</li> <li>Fan et al. (2020). Epigenomic Reprogramming toward Mesenchymal-Epithelial Transition in Ovarian-Cancer-Associated Mesenchymal Stem Cells Drives Metastasis Cell Reports.  DOI: 10.1016/j.celrep.2020.108473</li> <li>ROBERTSON et al. (2020). 112-OR: Integrative Analysis of Chromatin Accessibility and Genetic Risk in T1D Patients and Controls Diabetes.  DOI: 10.2337/db20-112-or</li> <li>Smith and Sheffield (2020). Analytical Approaches for ATAC-seq Data Analysis Current Protocols in Human Genetics.  DOI: 10.1002/cphg.101</li> <li>Liu (2020). Clinical implications of chromatin accessibility in human cancers DOI: 10.18632/oncotarget.27584</li> <li>Zhou et al. (2020). CATA: a comprehensive chromatin accessibility database for cancer bioRxiv.  DOI: 10.1101/2020.05.16.099325</li> <li>Wang et al. (2020). Interdependence between histone marks and steps in Pol II transcription DOI: 10.1101/2020.04.08.032730</li> <li>Cai et al. (2020). Specific chromatin landscapes and transcription factors couple breast cancer subtype with metastatic relapse to lung or brain DOI: 10.1186/s12920-020-0695-0</li> <li>Li et al. (2020). Acetate supplementation restores chromatin accessibility and promotes tumor cell differentiation under hypoxia Cell Death &amp;$$ Disease.  DOI: 10.1038/s41419-020-2303-9</li> <li>Stolarczyk et al. (2020). Refgenie: a reference genome resource manager GigaScience.  DOI: 10.1093/gigascience/giz149</li> <li>Liang et al. (2019). Global changes in chromatin accessibility and transcription following ATRX inactivation in human cancer cells DOI: 10.1002/1873-3468.13549</li> <li>Corces et al. (2018). The chromatin accessibility landscape of primary human cancers Science.  DOI: 10.1126/science.aav1898</li> <li>Datlinger et al. (2017). Pooled CRISPR screening with single-cell transcriptome readout Nat. Methods.  DOI: 10.1038/nmeth.4177</li> <li>Sheffield et al. (2017). DNA methylation heterogeneity defines a disease spectrum in Ewing sarcoma Nature Medicine.  DOI: 10.1038/nm.4273</li> </ul>"},{"location":"statistics/#pep-shield","title":"PEP shield","text":"<p>If your project is PEP-compatible, please add it to this list with a pull request and use this shield to showcase PEP:</p> <p></p> <p>Here's <code>markdown</code> (for use on GitHub READMEs): <pre><code>[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pepkit.github.io)\n</code></pre></p> <p>Or <code>HTML</code>: <pre><code>&lt;a href=\"https://pepkit.github.io\"&gt;&lt;img src=\"https://pepkit.github.io/img/PEP-compatible-green.svg\" alt=\"PEP compatible\" style=\"float:left; margin:10px\"&gt;&lt;/a&gt;\n</code></pre></p>"},{"location":"eido/","title":"Eido","text":""},{"location":"eido/#introduction","title":"Introduction","text":"<p>Eido is used to 1) validate or 2) convert format of sample metadata. Sample metadata is stored according to the standard PEP specification. For validation, eido is based on JSON Schema and extends it with new features, like required input files. You can write your own schema for your pipeline and use eido to validate sample metadata. For conversion, eido filters convert sample metadata input into any output format, including custom filters.</p>"},{"location":"eido/#why-do-we-need-eido","title":"Why do we need eido?","text":"<p>Data-intensive bioinformatics projects often include metadata describing a set of samples. When it comes to handling such sample metadata, there are two common challenges that eido solves:</p> <p></p> <ul> <li>Validation. Tool authors use eido to specify and describe required input sample attributes. Input sample attributes are described with a schema, and eido validates the sample metadata to ensure it satisfies the tool's needs. Eido uses JSON Schema, which annotates and validates JSON. JSON schema alone is great for validating JSON, but bioinformatics sample metadata is more complicated, so eido provides additional capability and features tailored to bioinformatics projects listed below. </li> </ul> <p></p> <ul> <li>Format conversion. Tools often require sample metadata in a specific format. Eido filters take a metadata in standard PEP format and convert it to any desired output format. Filters can be either built-in or custom. This allows a single sample metadata source to be used for multiple downstream analyses.</li> </ul>"},{"location":"eido/#eido-validation-features","title":"Eido validation features","text":"<p>An eido schema is written using the JSON Schema vocabulary, plus a few additional features:</p> <ol> <li>required input files. Eido adds <code>required_files</code>, which allows a schema author to specify which attributes must point to files that exist.</li> <li>optional input files. <code>files</code> specifies which attributes point to files that may or may not exist.</li> <li>project and sample validation. Eido validates project attributes separately from sample attributes.</li> <li>schema imports. Eido adds an <code>imports</code> section for schemas that should be validated prior to this schema</li> <li>automatic multi-value support. Eido validates successfully for singular or plural sample attributes for strings, booleans, and numbers. This accommodates the PEP subsample_table feature.</li> </ol>"},{"location":"eido/#how-to-use-eido","title":"How to use eido","text":"<ul> <li>Use eido to validate data from the command line</li> <li>Use eido to validate data from Python</li> <li>Write your own schema</li> </ul>"},{"location":"eido/#why-the-name-eido","title":"Why the name 'eido'?","text":"<p>Eidos is a Greek term meaning form, essence, or type (see Plato's Theory of Forms). Schemas are analogous to forms, and eido tests claims that an instance is of a particular form. Eido also helps change forms using filters.</p>"},{"location":"eido/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"eido/changelog/#024-2024-09-30","title":"[0.2.4] - 2024-09-30","text":""},{"location":"eido/changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed eido convert filters for csv and yaml appear to be broken #70</li> <li>Fixed error in validating valid sample item in a project #76</li> </ul>"},{"location":"eido/changelog/#added","title":"Added","text":"<ul> <li>added new tests</li> </ul>"},{"location":"eido/changelog/#023-2024-09-10","title":"[0.2.3] - 2024-09-10","text":""},{"location":"eido/changelog/#changed","title":"Changed","text":"<ul> <li>bump peppy reqs to 0.40.6</li> <li>refactor files_key to sizing, required_files_key to tangible_key, _samples to samples</li> </ul>"},{"location":"eido/changelog/#022-2023-11-16","title":"[0.2.2] - 2023-11-16","text":""},{"location":"eido/changelog/#changed_1","title":"Changed","text":"<ul> <li>remove unused <code>exclude-case</code> from CLI. Fixes #65</li> </ul>"},{"location":"eido/changelog/#021-2023-07-05","title":"[0.2.1] - 2023-07-05","text":""},{"location":"eido/changelog/#changed_2","title":"Changed","text":"<ul> <li>printing project to logger.debug</li> </ul>"},{"location":"eido/changelog/#020-2023-06-21","title":"[0.2.0] - 2023-06-21","text":""},{"location":"eido/changelog/#added_1","title":"Added","text":"<ul> <li>CLI options for modulating peppy project (#50)</li> </ul>"},{"location":"eido/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Rewrote history to remove large files committed by mistake</li> <li>You can now check for existing of files in subsample table (#26)</li> </ul>"},{"location":"eido/changelog/#changed_3","title":"Changed","text":"<ul> <li>All validation functions now use similar error-raising behavior</li> <li>Validation functions now return all individual error objects grouped by type</li> </ul>"},{"location":"eido/changelog/#019-2022-09-12","title":"[0.1.9] - 2022-09-12","text":""},{"location":"eido/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>CSV filter bug</li> </ul>"},{"location":"eido/changelog/#added_2","title":"Added","text":"<ul> <li>New test cases</li> </ul>"},{"location":"eido/changelog/#018-2022-08-29","title":"[0.1.8] - 2022-08-29","text":""},{"location":"eido/changelog/#changed_4","title":"Changed","text":"<ul> <li>the way of merging tables for multiline output format from eido convert</li> </ul>"},{"location":"eido/changelog/#added_3","title":"Added","text":"<ul> <li>better architecture for output formatters that goes well with open-closed principle</li> <li>using mock in some testcases</li> <li>test data in the format that was causing the errors previously</li> </ul>"},{"location":"eido/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>passing plugin keyword arguments to <code>run_filter</code> function</li> <li>saving output file will now work for path like <code>file.txt</code>, no need to pass full path</li> </ul>"},{"location":"eido/changelog/#017-2022-08-11","title":"[0.1.7] - 2022-08-11","text":""},{"location":"eido/changelog/#changed_5","title":"Changed","text":"<ul> <li>When a validation fails, <code>eido</code> will now return all errors instead of just the first one it finds.</li> </ul>"},{"location":"eido/changelog/#016-2022-05-16","title":"[0.1.6] - 2022-05-16","text":""},{"location":"eido/changelog/#added_4","title":"Added","text":"<ul> <li>a possibility to set a custom sample table index with <code>-s/--st-index</code> option</li> <li>an option to see filters docs via CLI: <code>eido filters -f &lt;filter_name&gt;</code></li> <li>PEP filters now return their conversion result for progrommatic use.</li> <li>PEP filters can write to files.</li> <li>A filter can write multiple outputs to multiple files using the <code>paths</code> keyword arg.</li> </ul>"},{"location":"eido/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Some error messages with incorrectly defined schemas.</li> <li>'required' attribute is no longer required in schema</li> </ul>"},{"location":"eido/changelog/#changed_6","title":"Changed","text":"<ul> <li>Moved all <code>eido filter</code> functionality into the <code>eido convert</code> command for simplicity. This way, a single top-level command namespace holds all related functionality. Filters are still EXPERIMENTAL.</li> </ul>"},{"location":"eido/changelog/#015-2021-04-15","title":"[0.1.5] - 2021-04-15","text":""},{"location":"eido/changelog/#added_5","title":"Added","text":"<ul> <li><code>eido convert</code> converts the provided PEP to a specified format (EXPERIMENTAL! may change in future versions)</li> <li><code>eido filter</code> lists available filters in current environment (EXPERIMENTAL! may change in future versions)</li> <li>built-in plugins (EXPERIMENTAL! may change in future versions):</li> <li><code>basic_pep_filter</code></li> <li><code>yaml_pep_filter</code></li> <li><code>csv_pep_filter</code></li> <li><code>yaml_samples_pep_filter</code></li> </ul>"},{"location":"eido/changelog/#changed_7","title":"Changed","text":"<ul> <li>in <code>validate_inputs</code> function sample attributes are first validated for existence before their values' existence is checked</li> </ul>"},{"location":"eido/changelog/#014-2021-03-14","title":"[0.1.4] - 2021-03-14","text":""},{"location":"eido/changelog/#changed_8","title":"Changed","text":"<ul> <li>update schema preprocessing</li> </ul>"},{"location":"eido/changelog/#013-2020-10-07","title":"[0.1.3] - 2020-10-07","text":""},{"location":"eido/changelog/#changed_9","title":"Changed","text":"<ul> <li><code>validate_inputs</code> function now returns a dictionary with validation data, i.e missing, required_inputs, all_inputs, input_file_size rather than a list of missing files</li> <li><code>validate_inputs</code> function does not modify <code>Sample</code> objects</li> </ul>"},{"location":"eido/changelog/#012-2020-08-06","title":"[0.1.2] - 2020-08-06","text":""},{"location":"eido/changelog/#added_6","title":"Added","text":"<ul> <li>license in the package source distribution</li> </ul>"},{"location":"eido/changelog/#011-2020-05-27","title":"[0.1.1] - 2020-05-27","text":""},{"location":"eido/changelog/#changed_10","title":"Changed","text":"<ul> <li>documentation updates</li> <li>CLI behavior when no subcommand provided; #20</li> </ul>"},{"location":"eido/changelog/#010-2020-05-26","title":"[0.1.0] - 2020-05-26","text":""},{"location":"eido/changelog/#added_7","title":"Added","text":"<ul> <li>automatic support for subsamples for sample the following property types:<ul> <li><code>string</code></li> <li><code>boolean</code></li> <li><code>numeric</code></li> </ul> </li> <li><code>eido inspect</code> CLI command</li> <li>schema importing functionality (via top level <code>imports</code> key)</li> <li>exported functions:<ul> <li><code>validate_inputs</code></li> <li><code>inspect_project</code></li> <li><code>read_schema</code></li> </ul> </li> </ul>"},{"location":"eido/changelog/#changed_11","title":"Changed","text":"<ul> <li>previous CLI <code>eido</code> functionality moved to <code>eido validate</code></li> </ul>"},{"location":"eido/changelog/#006-2020-02-07","title":"[0.0.6] - 2020-02-07","text":""},{"location":"eido/changelog/#changed_12","title":"Changed","text":"<ul> <li>CLI can accommodate URLs.</li> </ul>"},{"location":"eido/changelog/#005-2020-02-04","title":"[0.0.5] - 2020-02-04","text":""},{"location":"eido/changelog/#added_8","title":"Added","text":"<ul> <li>documentation website</li> <li>include version in the CLI help</li> </ul>"},{"location":"eido/changelog/#004-2020-01-31","title":"[0.0.4] - 2020-01-31","text":""},{"location":"eido/changelog/#added_9","title":"Added","text":"<ul> <li><code>validate_sample</code> function for sample level validation</li> <li>sample validation CLI support (via <code>-n</code>/<code>--sample-name</code> argument)</li> <li><code>validate_config</code> to facilitate samples exclusion in validation</li> <li>config validation CLI support (via <code>-c</code>/<code>--just-config</code> argument)</li> </ul>"},{"location":"eido/changelog/#003-2020-01-30","title":"[0.0.3] - 2020-01-30","text":""},{"location":"eido/changelog/#added_10","title":"Added","text":"<ul> <li>Option to exclude the validation case from error messages in both Python API and CLI app with <code>exclude_case</code> and <code>-e</code>/<code>--exclude-case</code>, respectively.</li> <li>include requirements in the source distribution</li> </ul>"},{"location":"eido/changelog/#002-2020-01-12","title":"[0.0.2] - 2020-01-12","text":""},{"location":"eido/changelog/#added_11","title":"Added","text":"<ul> <li>Initial project release</li> </ul>"},{"location":"eido/contributing/","title":"Contributing","text":"<p>Pull requests or issues are welcome. After adding a new feature, please add tests in the <code>tests</code> folder and run the test suite. The only additional dependencies needed beyond those for the package can be installed with: <code>pip install -r requirements/requirements-dev.txt</code>.</p> <p>Once those are installed, run the tests with <code>pytest</code> or <code>python setup.py test</code>.</p>"},{"location":"eido/example-schemas/","title":"List of schemas","text":"<p>With <code>eido</code> you can create your own schema to describe the kind of projects your tool can accept. We are currently hosting schemas at schema.databio.org in a GitHub repository. You can browse that repository for examples, or look at few examples here:</p> <ul> <li>Generic PEP2.0.0 schema -- all PEPs should validate against this schema</li> <li>PEPPRO pipeline schema -- describes PEPs compatible with the PEPPRO pipeline</li> <li>PEPATAC pipeline schema -- describes PEPs compatible with the PEPATAC pipeline</li> <li>refgenie databio build schema -- describes PEPs compatible with building refgenie assets</li> </ul>"},{"location":"eido/filters/","title":"Using eido filters","text":"<p>Filters are an experimental feature and may change in future versions of <code>eido</code>.</p>"},{"location":"eido/filters/#using-eido-filters","title":"Using eido filters","text":"<p>Eido provides a CLI to convert a PEP into different output formats. These include some built-in formats, like csv (which produces a processed csv file, with project/sample already modified), yaml, and a few others. It also provides a plugin system so that you can write your own Python functions to provide custom output formats. You access filters through the <code>eido convert</code> command.</p>"},{"location":"eido/filters/#view-available-filters","title":"View available filters","text":"<p>To list available filters:</p> <pre><code>eido convert --list\n</code></pre> <p>You'll see some output like this. There are a few built-in filters available:</p> <pre><code>Available filters:\n - basic\n - csv\n - yaml\n - yaml-samples\n</code></pre> <p>You can add to this list by writing a custom filter, which will write your PEP into whatever format you need.</p>"},{"location":"eido/filters/#convert-a-pep-into-an-alternative-format-with-a-filter","title":"Convert a PEP into an alternative format with a filter","text":"<p>To convert a PEP into an output format, do this:</p> <pre><code>eido convert config.yaml -f basic\nrunning plugin pep\nProject 'pepconvert' (/home/nsheff/code/pepconvert/config.yaml)\n5 samples: WT_REP1, WT_REP2, RAP1_UNINDUCED_REP1, RAP1_UNINDUCED_REP2, RAP1_IAA_30M_REP1\nSections: pep_version, sample_table, subsample_table\n...\n</code></pre> <p>This basic format just lists the config file, the number of samples and their names, and identifies the sections in the project config file. Another format is <code>-f yaml</code>,</p> <pre><code>eido convert config.yaml -f yaml\n</code></pre> <p>This will output your samples in yaml format.</p>"},{"location":"eido/filters/#parametrizing-filters","title":"Parametrizing filters","text":"<p>Filter functions are parameterizable. Some filters may request or require parameters. To learn more about a filter's parameters, use <code>-d</code> or <code>--describe</code>: <code>eido convert -f &lt;filter_name&gt; -d</code>, which displays the plugin documentation. For example:</p> <pre><code>eido convert -f yaml-samples -d\n\n    YAML samples PEP filter, that returns only Sample object representations.\n\n    This filter can save the YAML to file, if kwargs include `path`.\n\n    :param peppy.Project p: a Project to run filter on\n</code></pre> <p>In this case, the argument <code>path</code> can be provided as an output file. Like this: </p> <pre><code>eido convert config.yaml -f yaml-samples -a path=output.yaml\n</code></pre> <p>More generally, the form to provide parameters is like this</p> <pre><code>eido convert config.yaml -f &lt;filter_name&gt; -a argument1=value1 argument2=value2\n</code></pre>"},{"location":"eido/install/","title":"Installing eido","text":"<p>Install from GitHub releases or from PyPI using <code>pip</code>:</p> <ul> <li><code>pip install --user eido</code>: install into user space.</li> <li><code>pip install --user --upgrade eido</code>: update in user space.</li> <li><code>pip install eido</code>: install into an active virtual environment.</li> <li><code>pip install --upgrade eido</code>: update in virtual environment.</li> </ul> <p>See if your install worked by calling <code>eido -h</code> on the command line. If the <code>eido</code> executable in not in your <code>$PATH</code>, append this to your <code>.bashrc</code> or <code>.profile</code> (or <code>.bash_profile</code> on macOS):</p> <pre><code>export PATH=~/.local/bin:$PATH\n</code></pre>"},{"location":"eido/writing-a-filter/","title":"Writing a custom filter","text":"<p>Filters are an experimental feature and may change in future versions of <code>eido</code></p>"},{"location":"eido/writing-a-filter/#how-to-write-a-custom-eido-filter","title":"How to write a custom eido filter","text":"<p>One of <code>eido</code>'s tasks is to provide a CLI to convert a PEP into alternative formats. These include some built-in formats, like <code>csv</code> (which spits out a processed <code>csv</code> file, with project/sample modified), <code>yaml</code>, and a few others. It also provides a plugin system so that you can write your own Python functions to provide custom output formats.</p>"},{"location":"eido/writing-a-filter/#custom-filters","title":"Custom filters","text":"<p>To write a custom filter, start by writing a Python package. You will need to include a function that takes a <code>peppy.Project</code> object as input, and prints out the custom file format. The filter functions also can require additional keyword arguments.</p>"},{"location":"eido/writing-a-filter/#1-write-functions-to-call","title":"1. Write functions to call","text":"<p>The package contain one or more functions. The filter function must take a peppy.Project object and <code>**kwargs</code> as parameters. Example:</p> <p><pre><code>import peppy\n\ndef my_custom_filter(p, **kwargs):\n    import re\n    import sys\n    import yaml\n\n    for s in p.samples:\n        sys.stdout.write(\"- \")\n        out = re.sub('\\n', '\\n  ', yaml.safe_dump(s.to_dict(), default_flow_style=False))\n        sys.stdout.write(out + \"\\n\")\n</code></pre> For reference you can check the signatures of the functions in Built-in <code>eido</code> Plugins Documentation. Importantly, if the function requires any arguments (always provided via <code>**kwargs</code>), the creator of the function should take care of handling missing/faulty input.</p> <p>Next, we need to link that function in to the <code>eido</code> filter plugin system.</p>"},{"location":"eido/writing-a-filter/#2-add-entry_points-to-setuppy","title":"2. Add entry_points to setup.py","text":"<p>The <code>setup.py</code> file uses <code>entry_points</code> to specify a mapping of eido hooks to functions to call.</p> <pre><code>entry_points={\n    \"pep.filters\": [\n        \"basic=eido.conversion_plugins:basic_pep_filter\",\n        \"yaml=eido.conversion_plugins:yaml_pep_filter\",\n        \"csv=eido.conversion_plugins:csv_pep_filter\",\n        \"yaml-samples=eido.conversion_plugins:yaml_samples_pep_filter\",\n    ],\n},\n</code></pre> <p>The format is: <code>'pep.filters': 'FILTER_NAME=PLUGIN_PACKAGE_NAME:FUNCTION_NAME'</code>.</p> <ul> <li>\"FILTER_NAME\" can be any unique identifier for your plugin</li> <li>\"PLUGIN_PACKAGE_NAME\" must be the name of python package the holds your plugin.</li> <li>\"FUNCTION_NAME\" must match the name of the function in your package</li> </ul>"},{"location":"eido/writing-a-filter/#3-install-package","title":"3. Install package","text":"<p>If you install this package, any filters provided by it will be available for use with eido, which you can see using <code>eido filters</code>.</p>"},{"location":"eido/writing-a-schema/","title":"How to write a PEP schema","text":"<p>If you are a tool developer, we recommend you write a PEP schema that describes what sample and project attributes are required for your tool to work. PEP schemas use the JSON Schema vocabulary, plus some additional features. This guide will walk you through everything you need to know to write your own schema. It assumes you already have a basic familiarity with JSON Schema.</p>"},{"location":"eido/writing-a-schema/#importing-the-base-pep-schema","title":"Importing the base PEP schema","text":"<p>One of the features added by <code>eido</code> is the <code>imports</code> attribute. This allows you to extend existing schemas. We recommend your new PEP schema start by importing the base PEP schema. This will ensure that the putative PEP at least follows the basic PEP specification, which you will then build on with your tool-specific requirements. Here's how we'll start with importing the generic base PEP schema:</p> <pre><code>description: A example schema for a pipeline.\nimports:\n  - http://schema.databio.org/pep/2.0.0.yaml\n</code></pre> <p>You can also use the <code>imports</code> to build other schemas that subclass your own schemas.</p>"},{"location":"eido/writing-a-schema/#project-and-sample-sections","title":"Project and sample sections","text":"<p>Like the PEP itself, the schema is divided into two sections, one for the project config, and one for the samples. So, base PEP schema defines an object with two components: a <code>config</code> object, and a <code>samples</code> array:</p> <pre><code>description: A example schema for a pipeline.\nimports:\n  - http://schema.databio.org/pep/2.0.0.yaml\nproperties:\n  config:\n    type: object\n  samples:\n    type: array\nrequired:\n  - samples\n  - config\n</code></pre>"},{"location":"eido/writing-a-schema/#required-sample-attributes","title":"Required sample attributes","text":"<p>Let's say you're writing a PEP-compatible tool that requires 3 arguments: <code>read1</code>, <code>read2</code>, and <code>genome</code>, and also offers optional argument <code>read_length</code>.  Validating the generic PEP specification will not confirm all required attributes, so you want to write an extended schema. Starting from the base above, we're not changing the <code>config</code> section so we can drop that, and we add new parameters for the required sample attributes like this:</p> <pre><code>description: A example schema for a pipeline.\nimports:\n  - http://schema.databio.org/pep/2.0.0.yaml\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        read1:\n          type: string\n          description: \"Fastq file for read 1\"\n        read2:\n          type: string\n          description: \"Fastq file for read 2\"\n        genome:\n          type: string\n          description: \"Refgenie genome registry identifier\"\n        read_length:\n          type: integer\n          description: \"Length of the Unique Molecular Identifier, if any\"\n      required:\n        - read1\n        - read2\n        - genome\nrequired:\n  - samples\n</code></pre> <p>This document defines the required an optional sample attributes for this pipeline. That's all you need to do, and your users can validate an existing PEP to see if it meets the requirements of your tool.</p>"},{"location":"eido/writing-a-schema/#required-input-files","title":"Required input files","text":"<p>In the above example, we listed <code>read1</code> and <code>read2</code> attributes as required. This will enforce that these attributes must be defined on the samples, but for this example, this is not enough -- these also must point to files that exist. Checking for files is outside the scope of JSON Schema, which only validates JSON documents, so eido extends JSON Schema with the ability to specify which attributes should point to files.</p> <p>Eido provides two ways to do it: <code>sizing</code> and <code>tangible</code>. The basic <code>sizing</code> is simply used to specify which attributes point to files, which are not required to exist. This is useful for tools that want to calculate the total size of any provided inputs, for example. The <code>tangible</code> list specifies that the attributes point to files that must exist, otherwise the PEP doesn't validate. Here's an example of specifying an optional and required input attribute:</p> <pre><code>description: A PEP for ATAC-seq samples for the PEPATAC pipeline.\nimports:\n  - http://schema.databio.org/pep/2.0.0.yaml\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        sample_name:\n          type: string\n          description: \"Name of the sample\"\n        organism:\n          type: string\n          description: \"Organism\"\n        protocol:\n          type: string\n          description: \"Must be an ATAC-seq or DNAse-seq sample\"\n        genome:\n          type: string\n          description: \"Refgenie genome registry identifier\"\n        read_type:\n          type: string\n          description: \"Is this single or paired-end data?\"\n          enum: [\"SINGLE\", \"PAIRED\"]\n        read1:\n          type: string\n          description: \"Fastq file for read 1\"\n        read2:\n          type: string\n          description: \"Fastq file for read 2 (for paired-end experiments)\"\n      tangible:\n        - read1\n      files:\n        - read1\n        - read2\n</code></pre> <p>This could a valid example for a pipeline that accepts either single-end or paired-end data, so <code>read1</code> must point to a file, whereas <code>read2</code> isn't required, but if it does point to a file, then this file is also to be considered an input file.</p>"},{"location":"eido/writing-a-schema/#example-schemas","title":"Example schemas","text":"<p>If you need more information, it would be a good idea to look at example schemas for ideas.</p>"},{"location":"eido/code/cli/","title":"<code>eido</code> command line usage","text":"<p>To use the command line application one just needs a path to a project configuration file. It is a positional argument in the <code>eido</code> command.</p> <p>For this tutorial, let's grab a PEP from a public example repository that describes a few PRO-seq test samples:</p> <pre><code>rm -rf ppqc\ngit clone https://github.com/databio/ppqc.git --branch cfg2\n</code></pre> <pre><code>Cloning into 'ppqc'...\nremote: Enumerating objects: 154, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (15/15), done.\u001b[K\nremote: Total 154 (delta 7), reused 17 (delta 5), pack-reused 134\u001b[K\nReceiving objects: 100% (154/154), 81.69 KiB | 3.27 MiB/s, done.\nResolving deltas: 100% (82/82), done.\n</code></pre> <pre><code>cd ppqc\nexport DATA=$HOME\nexport SRAFQ=$HOME\n</code></pre>"},{"location":"eido/code/cli/#pep-inspection","title":"PEP inspection","text":"<p>First, let's use <code>eido inspect</code> to inspect a PEP. </p> <ul> <li>To inspect the entire <code>Project</code> object just provide the path to the project configuration file.</li> </ul> <pre><code>eido inspect peppro_paper.yaml\n</code></pre> <pre><code>Project 'PEPPRO' (peppro_paper.yaml)\n47 samples (showing first 20): K562_PRO-seq_02, K562_PRO-seq_04, K562_PRO-seq_06, K562_PRO-seq_08, K562_PRO-seq_10, K562_PRO-seq_20, K562_PRO-seq_30, K562_PRO-seq_40, K562_PRO-seq_50, K562_PRO-seq_60, K562_PRO-seq_70, K562_PRO-seq_80, K562_PRO-seq_90, K562_PRO-seq_100, K562_RNA-seq_0, K562_RNA-seq_10, K562_RNA-seq_20, K562_RNA-seq_30, K562_RNA-seq_40, K562_RNA-seq_50\nSections: name, pep_version, sample_table, looper, sample_modifiers\n</code></pre> <ul> <li>To inspect a specific sample, one needs to provide the sample name (via <code>-n</code>/<code>--sample-name</code> optional argument)</li> </ul> <pre><code>eido inspect peppro_paper.yaml -n K562_PRO-seq K562_RNA-seq_10\n</code></pre> <pre><code>Sample 'K562_RNA-seq_10' in Project (peppro_paper.yaml)\n\nsample_name:         K562_RNA-seq_10\nsample_desc:         90% K562 PRO-seq + 10% K562 RNA-seq\ntreatment:           70M total reads\nprotocol:            PRO\norganism:            human\nread_type:           SINGLE\numi_len:             0\nread1:               /Users/mstolarczyk/K562_10pctRNA.fastq.gz\nsrr:                 K562_10pctRNA\npipeline_interfaces: $CODE/peppro/sample_pipeline_interface.yaml\ngenome:              hg38\n\n...                (showing first 10)\n</code></pre>"},{"location":"eido/code/cli/#pep-validation","title":"PEP validation","text":"<p>Next, let's use <code>eido</code> to validate this project against the generic PEP schema. You just need to provide a path to the project config file and schema as an input.</p> <pre><code>eido validate peppro_paper.yaml -s http://schema.databio.org/pep/2.0.0.yaml -e\n</code></pre> <pre><code>Validation successful\n</code></pre> <p>Any PEP should validate against that schema, which describes generic PEP format. We can go one step further and validate it against the PEPPRO schema, which describes Proseq projects specfically for this pipeline:</p> <pre><code>eido validate peppro_paper.yaml -s http://schema.databio.org/pipelines/ProseqPEP.yaml\n</code></pre> <pre><code>Validation successful\n</code></pre> <p>This project would not validate against a different pipeline's schema.</p> <p>Following <code>jsonschema</code>, <code>eido</code> produces comprehensive error messages that include the objects that did not pass validation. When validating PEPs that include lots of samples one can use option <code>-e</code>/<code>--exclude-case</code> to limit the error output just to the human readable message. This is the option used in the example below:</p> <pre><code>eido validate peppro_paper.yaml -s http://schema.databio.org/pipelines/bedmaker.yaml -e\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"/usr/local/bin/eido\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/usr/local/lib/python3.9/site-packages/eido/cli.py\", line 89, in main\n    validate_project(p, args.schema, args.exclude_case)\n  File \"/usr/local/lib/python3.9/site-packages/eido/validation.py\", line 45, in validate_project\n    _validate_object(project_dict, preprocess_schema(schema_dict), exclude_case)\n  File \"/usr/local/lib/python3.9/site-packages/eido/validation.py\", line 30, in _validate_object\n    raise jsonschema.exceptions.ValidationError(e.message)\njsonschema.exceptions.ValidationError: 'input_file_path' is a required property\n</code></pre> <p>Optionally, to validate just the config part of the PEP or a specific sample, <code>-n</code>/<code>--sample-name</code> or <code>-c</code>/<code>--just-config</code> arguments should be used, respectively. Please refer to the help for more details:</p> <pre><code>eido validate -h\n</code></pre> <pre><code>usage: eido validate [-h] -s S [-e] [-n S | -c] PEP\n\nValidate the PEP or its components.\n\npositional arguments:\n  PEP                   Path to a PEP configuration file in yaml format.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s S, --schema S      Path to a PEP schema file in yaml format.\n  -e, --exclude-case    Whether to exclude the validation case from an error.\n                        Only the human readable message explaining the error\n                        will be raised. Useful when validating large PEPs.\n  -n S, --sample-name S\n                        Name or index of the sample to validate. Only this\n                        sample will be validated.\n  -c, --just-config     Whether samples should be excluded from the\n                        validation.\n</code></pre>"},{"location":"eido/code/cli/#pep-conversion","title":"PEP conversion","text":"<p>Let's use <code>eido convert</code> command to convert PEPs to a variety of different formats. <code>eido</code> supports a plugin system, which can be used by other tool developers to create Python plugin functions that save PEPs in a desired format. Please refer to the documentation for more details. For now let's focus on a couple of plugins that are built-in in <code>eido</code>.</p> <p>To see what plugins are currently avaialable in your Python environment call:</p> <pre><code>eido filters\n</code></pre> <pre><code>Available filters:\n - basic\n - csv\n - yaml\n - yaml-samples\n</code></pre> <pre><code>eido convert peppro_paper.yaml --format basic\n</code></pre> <pre><code>Running plugin basic\nProject 'PEPPRO' (peppro_paper.yaml)\n47 samples (showing first 20): K562_PRO-seq_02, K562_PRO-seq_04, K562_PRO-seq_06, K562_PRO-seq_08, K562_PRO-seq_10, K562_PRO-seq_20, K562_PRO-seq_30, K562_PRO-seq_40, K562_PRO-seq_50, K562_PRO-seq_60, K562_PRO-seq_70, K562_PRO-seq_80, K562_PRO-seq_90, K562_PRO-seq_100, K562_RNA-seq_0, K562_RNA-seq_10, K562_RNA-seq_20, K562_RNA-seq_30, K562_RNA-seq_40, K562_RNA-seq_50\nSections: name, pep_version, sample_table, looper, sample_modifiers\n</code></pre> <pre><code>eido convert peppro_paper.yaml --format csv\n</code></pre> <pre><code>Running plugin csv\nsample_name,genome,organism,pipeline_interfaces,prealignments,protocol,read1,read_type,sample_desc,sample_name,srr,treatment,umi_len,read2\nK562_PRO-seq_02,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_2pct.fastq.gz,SINGLE,2% subsample of K562 PRO-seq,K562_PRO-seq_02,K562_PRO_2pct,2% subsample,0,\nK562_PRO-seq_04,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_4pct.fastq.gz,SINGLE,4% subsample of K562 PRO-seq,K562_PRO-seq_04,K562_PRO_4pct,4% subsample,0,\nK562_PRO-seq_06,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_6pct.fastq.gz,SINGLE,6% subsample of K562 PRO-seq,K562_PRO-seq_06,K562_PRO_6pct,6% subsample,0,\nK562_PRO-seq_08,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_8pct.fastq.gz,SINGLE,8% subsample of K562 PRO-seq,K562_PRO-seq_08,K562_PRO_8pct,8% subsample,0,\nK562_PRO-seq_10,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_10pct.fastq.gz,SINGLE,10% subsample of K562 PRO-seq,K562_PRO-seq_10,K562_PRO_10pct,10% subsample,0,\nK562_PRO-seq_20,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_20pct.fastq.gz,SINGLE,20% subsample of K562 PRO-seq,K562_PRO-seq_20,K562_PRO_20pct,20% subsample,0,\nK562_PRO-seq_30,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_30pct.fastq.gz,SINGLE,30% subsample of K562 PRO-seq,K562_PRO-seq_30,K562_PRO_30pct,30% subsample,0,\nK562_PRO-seq_40,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_40pct.fastq.gz,SINGLE,40% subsample of K562 PRO-seq,K562_PRO-seq_40,K562_PRO_40pct,40% subsample,0,\nK562_PRO-seq_50,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_50pct.fastq.gz,SINGLE,50% subsample of K562 PRO-seq,K562_PRO-seq_50,K562_PRO_50pct,50% subsample,0,\nK562_PRO-seq_60,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_60pct.fastq.gz,SINGLE,60% subsample of K562 PRO-seq,K562_PRO-seq_60,K562_PRO_60pct,60% subsample,0,\nK562_PRO-seq_70,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_70pct.fastq.gz,SINGLE,70% subsample of K562 PRO-seq,K562_PRO-seq_70,K562_PRO_70pct,70% subsample,0,\nK562_PRO-seq_80,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_80pct.fastq.gz,SINGLE,80% subsample of K562 PRO-seq,K562_PRO-seq_80,K562_PRO_80pct,80% subsample,0,\nK562_PRO-seq_90,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_PRO_90pct.fastq.gz,SINGLE,90% subsample of K562 PRO-seq,K562_PRO-seq_90,K562_PRO_90pct,90% subsample,0,\nK562_PRO-seq_100,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/SRR155431[1-2].fastq.gz,SINGLE,Unsampled K562 PRO-seq,K562_PRO-seq_100,SRR155431[1-2],none,0,\nK562_RNA-seq_0,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_0pctRNA.fastq.gz,SINGLE,100% K562 PRO-seq + 0% K562 RNA-seq,K562_RNA-seq_0,K562_0pctRNA,70M total reads,0,\nK562_RNA-seq_10,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_10pctRNA.fastq.gz,SINGLE,90% K562 PRO-seq + 10% K562 RNA-seq,K562_RNA-seq_10,K562_10pctRNA,70M total reads,0,\nK562_RNA-seq_20,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_20pctRNA.fastq.gz,SINGLE,80% K562 PRO-seq + 20% K562 RNA-seq,K562_RNA-seq_20,K562_20pctRNA,70M total reads,0,\nK562_RNA-seq_30,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_30pctRNA.fastq.gz,SINGLE,70% K562 PRO-seq + 30% K562 RNA-seq,K562_RNA-seq_30,K562_30pctRNA,70M total reads,0,\nK562_RNA-seq_40,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_40pctRNA.fastq.gz,SINGLE,60% K562 PRO-seq + 40% K562 RNA-seq,K562_RNA-seq_40,K562_40pctRNA,70M total reads,0,\nK562_RNA-seq_50,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_50pctRNA.fastq.gz,SINGLE,50% K562 PRO-seq + 50% K562 RNA-seq,K562_RNA-seq_50,K562_50pctRNA,70M total reads,0,\nK562_RNA-seq_60,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_60pctRNA.fastq.gz,SINGLE,40% K562 PRO-seq + 60% K562 RNA-seq,K562_RNA-seq_60,K562_60pctRNA,70M total reads,0,\nK562_RNA-seq_70,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_70pctRNA.fastq.gz,SINGLE,30% K562 PRO-seq + 70% K562 RNA-seq,K562_RNA-seq_70,K562_70pctRNA,70M total reads,0,\nK562_RNA-seq_80,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_80pctRNA.fastq.gz,SINGLE,20% K562 PRO-seq + 80% K562 RNA-seq,K562_RNA-seq_80,K562_80pctRNA,70M total reads,0,\nK562_RNA-seq_90,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_90pctRNA.fastq.gz,SINGLE,10% K562 PRO-seq + 90% K562 RNA-seq,K562_RNA-seq_90,K562_90pctRNA,70M total reads,0,\nK562_RNA-seq_100,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/K562_100pctRNA.fastq.gz,SINGLE,0% K562 PRO-seq + 100% K562 RNA-seq,K562_RNA-seq_100,K562_100pctRNA,70M total reads,0,\nK562_GRO-seq,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,GRO,/Users/mstolarczyk/SRR1552484.fastq.gz,SINGLE,K562 GRO-seq,K562_GRO-seq,SRR1552484,none,0,\nHelaS3_GRO-seq,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,GRO,/Users/mstolarczyk/SRR169361[1-2].fastq.gz,SINGLE,HelaS3 GRO-seq,HelaS3_GRO-seq,SRR169361[1-2],none,0,\nJurkat_ChRO-seq_1,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/SRR7616133.fastq.gz,SINGLE,Jurkat ChRO-seq,Jurkat_ChRO-seq_1,SRR7616133,none,6,\nJurkat_ChRO-seq_2,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/SRR7616134.fastq.gz,SINGLE,Jurkat ChRO-seq,Jurkat_ChRO-seq_2,SRR7616134,none,6,\nHEK_PRO-seq,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/SRR8608074_PE1.fastq.gz,PAIRED,\"HEK w/ osTIR1, ZNF143AID PRO-seq\",HEK_PRO-seq,SRR8608074,Auxin,8,/Users/mstolarczyk/SRR8608074_PE2.fastq.gz\nHEK_ARF_PRO-seq,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/SRR8608070_PE1.fastq.gz,PAIRED,\"HEK w/ osTIR1, ZNF143AID, ARF PRO-seq\",HEK_ARF_PRO-seq,SRR8608070,Auxin,8,/Users/mstolarczyk/SRR8608070_PE2.fastq.gz\nH9_PRO-seq_1,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_DMSO_rep1_PE1.fastq.gz,PAIRED,H9 PRO-seq,H9_PRO-seq_1,H9_DMSO_rep1,DMSO,8,/Users/mstolarczyk/H9_DMSO_rep1_PE2.fastq.gz\nH9_PRO-seq_2,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_DMSO_rep2_PE1.fastq.gz,PAIRED,H9 PRO-seq,H9_PRO-seq_2,H9_DMSO_rep2,DMSO,8,/Users/mstolarczyk/H9_DMSO_rep2_PE2.fastq.gz\nH9_PRO-seq_3,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_DMSO_rep3_PE1.fastq.gz,PAIRED,H9 PRO-seq,H9_PRO-seq_3,H9_DMSO_rep3,DMSO,8,/Users/mstolarczyk/H9_DMSO_rep3_PE2.fastq.gz\nH9_treated_PRO-seq_1,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_200nM_romidepsin_rep1_PE1.fastq.gz,PAIRED,H9 treated PRO-seq,H9_treated_PRO-seq_1,H9_200nM_romidepsin_rep1,200 nM romidepsin,8,/Users/mstolarczyk/H9_200nM_romidepsin_rep1_PE2.fastq.gz\nH9_treated_PRO-seq_2,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_200nM_romidepsin_rep2_PE1.fastq.gz,PAIRED,H9 treated PRO-seq,H9_treated_PRO-seq_2,H9_200nM_romidepsin_rep2,200 nM romidepsin,8,/Users/mstolarczyk/H9_200nM_romidepsin_rep2_PE2.fastq.gz\nH9_treated_PRO-seq_3,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_200nM_romidepsin_rep3_PE1.fastq.gz,PAIRED,H9 treated PRO-seq,H9_treated_PRO-seq_3,H9_200nM_romidepsin_rep3,200 nM romidepsin,8,/Users/mstolarczyk/H9_200nM_romidepsin_rep3_PE2.fastq.gz\n\n\nH9_PRO-seq_10,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_10pct_PE1.fastq.gz,PAIRED,10% subset H9 PRO-seq 2,H9_PRO-seq_10,H9_PRO-seq_10pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_10pct_PE2.fastq.gz\nH9_PRO-seq_20,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_20pct_PE1.fastq.gz,PAIRED,20% subset H9 PRO-seq 2,H9_PRO-seq_20,H9_PRO-seq_20pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_20pct_PE2.fastq.gz\nH9_PRO-seq_30,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_30pct_PE1.fastq.gz,PAIRED,30% subset H9 PRO-seq 2,H9_PRO-seq_30,H9_PRO-seq_30pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_30pct_PE2.fastq.gz\nH9_PRO-seq_40,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_40pct_PE1.fastq.gz,PAIRED,40% subset H9 PRO-seq 2,H9_PRO-seq_40,H9_PRO-seq_40pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_40pct_PE2.fastq.gz\nH9_PRO-seq_50,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_50pct_PE1.fastq.gz,PAIRED,50% subset H9 PRO-seq 2,H9_PRO-seq_50,H9_PRO-seq_50pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_50pct_PE2.fastq.gz\nH9_PRO-seq_60,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_60pct_PE1.fastq.gz,PAIRED,60% subset H9 PRO-seq 2,H9_PRO-seq_60,H9_PRO-seq_60pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_60pct_PE2.fastq.gz\nH9_PRO-seq_70,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_70pct_PE1.fastq.gz,PAIRED,70% subset H9 PRO-seq 2,H9_PRO-seq_70,H9_PRO-seq_70pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_70pct_PE2.fastq.gz\nH9_PRO-seq_80,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_80pct_PE1.fastq.gz,PAIRED,80% subset H9 PRO-seq 2,H9_PRO-seq_80,H9_PRO-seq_80pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_80pct_PE2.fastq.gz\nH9_PRO-seq_90,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_90pct_PE1.fastq.gz,PAIRED,90% subset H9 PRO-seq 2,H9_PRO-seq_90,H9_PRO-seq_90pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_90pct_PE2.fastq.gz\nH9_PRO-seq_100,hg38,human,['$CODE/peppro/sample_pipeline_interface.yaml'],human_rDNA,PRO,/Users/mstolarczyk/H9_PRO-seq_100pct_PE1.fastq.gz,PAIRED,100% H9 PRO-seq 2,H9_PRO-seq_100,H9_PRO-seq_100pct,DMSO,8,/Users/mstolarczyk/H9_PRO-seq_100pct_PE2.fastq.gz\n</code></pre> <pre><code>\n</code></pre>"},{"location":"eido/code/demo/","title":"Python API usage","text":"<p>There are 3 validation functions in the public <code>eido</code> package interface:</p> <ul> <li><code>validate_project</code> to validate the entire PEP</li> <li><code>validate_sample</code> to validate only a selected sample</li> <li><code>validate_config</code> to validate only the config part of the PEP</li> </ul> <p>Additionally there is a <code>read_schema</code> function that lets you read the schema.</p>"},{"location":"eido/code/demo/#schema-reading","title":"Schema reading","text":"<p>As noted above <code>read_schema</code> function can be used to read a YAML-formatted schema to Python. Depending on the class of the argument used, it will get a remote schema (argument is a URL) or will read one from disk (argument is a path).</p>"},{"location":"eido/code/demo/#remote","title":"Remote","text":"<pre><code>from eido import *\n\nread_schema(\"https://schema.databio.org/pep/2.0.0.yaml\")\n</code></pre> <pre><code>[{'description': 'Schema for a minimal PEP',\n  'version': '2.0.0',\n  'properties': {'config': {'properties': {'name': {'type': 'string',\n      'pattern': '^\\\\S*$',\n      'description': 'Project name with no whitespace'},\n     'pep_version': {'description': 'Version of the PEP Schema this PEP follows',\n      'type': 'string'},\n     'sample_table': {'type': 'string',\n      'description': 'Path to the sample annotation table with one row per sample'},\n     'subsample_table': {'type': 'string',\n      'description': 'Path to the subsample annotation table with one row per subsample and sample_name attribute matching an entry in the sample table'},\n     'sample_modifiers': {'type': 'object',\n      'properties': {'append': {'type': 'object'},\n       'duplicate': {'type': 'object'},\n       'imply': {'type': 'array',\n        'items': {'type': 'object',\n         'properties': {'if': {'type': 'object'},\n          'then': {'type': 'object'}}}},\n       'derive': {'type': 'object',\n        'properties': {'attributes': {'type': 'array',\n          'items': {'type': 'string'}},\n         'sources': {'type': 'object'}}}},\n      'project_modifiers': {'type': 'object',\n       'properties': {'amend': {'description': 'Object overwriting original project attributes',\n         'type': 'object'},\n        'import': {'description': 'List of external PEP project config files to import',\n         'type': 'array',\n         'items': {'type': 'string'}}}}}},\n    'required': ['pep_version']},\n   'samples': {'type': 'array',\n    'items': {'type': 'object',\n     'properties': {'sample_name': {'type': 'string',\n       'pattern': '^\\\\S*$',\n       'description': 'Unique name of the sample with no whitespace'}},\n     'required': ['sample_name']}}},\n  'required': ['samples']}]\n</code></pre> <p>With this simple call the PEP2.0.0 schema was downloaded from a remote file server and read into a <code>dict</code> object in Python.</p>"},{"location":"eido/code/demo/#local","title":"Local","text":"<pre><code>read_schema(\"../tests/data/schemas/test_schema.yaml\")\n</code></pre> <pre><code>[{'description': 'test PEP schema',\n  'properties': {'dcc': {'type': 'object',\n    'properties': {'compute_packages': {'type': 'object'}}},\n   'samples': {'type': 'array',\n    'items': {'type': 'object',\n     'properties': {'sample_name': {'type': 'string'},\n      'protocol': {'type': 'string'},\n      'genome': {'type': 'string'}}}}},\n  'required': ['samples']}]\n</code></pre> <p>This time the schema was read from disk.</p>"},{"location":"eido/code/demo/#schema-imports","title":"Schema imports","text":"<p><code>eido</code> lets you import schemas. Schema importing is a very powerful tool to make a cascading system of schemas that will keep the individual building blocks clear and simple.</p> <p>To import a schema from within a schema one just needs to use an <code>imports</code> section somewhere in the schema. The section has to be a YAML list, for example:</p> <pre><code>imports:\n    - ../tests/data/schemas/test_schema.yaml\n    - https://schema.databio.org/pep/2.0.0.yaml\n</code></pre> <p>or </p> <pre><code>imports: [../tests/data/schemas/test_schema.yaml, https://schema.databio.org/pep/2.0.0.yaml]\n</code></pre> <p>This functionality is particularly useful when one wants to restrict an object that already has a remote schema defined for. For example, to restrict the type of one more sample attribute in a <code>Project</code> object (defined by PEP2.0.0 schema).</p> <pre><code>imports:\n    - https://schema.databio.org/pep/2.0.0.yaml\ndescription: \"Schema for a more restrictive PEP\"\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        my_numeric_attribute: \n          type: integer\n          minimum: 0\n          maximum: 1\n      required:\n        - my_numeric_attribute\nrequired:\n  - samples\n</code></pre> <p>PEPs to succesfully validate against this schema will need to fulfill all the generic PEP2.0.0 schema requirements and fulfill the new <code>my_numeric_attribute</code> requirement.</p>"},{"location":"eido/code/demo/#how-importing-works","title":"How importing works","text":"<p>The output of the <code>read_schema</code> function is always a <code>list</code> object. In case there are no imports in the read schema it's just a <code>list</code> of length 1. </p> <p>If there are import statements the <code>list</code> length reflects the number of schemas imported. Please note that the schemas can be imported recursively, which means that an imported schema can import more schemas. </p> <p>The order of the output list is meaningful:</p> <ol> <li>It reflects the order of importing in the \"schema dependency chain\"; the schema used in the <code>read_schema</code> call is always last in the output list.</li> <li>It reflects the order of enumerating schemas in the <code>imports</code> section; the order is preserved</li> </ol> <p>This in turn implies the order of the validation in the functions described in detail below.</p>"},{"location":"eido/code/demo/#entire-pep-validation","title":"Entire PEP validation","text":"<pre><code>from peppy import Project\n</code></pre> <p>Within Python the <code>validate_project</code> function can be used to perform the entire PEP validation. It requires <code>peppy.Project</code> object and either a path to the YAML schema file or a read schema (<code>dict</code>) as inputs.</p> <pre><code>p = Project(\"../tests/data/peps/test_cfg.yaml\")\nvalidate_project(project=p, schema=\"../tests/data/schemas/test_schema.yaml\")\n\nfrom eido.eido import load_yaml\n\ns = _load_yaml(\"../tests/data/schemas/test_schema.yaml\")\nvalidate_project(project=p, schema=s)\n</code></pre> <p>If a validation is successful, no message is printed. An unsuccessful one is signalized with a corresponding <code>jsonschema.exceptions.ValidationError</code></p> <pre><code>validate_project(project=p, schema=\"../tests/data/schemas/test_schema_invalid.yaml\")\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nValidationError                           Traceback (most recent call last)\n\n&lt;ipython-input-6-29fa9395c52f&gt; in &lt;module&gt;\n----&gt; 1 validate_project(project=p, schema=\"../tests/data/schemas/test_schema_invalid.yaml\")\n\n\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/eido/eido.py in validate_project(project, schema, exclude_case)\n    112     for schema_dict in schema_dicts:\n    113         project_dict = project.to_dict()\n--&gt; 114         _validate_object(project_dict, _preprocess_schema(schema_dict), exclude_case)\n    115         _LOGGER.debug(\"Project validation successful\")\n    116\n\n\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/eido/eido.py in _validate_object(object, schema, exclude_case)\n     93     \"\"\"\n     94     try:\n---&gt; 95         jsonschema.validate(object, schema)\n     96     except jsonschema.exceptions.ValidationError as e:\n     97         if not exclude_case:\n\n\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jsonschema/validators.py in validate(instance, schema, cls, *args, **kwargs)\n    932     error = exceptions.best_match(validator.iter_errors(instance))\n    933     if error is not None:\n--&gt; 934         raise error\n    935 \n    936\n\n\nValidationError: 'invalid' is a required property\n\nFailed validating 'required' in schema:\n    {'description': 'test PEP schema',\n     'properties': {'_samples': {'items': {'properties': {'genome': {'anyOf': [{'type': 'string'},\n                                                                               {'items': {'type': 'string'},\n                                                                                'type': 'array'}]},\n                                                          'protocol': {'anyOf': [{'type': 'string'},\n                                                                                 {'items': {'type': 'string'},\n                                                                                  'type': 'array'}]},\n                                                          'sample_name': {'anyOf': [{'type': 'string'},\n                                                                                    {'items': {'type': 'string'},\n                                                                                     'type': 'array'}]}},\n                                           'type': 'object'},\n                                 'type': 'array'},\n                    'dcc': {'properties': {'compute_packages': {'type': 'object'}},\n                            'type': 'object'},\n                    'invalid': {'type': 'string'}},\n     'required': ['_samples', 'invalid']}\n\nOn instance:\n    {'_config': {'name': 'test',\n                 'output_dir': 'test',\n                 'pep_version': '2.0.0',\n                 'sample_modifiers': {'append': {'organism': {'Homo sapiens': {'genome': 'hg38'}}}},\n                 'sample_table': '/Users/mstolarczyk/code/eido/tests/data/peps/test_sample_table.csv'},\n     '_config_file': '/Users/mstolarczyk/code/eido/tests/data/peps/test_cfg.yaml',\n     '_sample_df':   sample_name protocol genome\n    0  GSM1558746      GRO   hg38\n    1  GSM1480327      PRO   hg38,\n     '_sample_table':             genome                              organism protocol sample_name\n    sample_name                                                                  \n    GSM1558746    hg38  {'Homo sapiens': {'genome': 'hg38'}}      GRO  GSM1558746\n    GSM1480327    hg38  {'Homo sapiens': {'genome': 'hg38'}}      PRO  GSM1480327,\n     '_samples': [{'_attributes': ['sample_name', 'protocol', 'genome'],\n                   '_derived_cols_done': [],\n                   '_project': {'_config': {'name': 'test',\n                                            'output_dir': 'test',\n                                            'pep_version': '2.0.0',\n                                            'sample_modifiers': {'append': {'organism': {'Homo sapiens': {'genome': 'hg38'}}}},\n                                            'sample_table': '/Users/mstolarczyk/code/eido/tests/data/peps/test_sample_table.csv'},\n                                '_config_file': '/Users/mstolarczyk/code/eido/tests/data/peps/test_cfg.yaml',\n                                '_sample_df':   sample_name protocol genome\n    0  GSM1558746      GRO   hg38\n    1  GSM1480327      PRO   hg38,\n                                '_sample_table':             genome                              organism protocol sample_name\n    sample_name                                                                  \n    GSM1558746    hg38  {'Homo sapiens': {'genome': 'hg38'}}      GRO  GSM1558746\n    GSM1480327    hg38  {'Homo sapiens': {'genome': 'hg38'}}      PRO  GSM1480327,\n                                '_samples': &lt;Recursion on list with id=140711461083656&gt;,\n                                '_samples_touched': True,\n                                '_subsample_df': None,\n                                'description': None,\n                                'name': 'test',\n                                'sst_index': ['sample_name',\n                                              'subsample_name'],\n                                'st_index': 'sample_name'},\n                   'genome': 'hg38',\n                   'organism': PathExAttMap\n    Homo sapiens:\n      genome: hg38,\n                   'protocol': 'GRO',\n                   'sample_name': 'GSM1558746'},\n                  {'_attributes': ['sample_name', 'protocol', 'genome'],\n                   '_derived_cols_done': [],\n                   '_project': {'_config': {'name': 'test',\n                                            'output_dir': 'test',\n                                            'pep_version': '2.0.0',\n                                            'sample_modifiers': {'append': {'organism': {'Homo sapiens': {'genome': 'hg38'}}}},\n                                            'sample_table': '/Users/mstolarczyk/code/eido/tests/data/peps/test_sample_table.csv'},\n                                '_config_file': '/Users/mstolarczyk/code/eido/tests/data/peps/test_cfg.yaml',\n                                '_sample_df':   sample_name protocol genome\n    0  GSM1558746      GRO   hg38\n    1  GSM1480327      PRO   hg38,\n                                '_sample_table':             genome                              organism protocol sample_name\n    sample_name                                                                  \n    GSM1558746    hg38  {'Homo sapiens': {'genome': 'hg38'}}      GRO  GSM1558746\n    GSM1480327    hg38  {'Homo sapiens': {'genome': 'hg38'}}      PRO  GSM1480327,\n                                '_samples': &lt;Recursion on list with id=140711461083656&gt;,\n                                '_samples_touched': True,\n                                '_subsample_df': None,\n                                'description': None,\n                                'name': 'test',\n                                'sst_index': ['sample_name',\n                                              'subsample_name'],\n                                'st_index': 'sample_name'},\n                   'genome': 'hg38',\n                   'organism': PathExAttMap\n    Homo sapiens:\n      genome: hg38,\n                   'protocol': 'PRO',\n                   'sample_name': 'GSM1480327'}],\n     '_samples_touched': True,\n     '_subsample_df': None,\n     'description': None,\n     'name': 'test',\n     'sst_index': ['sample_name', 'subsample_name'],\n     'st_index': 'sample_name'}\n</code></pre>"},{"location":"eido/code/demo/#config-validation","title":"Config validation","text":"<p>Similarily, the config part of the PEP can be validated; the function inputs remain the same</p> <pre><code>validate_config(project=p, schema=\"../tests/data/schemas/test_schema.yaml\")\n</code></pre>"},{"location":"eido/code/demo/#sample-validation","title":"Sample validation","text":"<p>To validate a specific <code>peppy.Sample</code> object within a PEP, one needs to also specify the <code>sample_name</code> argument which can be the <code>peppy.Sample.name</code> attribute (<code>str</code>) or the ID of the sample (<code>int</code>)</p> <pre><code>validate_sample(\n    project=p, schema=\"../tests/data/schemas/test_schema.yaml\", sample_name=0\n)\n</code></pre>"},{"location":"eido/code/demo/#output-details","title":"Output details","text":"<p>As depicted above the error raised by the <code>jsonschema</code> package is very detailed. That's because the entire validated PEP is printed out for the user reference. Since it can get overwhelming in case of the multi sample PEPs each of the <code>eido</code> functions presented above privide a way to limit the output to just the general information indicating the unmet schema requirements</p> <pre><code>validate_project(\n    project=p,\n    schema=\"../tests/data/schemas/test_schema_invalid.yaml\",\n    exclude_case=True,\n)\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nValidationError                           Traceback (most recent call last)\n\n&lt;ipython-input-10-e51679763445&gt; in &lt;module&gt;\n      2     project=p,\n      3     schema=\"../tests/data/schemas/test_schema_invalid.yaml\",\n----&gt; 4     exclude_case=True,\n      5 )\n\n\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/eido/eido.py in validate_project(project, schema, exclude_case)\n    112     for schema_dict in schema_dicts:\n    113         project_dict = project.to_dict()\n--&gt; 114         _validate_object(project_dict, _preprocess_schema(schema_dict), exclude_case)\n    115         _LOGGER.debug(\"Project validation successful\")\n    116\n\n\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/eido/eido.py in _validate_object(object, schema, exclude_case)\n     97         if not exclude_case:\n     98             raise\n---&gt; 99         raise jsonschema.exceptions.ValidationError(e.message)\n    100 \n    101\n\n\nValidationError: 'invalid' is a required property\n</code></pre>"},{"location":"eido/code/plugin-api-docs/","title":"Built-in filters API","text":""},{"location":"eido/code/plugin-api-docs/#package-eido-documentation","title":"Package <code>eido</code> Documentation","text":"<p>Project configuration</p> <pre><code>def basic_pep_filter(p, **kwargs) -&gt; Dict[str, str]\n</code></pre> <p>Basic PEP filter, that does not convert the Project object.</p> <p>This filter can save the PEP representation to file, if kwargs include <code>path</code>.</p>"},{"location":"eido/code/plugin-api-docs/#parameters","title":"Parameters:","text":"<ul> <li><code>p</code> (<code>peppy.Project</code>):  a Project to run filter on</li> </ul> <pre><code>def yaml_pep_filter(p, **kwargs) -&gt; Dict[str, str]\n</code></pre> <p>YAML PEP filter, that returns Project object representation.</p> <p>This filter can save the YAML to file, if kwargs include <code>path</code>.</p>"},{"location":"eido/code/plugin-api-docs/#parameters_1","title":"Parameters:","text":"<ul> <li><code>p</code> (<code>peppy.Project</code>):  a Project to run filter on</li> </ul> <pre><code>def csv_pep_filter(p, **kwargs) -&gt; Dict[str, str]\n</code></pre> <p>CSV PEP filter, that returns Sample object representations</p> <p>This filter can save the CSVs to files, if kwargs include <code>sample_table_path</code> and/or <code>subsample_table_path</code>.</p>"},{"location":"eido/code/plugin-api-docs/#parameters_2","title":"Parameters:","text":"<ul> <li><code>p</code> (<code>peppy.Project</code>):  a Project to run filter on</li> </ul> <pre><code>def yaml_samples_pep_filter(p, **kwargs) -&gt; Dict[str, str]\n</code></pre> <p>YAML samples PEP filter, that returns only Sample object representations.</p> <p>This filter can save the YAML to file, if kwargs include <code>path</code>.</p>"},{"location":"eido/code/plugin-api-docs/#parameters_3","title":"Parameters:","text":"<ul> <li><code>p</code> (<code>peppy.Project</code>):  a Project to run filter on</li> </ul> <p>Version Information: <code>eido</code> v0.2.2, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"eido/code/python-api/","title":"Eido Python API","text":""},{"location":"eido/code/python-api/#package-eido-documentation","title":"Package <code>eido</code> Documentation","text":"<p>Project configuration</p>"},{"location":"eido/code/python-api/#class-eidovalidationerror","title":"Class <code>EidoValidationError</code>","text":"<p>Object was not validated successfully according to schema.</p> <pre><code>def __init__(self, message, errors_by_type)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <pre><code>def validate_project(project, schema)\n</code></pre> <p>Validate a project object against a schema</p>"},{"location":"eido/code/python-api/#parameters","title":"Parameters:","text":"<ul> <li><code>project</code> (<code>peppy.Project</code>):  a project object to validate</li> <li><code>schema</code> (<code>str | dict</code>):  schema dict to validate against or a path to onefrom the error. Useful when used ith large projects</li> </ul> <pre><code>def validate_sample(project, sample_name, schema)\n</code></pre> <p>Validate the selected sample object against a schema</p>"},{"location":"eido/code/python-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>project</code> (<code>peppy.Project</code>):  a project object to validate</li> <li><code>sample_name</code> (<code>str | int</code>):  name or index of the sample to validate</li> <li><code>schema</code> (<code>str | dict</code>):  schema dict to validate against or a path to one</li> </ul> <pre><code>def validate_config(project, schema)\n</code></pre> <p>Validate the config part of the Project object against a schema</p>"},{"location":"eido/code/python-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>project</code> (<code>peppy.Project</code>):  a project object to validate</li> <li><code>schema</code> (<code>str | dict</code>):  schema dict to validate against or a path to one</li> </ul> <pre><code>def read_schema(schema)\n</code></pre> <p>Safely read schema from YAML-formatted file.</p> <p>If the schema imports any other schemas, they will be read recursively.</p>"},{"location":"eido/code/python-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>schema</code> (<code>str | Mapping</code>):  path to the schema fileor schema in a dict form</li> </ul>"},{"location":"eido/code/python-api/#returns","title":"Returns:","text":"<ul> <li><code>list[dict]</code>:  read schemas</li> </ul>"},{"location":"eido/code/python-api/#raises","title":"Raises:","text":"<ul> <li><code>TypeError</code>:  if the schema arg is neither a Mapping nor a file path orif the 'imports' sections in any of the schemas is not a list</li> </ul> <pre><code>def inspect_project(p, sample_names=None, max_attr=10)\n</code></pre> <p>Print inspection info: Project or, if sample_names argument is provided, matched samples</p>"},{"location":"eido/code/python-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>p</code> (<code>peppy.Project</code>):  project to inspect</li> <li><code>sample_names</code> (<code>Iterable[str]</code>):  list of samples to inspect</li> <li><code>max_attr</code> (<code>int</code>):  max number of sample attributes to display</li> </ul> <pre><code>def get_available_pep_filters()\n</code></pre> <p>Get a list of available target formats</p>"},{"location":"eido/code/python-api/#returns_1","title":"Returns:","text":"<ul> <li><code>List[str]</code>:  a list of available formats</li> </ul> <pre><code>def convert_project(prj, target_format, plugin_kwargs=None)\n</code></pre> <p>Convert a <code>peppy.Project</code> object to a selected format</p>"},{"location":"eido/code/python-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>prj</code> (<code>peppy.Project</code>):  a Project object to convert</li> <li><code>plugin_kwargs</code> (<code>dict</code>):  kwargs to pass to the plugin function</li> <li><code>target_format</code> (<code>str</code>):  the format to convert the Project object to</li> </ul>"},{"location":"eido/code/python-api/#raises_1","title":"Raises:","text":"<ul> <li><code>EidoFilterError</code>:  if the requested filter is not defined</li> </ul> <pre><code>def validate_input_files(project, schemas, sample_name=None)\n</code></pre> <p>Determine which of the required and optional files are missing.</p> <p>The names of the attributes that are required and/or deemed as inputs are sourced from the schema, more specifically from <code>required_files</code> and <code>files</code> sections in samples section: - If any of the required files are missing, this function raises an error. - If any of the optional files are missing, the function raises a warning. Note, this function also performs Sample object validation with jsonschema.</p>"},{"location":"eido/code/python-api/#parameters_6","title":"Parameters:","text":"<ul> <li><code>project</code> (<code>peppy.Project</code>):  project that defines the samples to validate</li> <li><code>schema</code> (<code>str | dict</code>):  schema dict to validate against or a path to one</li> <li><code>sample_name</code> (<code>str | int</code>):  name or index of the sample to validate. If None,validate all samples in the project</li> </ul>"},{"location":"eido/code/python-api/#raises_2","title":"Raises:","text":"<ul> <li><code>PathAttrNotFoundError</code>:  if any required sample attribute is missing</li> </ul> <pre><code>def get_input_files_size(sample, schema)\n</code></pre> <p>Determine which of this Sample's required attributes/files are missing and calculate sizes of the files (inputs).</p> <p>The names of the attributes that are required and/or deemed as inputs are sourced from the schema, more specifically from required_input_attrs and input_attrs sections in samples section. Note, this function does perform actual Sample object validation with jsonschema.</p>"},{"location":"eido/code/python-api/#parameters_7","title":"Parameters:","text":"<ul> <li><code>sample</code> (<code>peppy.Sample</code>):  sample to investigate</li> <li><code>schema</code> (<code>list[dict] | str</code>):  schema dict to validate against or a path to one</li> </ul>"},{"location":"eido/code/python-api/#returns_2","title":"Returns:","text":"<ul> <li><code>dict</code>:  dictionary with validation data, i.e missing,required_inputs, all_inputs, input_file_size</li> </ul>"},{"location":"eido/code/python-api/#raises_3","title":"Raises:","text":"<ul> <li><code>ValidationError</code>:  if any required sample attribute is missing</li> </ul> <p>Version Information: <code>eido</code> v0.2.2, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"geofetch/","title":"GEOfetch","text":"<p><code>geofetch</code> is a command-line tool that downloads and organizes data and metadata from GEO and SRA. When given one or more GEO/SRA accessions, <code>geofetch</code> will:</p> <ul> <li>Download either raw or processed data from either SRA or GEO</li> <li>Produce a standardized PEP sample table. This makes it really easy to run looper-compatible pipelines on public datasets by handling data acquisition and metadata formatting and standardization for you.</li> <li>Prepare a project to run with sraconvert to convert SRA files into FASTQ files.</li> </ul> <p></p>"},{"location":"geofetch/#key-geofetch-advantages","title":"Key geofetch advantages:","text":"<ul> <li>Works with GEO and SRA metadata</li> <li>Combines samples from different projects</li> <li></li> <li>Standardizes output metadata</li> <li>Filters type and size of processed files (from GEO) before downloading them</li> <li>Easy to use</li> <li>Fast execution time</li> <li>Can search GEO to find relevant data</li> <li>Can be used either as a command-line tool or from within Python using an API</li> </ul>"},{"location":"geofetch/#quick-example","title":"Quick example","text":"<p><code>geofetch</code> runs on the command line. This command will download the raw data and metadata for the given GSE number.</p> <pre><code>geofetch -i GSE95654\n</code></pre> <p>You can add <code>--processed</code> if you want to download processed files from the given experiment.</p> <pre><code>geofetch -i GSE95654 --processed\n</code></pre> <p>You can add <code>--just-metadata</code> if you want to download metadata without the raw SRA files or processed GEO files.</p> <pre><code>geofetch -i GSE95654 --just-metadata\n</code></pre> <pre><code>geofetch -i GSE95654 --processed --just-metadata\n</code></pre> <p>\u2063Note: We ensure that GEOfetch is compatible with Unix, Linux, and MacOS.  However, due to dependencies, some features of GEOfetch may not be available on Windows.</p>"},{"location":"geofetch/#check-out-what-exactly-argument-you-want-to-use-to-download-data","title":"Check out what exactly argument you want to use to download data:","text":""},{"location":"geofetch/#new-features-available-in-geofetch-0110","title":"New features available in geofetch 0.11.0:","text":"<p>1) Now geofetch is available as Python API package. Geofetch can initialize peppy projects without downloading any soft files. Example:</p> <pre><code>from geofetch import Geofetcher\n\n# initiate Geofetcher with all necessary arguments:\ngeof = Geofetcher(processed=True, acc_anno=True, discard_soft=True)\n\n# get projects by providing as input GSE or file with GSEs\ngeof.get_projects(\"GSE160204\")\n</code></pre> <p>2) Now to find GSEs and save them to file you can use <code>Finder</code> - GSE finder tool:</p> <p><pre><code>from geofetch import Finder\n\n# initiate Finder (use filters if necessary)\nfind_gse = Finder(filters='bed')\n\n# get all projects that were found:\ngse_list = find_gse.get_gse_all()\n</code></pre> Find more information here: GSE Finder</p> <p>For more details, check out the usage reference, installation instructions, or head on over to the tutorial for raw data and tutorial for processed data for a detailed walkthrough.</p>"},{"location":"geofetch/changelog/","title":"Changelog","text":""},{"location":"geofetch/changelog/#0129-2025-12-01","title":"[0.12.9] -- 2025-12-01","text":"<ul> <li>Improved error handling</li> <li>Fixed incorrect return for processed series metadata</li> </ul>"},{"location":"geofetch/changelog/#0128-2025-07-08","title":"[0.12.8] -- 2025-07-08","text":"<ul> <li>Updated docs</li> <li>Fixed parsing nested items. [143]</li> <li>Added pypiper to requirements [142]</li> <li>Fixed white spaces in reference genome string [141]</li> <li>Added version in CLI help [135]</li> <li>Updated SRA convert to use looper &gt; 2.0.0 and fully automate process</li> </ul>"},{"location":"geofetch/changelog/#0127-2024-09-11","title":"[0.12.7] -- 2024-09-11","text":"<ul> <li>Updated Python support 3.13</li> <li>Cleaned code and Readme</li> </ul>"},{"location":"geofetch/changelog/#0126-2024-02-05","title":"[0.12.6] -- 2024-02-05","text":"<ul> <li>Updated support for Windows in Prefetch (Note: Some functionality may still be unavailable on Windows)</li> </ul>"},{"location":"geofetch/changelog/#0125-2023-11-29","title":"[0.12.5] -- 2023-11-29","text":"<ul> <li>Fixed bug, where description was not populated in PEP</li> </ul>"},{"location":"geofetch/changelog/#0124-2023-08-01","title":"[0.12.4] -- 2023-08-01","text":"<ul> <li>Fixed SRA convert</li> <li>Added how to convert SRA</li> </ul>"},{"location":"geofetch/changelog/#0123-2023-06-21","title":"[0.12.3] -- 2023-06-21","text":"<ul> <li>Fixed preserving order of project keys (#119)</li> </ul>"},{"location":"geofetch/changelog/#0122-2023-04-25","title":"[0.12.2] -- 2023-04-25","text":"<ul> <li>Added <code>max-prefetch-size</code> argument. #113</li> <li>Improved code and logger structure.</li> </ul>"},{"location":"geofetch/changelog/#0120-2023-03-27","title":"[0.12.0] -- 2023-03-27","text":"<ul> <li>Added functionality that saves gse metadata to config file</li> <li>Fixed description in initialization of pepy object</li> </ul>"},{"location":"geofetch/changelog/#0112-2022-12-25","title":"[0.11.2] -- 2022-12-25","text":"<ul> <li>Changed sample_name of PEP of processed files to file oriented</li> <li>Added <code>--max-soft-size</code> argument, that sets size limit of soft files</li> <li> <ul> <li>Added functionality that skips downloading GEO tables that are in soft files</li> </ul> </li> <li>Fixed bug of creating unwanted empty folders</li> <li>Fixed problem with missing data</li> </ul>"},{"location":"geofetch/changelog/#0111-2022-11-28","title":"[0.11.1] -- 2022-11-28","text":"<ul> <li>Fixed requirements file</li> <li>Fixed bug in expanding metadata list</li> <li>Fixed bug in metadata links</li> </ul>"},{"location":"geofetch/changelog/#0110-2022-10-26","title":"[0.11.0] -- 2022-10-26","text":"<ul> <li>Added initialization of peppy Project without saving any files (from within Python using an API)</li> <li>Added Finder (searching GSE tool)</li> <li>Added progress bar</li> <li>Switched way of saving soft files to request library</li> <li>Improved documentation</li> <li>Refactored code</li> <li>Added <code>--add-convert-modifier</code> flag</li> <li>fixed looper amendments in the config file</li> <li>Fixed special character bug in the config file</li> <li>Fixed None issue in config file</li> <li>Fixed saving raw peps bug</li> </ul>"},{"location":"geofetch/changelog/#0101-2022-08-04","title":"[0.10.1] -- 2022-08-04","text":"<ul> <li>Updated metadata fetching requests from SRA database</li> </ul>"},{"location":"geofetch/changelog/#0100-2022-07-07","title":"[0.10.0] -- 2022-07-07","text":"<ul> <li>Fixed subprocesses continuing to run during program interrupt.</li> <li>Fixed issues with compatibility with NCBI API</li> </ul>"},{"location":"geofetch/changelog/#090-2022-06-20","title":"[0.9.0] -- 2022-06-20","text":"<ul> <li>Updated <code>--pipeline-interface</code> argument that adds it in for looper. <code>--pipeline-interface</code> argument was divided into:  <code>--pipeline-samples</code> and <code>--pipeline-project</code>.</li> <li>Fixed empty sample_name error while creating PEP.</li> <li>Added <code>--discard-soft</code> argument.</li> <li>Added <code>--const-limit-project</code> argument.</li> <li>Added <code>--const-limit-discard</code> argument.</li> <li>Added <code>--attr-limit-truncate</code> argument.</li> <li>Added <code>\"--add-dotfile\"</code> argument.</li> <li>Disabled creating combined pep when flag <code>--acc-anno</code> is set.</li> <li>Improved finding and separating metadata keys and genome assembly information.</li> <li>Added standardization of column names by replacing characters to lowercase and spaces by underscore.</li> </ul>"},{"location":"geofetch/changelog/#080-2022-03-10","title":"[0.8.0] -- 2022-03-10","text":"<ul> <li>Added <code>--filter-size</code> argument.</li> <li>Added <code>--data-source</code> argument.</li> <li>Removed <code>--tar_re</code> argument.</li> <li>Added PEP for processed data.</li> <li>Updated regex filter (case-insensitive update).</li> <li>Changed way of downloading processed data (downloading each file separately).</li> <li>Fixed code errors.</li> <li>Separated sample and experiment processed data.</li> </ul>"},{"location":"geofetch/changelog/#070-2020-05-21","title":"[0.7.0] -- 2020-05-21","text":"<ul> <li>Fixed user interface for bam conversions</li> <li>Added regex filter for processed data filenames, which will also auto-extract from tar archives</li> <li>Updated output to PEP 2.0</li> <li>Added <code>--skip</code> argument</li> <li>Added more control over where to store results.</li> <li>Integrate <code>sraconvert</code> into geofetch package.</li> </ul>"},{"location":"geofetch/changelog/#060-2019-06-20","title":"[0.6.0] -- 2019-06-20","text":"<ul> <li>Fixed a bug with specifying a processed data output folder</li> <li>Added a pre-check and warning message for <code>prefetch</code> command </li> </ul>"},{"location":"geofetch/changelog/#050-2019-05-09","title":"[0.5.0] -- 2019-05-09","text":"<ul> <li><code>geofetch</code> will now re-try a failed prefetch 3 times and warn if unsuccessful.</li> <li>Fixed a bug that prevented writing metadata in python3.</li> <li>More robust SOFT line parsing</li> <li>Use <code>logmuse</code> for messaging</li> <li>Improve modularity to facilitate non-CLI use if desired</li> <li>Better documentation</li> </ul>"},{"location":"geofetch/changelog/#040-2019-03-13","title":"[0.4.0] -- (2019-03-13)","text":"<ul> <li>Fixed a bug with default generic config template</li> <li>Added <code>--version</code> option</li> <li>Improved python 3 compatibility</li> </ul>"},{"location":"geofetch/changelog/#020-2019-02-28","title":"[0.2.0] -- (2019-02-28)","text":"<ul> <li>Fixed bugs that prevented install from pypi</li> </ul>"},{"location":"geofetch/changelog/#010-2019-02-27","title":"[0.1.0] -- (2019-02-27)","text":"<ul> <li>First official release</li> <li>Enabled command-line usage</li> <li>Packaged <code>geofetch</code> for release on PyPI</li> </ul>"},{"location":"geofetch/changelog/#000-2017-10-24","title":"[0.0.0] -- (2017-10-24)","text":"<ul> <li>Legacy, unversioned development initiated</li> </ul>"},{"location":"geofetch/contributing/","title":"Contributing","text":"<p>Pull requests or issues are welcome.</p> <ul> <li>After adding tests in <code>tests</code> for a new feature or a bug fix, please run the test suite.</li> <li>To do so, the only additional dependencies needed beyond those for the package can be  installed with:</li> </ul> <p><code>pip install -r requirements/requirements-dev.txt</code></p> <ul> <li>Once those are installed, the tests can be run with <code>pytest</code>. Alternatively,  <code>python setup.py test</code> can be used.</li> </ul>"},{"location":"geofetch/faq/","title":"FAQ","text":""},{"location":"geofetch/faq/#i-get-an-error-geofetch-command-not-found-after-installing-why-isnt-the-geofetch-executable-in-my-path","title":"I get an error: <code>geofetch: command not found</code> after installing. Why isn't the <code>geofetch</code> executable in my path?","text":"<p>By default, Python packages are installed to <code>~/.local/bin</code>. You can add this location to your path by appending it:</p> <pre><code>export PATH=$PATH:~/.local/bin\n</code></pre> <p>Add this line to your <code>.bashrc</code> or <code>.profile</code> to make it permanent.</p>"},{"location":"geofetch/file-specification/","title":"How to specify samples to download","text":"<p>The command-line interface provides a way to give GSE or SRA accession IDs. By default, <code>geofetch</code> will download all the samples it can find in the accession you give it. What if you want to restrict the download to just a few samples? Or what if you want to combine samples from multiple accessions? If you want more control, either because you have multiple accessions or you want to specify a subset of samples, then you can use the file-based sample specification, in which you provide <code>geofetch</code> with a file listing your GSE/GSM accessions.</p>"},{"location":"geofetch/file-specification/#the-file-based-sample-specification","title":"The file-based sample specification","text":"<p>Create a file with 3 columns that correspond to <code>GSE</code>, <code>GSM</code>, and `Sample_name. You may mix 1, 2, and 3 column lines in the file. An example input file could look like this:</p> <pre><code>GSE123  GSM#### Sample1\nGSE123  GSM#### Sample2\nGSE123  GSM####\nGSE456\n</code></pre> <p>By default, <code>geofetch</code> will download all the samples in every included accession, but you can limit this by adding a second column with GSM accessions (which specify individual samples with a GSE dataset). If the second column is included, a third column may also be included and will be used as the sample_name; otherwise, the sample will be named according to the GEO Sample_title field. Any columns after the third will be ignored.</p> <p>This will download 3 particular GSM experiments from GSE123, and everything from GSE456. It will name the first two samples Sample1 and Sample2, and the third, plus any from GSE456, will have names according to GEO metadata.</p>"},{"location":"geofetch/gse-finder/","title":"GSE Finder","text":"<p>is a geofetch class that provides functions to find and retrieve a list of GSE (GEO accession number) by using NCBI searching tool.</p>"},{"location":"geofetch/gse-finder/#the-main-features-of-the-geofetch-finder-are","title":"The main features of the geofetch Finder are:","text":"<ul> <li>Find GEO accession numbers (GSE) of the project that were uploaded or updated in certain period of time.</li> <li>Use the same filter query as GEO DataSets Advanced Search Builder is using</li> <li>Save list of the GSEs to file (This file with geo can be used later in geofetch)</li> <li>Easier and faster to get GSEs using NCBI filter and certain period of time.</li> </ul>"},{"location":"geofetch/gse-finder/#tutorial","title":"Tutorial","text":"<p>0) Initial Finder object.  <pre><code>from geofetch import Finder\ngse_obj = Finder()\n\n# Optionally: provide filter string and max number of retrieve elements\ngse_obj = Finder(filters=\"((bed) OR narrow peak) AND Homo sapiens[Organism]\", retmax=10)\n</code></pre></p> <p>1) Get list of all GSE in GEO  <pre><code>gse_list =  gse_obj.get_gse_all()\n</code></pre></p> <p>2) Get list of GSE that were uploaded and updated last week <pre><code>gse_list = gse_obj.get_gse_last_week() \n</code></pre></p> <p>3) Get list of GSE that were uploaded and updated last 3 month <pre><code>gse_list = gse_obj.get_gse_last_3_month()\n</code></pre></p> <p>4) Get list of GSE that were uploaded and updated in las number of days <pre><code># project that were uploaded in last 5 days:\ngse_list = gse_obj.get_gse_by_day_count(5)\n</code></pre></p> <p>5) Get list of GSE that were uploaded in certain period of time <pre><code>gse_list = gse_obj.get_gse_by_date(start_date=\"2015/05/05\", end_date=\"2020/05/05\")\n</code></pre></p> <p>6) Save last searched list of items to the file <pre><code>gse_obj.generate_file(\"path/to/the/file\")\n\n# if you want to save different list of files you can provide it to the function\ngse_obj.generate_file(\"path/to/the/file\", gse_list=[\"123\", \"124\"])\n</code></pre></p> <p>7) Compare two lists: <pre><code>new_gse_list = gse_obj.find_differences(list1, list2)\n</code></pre></p> <p>More information about gse and queries and id: - https://www.ncbi.nlm.nih.gov/geo/info/geo_paccess.html - https://newarkcaptain.com/how-to-retrieve-ncbi-geo-information-using-apis-part1/ - https://www.ncbi.nlm.nih.gov/books/NBK3837/#EntrezHelp.Using_the_Advanced_Search_Pag</p>"},{"location":"geofetch/howto-location/","title":"Set SRA data download location","text":""},{"location":"geofetch/howto-location/#setting-data-download-location-with-sratools","title":"Setting data download location with <code>sratools</code>","text":"<p><code>geofetch</code> is using the sratoolkit to download raw data from SRA -- which means it's stuck with the default path for downloading SRA data, which I've written about. So before you run <code>geofetch</code>, make sure you have set up your download location to the correct place. In our group, we use a shared group environment variable called <code>${SRARAW}</code>, which points to a shared folder (<code>${DATA}/sra</code>) where the whole group has access to downloaded SRA data. You can point the <code>sratoolkit</code> (and therefore <code>geofetch</code>) to use that location with this one-time configuration code:</p> <pre><code># Set your $DATA environment variable\nexport DATA=\"/path/to/data/\"\n</code></pre> <pre><code>echo \"/repository/user/main/public/root = \\\"$DATA\\\"\" &gt; ${HOME}/.ncbi/user-settings.mkfg\n</code></pre> <p>Now <code>sratoolkit</code> will download data into an <code>/sra</code> folder in <code>${DATA}</code>, which is what <code>${SRARAW}</code> points to.</p> <p>If you are getting an error that the <code>.ncbi</code> folder does not exist in your home directory, you can just make a folder <code>.ncbi</code> with an empty file <code>user-settings.mkfg</code> and follow the same command above.</p>"},{"location":"geofetch/howto-prefetch/","title":"How to install <code>prefetch</code>","text":"<p>To install the prefetch tool, you need to install the NCBI SRA  Toolkit. Prefetch is a part of this toolkit and is used to download data from the Sequence Read Archive (SRA). The installation process varies depending on your operating system. </p>"},{"location":"geofetch/howto-prefetch/#documentation","title":"Documentation:","text":"<p>The best way to install the NCBI SRA Toolkit is to follow the official instructions:</p> <p>Official NCBI SRA Toolkit Installation Guide</p>"},{"location":"geofetch/install/","title":"Installing geofetch","text":""},{"location":"geofetch/install/#installing-geofetch_1","title":"Installing geofetch","text":"<p>Releases are posted as GitHub releases, or you can install from PyPI using <code>pip</code>:</p> <pre><code>pip install geofetch\n</code></pre> <p>Confirm it was successful by running it on the command line:</p> <pre><code>geofetch --help\n</code></pre> <p>If the executable in not in your $PATH, append this to your <code>.bashrc</code> or <code>.profile</code> (or <code>.bash_profile</code> on macOS):</p> <pre><code>export PATH=~/.local/bin:$PATH\n</code></pre>"},{"location":"geofetch/install/#prerequisites-for-sra-data-downloading","title":"Prerequisites for SRA data downloading","text":"<p>To download raw data You must have the sratoolkit from NCBI installed, with the tools in your PATH. Once it's installed, you should check to make sure you can run <code>prefetch</code>. Also, make sure it's configured to store SRA files where you want them. For more information, see how to change sratools download location.</p>"},{"location":"geofetch/install/#setting-data-download-location-for-sratools","title":"Setting data download location for <code>sratools</code>","text":"<p><code>geofetch</code> is using the sratoolkit to download raw data from SRA -- which means it's stuck with the default path for downloading SRA data, which is in your home directory. So before you run <code>geofetch</code>, make sure you have set up your download location to the correct place. In our group, we use a shared group environment variable called <code>${SRARAW}</code>, which points to a shared folder (<code>${DATA}/sra</code>) where the whole group has access to downloaded SRA data. You can point the <code>sratoolkit</code> (and therefore <code>geofetch</code>) to use that location with this one-time configuration code:</p> <pre><code>echo \"/repository/user/main/public/root = \\\"$DATA\\\"\" &gt; ${HOME}/.ncbi/user-settings.mkfg\n</code></pre> <p>Now <code>sratoolkit</code> will download data into an <code>/sra</code> folder in <code>${DATA}</code>, which is what <code>${SRARAW}</code> points to.</p> <p>If you are getting an error that the <code>.ncbi</code> folder does not exist in your home directory, you can just make a folder <code>.ncbi</code> with an empty file <code>user-settings.mkfg</code> and follow the same command above.</p>"},{"location":"geofetch/metadata-output/","title":"Metadata output","text":"<p>Geofetch produces PEPs for either processed or raw data (including metadata from SRA). A project can be created either for a single combined (whole) input or for each project separately.  (if <code>--acc-anno</code> is set). \"combined\" means that it will have rows for every sample in every GSE included  in your input. So if you just gave a single GSE, then the combined file is the same as the GSE file.</p> <p>For raw data: a metadata file will be created including SRA and GSM annotation.</p> <p>For processed data: a metadata file will be created just for GSE and GSM annotation. User can choose which data should he download. There are 3 downloading options for processed: samples, series and both.</p>"},{"location":"geofetch/metadata-output/#single-pep-will-contain","title":"Single PEP will contain:","text":"<ul> <li>project_name.csv - all metadata for sample processed data</li> <li>project_name_subannotation.csv (just for raw data) - for merged samples (samples for which there are multiple SRR Runs for a single SRX <code>Experiment</code>)</li> <li>project_name.yaml - project config file that stores all project information + common samples metadata</li> </ul> <p>Storing common metadata in project file is an efficient way to reduce project size and complexity of csv files.  To specify and manage common metadata (where and how it should be stored) you can use next arguments:  <code>--const-limit-project</code>, <code>--const-limit-discard</code>, <code>--attr-limit-truncate</code></p>"},{"location":"geofetch/metadata-output/#saving-actual-data","title":"Saving actual data:","text":"<p>Actual data will be saved if <code>--just-metadata</code> argument is not set. User should specify path to the folder where this data should be downloaded.</p> <p>Additionally, for each GSE input accession (ACC), <code>geofetch</code> produces (if discard-soft is not set):</p> <ul> <li>GSE_ACC####.soft a SOFT file (annotating the experiment itself)</li> <li>GSM_ACC####.soft a SOFT file (annotating the samples within the experiment)</li> <li>SRA_ACC####.soft a CSV file (annotating each SRA Run, retrieved from GSE-&gt;GSM-&gt;SRA)</li> </ul>"},{"location":"geofetch/metadata-output/#geofetch-geofetcher-using-python","title":"geofetch - Geofetcher using Python","text":"<p>user can use geofetch in Python without saving any files. All the geofetch projects will be automatically downloaded as peppy Project. It helps save time and processing work.</p> <p>THe output in this case will be dictionary of projects: <pre><code>{'key1': (some_project),\n 'key2': (second_project)}\n</code></pre></p> <p>More information you can find in tutorial files.</p>"},{"location":"geofetch/sra-convert/","title":"Sraconvert","text":"<p>When you install geofetch, you also get a second utility called <code>sraconvert</code> that handles converting sra data into either <code>bam</code> or <code>fastq</code> format for downstream processing. Sraconvert is essentially a wrapper around NCBI's sra-tools that provides more convenient interface to converting pre-downloaded <code>sra</code> files. </p> <p>The basic advantages over just using prefetch are:</p> <ul> <li>it provides the same interface to either download or delete sra files</li> <li>it uses the same interface to delete converted files, if desired</li> <li>it can automatically delete sra data that has been already converted</li> <li>it allows a more flexible specification of locations, using either environment variables or command-line arguments.</li> </ul> <p>This effectively makes it easier to interact with project-level management of sra and fastq data using looper and PEP-compatible projects.</p>"},{"location":"geofetch/sra-convert/#tutorial","title":"Tutorial","text":"<p>See the how-to SRA to FASTQ for an example of how to use <code>sraconvert</code>.</p>"},{"location":"geofetch/code/howto-sra-to-fastq/","title":"Run SRA convert","text":""},{"location":"geofetch/code/howto-sra-to-fastq/#how-to-extract-fastq-files-from-sra","title":"How to extract fastq files from SRA","text":"<ol> <li>Install geofetch</li> </ol> <pre><code>pip install geofetch\n</code></pre> <pre><code>Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: geofetch in /home/bnt4me/.local/lib/python3.10/site-packages (0.12.7)\nRequirement already satisfied: colorama&gt;=0.3.9 in /usr/lib/python3/dist-packages (from geofetch) (0.4.4)\nRequirement already satisfied: coloredlogs&gt;=15.0.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (15.0.1)\nRequirement already satisfied: logmuse&gt;=0.2.6 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (0.2.7)\nRequirement already satisfied: pandas&gt;=1.5.3 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (2.2.2)\nRequirement already satisfied: peppy&gt;=0.40.6 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (0.40.7)\nRequirement already satisfied: piper&gt;=0.14.4 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (0.14.4)\nRequirement already satisfied: requests&gt;=2.28.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (2.31.0)\nRequirement already satisfied: rich&gt;=12.5.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (13.7.1)\nRequirement already satisfied: ubiquerg&gt;=0.6.2 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (0.8.1)\nRequirement already satisfied: xmltodict&gt;=0.13.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from geofetch) (0.13.0)\nRequirement already satisfied: humanfriendly&gt;=9.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from coloredlogs&gt;=15.0.1-&gt;geofetch) (10.0)\nRequirement already satisfied: numpy&gt;=1.22.4 in /home/bnt4me/.local/lib/python3.10/site-packages (from pandas&gt;=1.5.3-&gt;geofetch) (1.25.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/bnt4me/.local/lib/python3.10/site-packages (from pandas&gt;=1.5.3-&gt;geofetch) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/lib/python3/dist-packages (from pandas&gt;=1.5.3-&gt;geofetch) (2022.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/bnt4me/.local/lib/python3.10/site-packages (from pandas&gt;=1.5.3-&gt;geofetch) (2023.3)\nRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peppy&gt;=0.40.6-&gt;geofetch) (5.4.1)\nRequirement already satisfied: pephubclient&gt;=0.4.2 in /home/bnt4me/.local/lib/python3.10/site-packages (from peppy&gt;=0.40.6-&gt;geofetch) (0.4.2)\nRequirement already satisfied: psutil in /home/bnt4me/.local/lib/python3.10/site-packages (from piper&gt;=0.14.4-&gt;geofetch) (5.9.4)\nRequirement already satisfied: yacman&gt;=0.9.3 in /home/bnt4me/.local/lib/python3.10/site-packages (from piper&gt;=0.14.4-&gt;geofetch) (0.9.3)\nRequirement already satisfied: pipestat&gt;=0.11.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from piper&gt;=0.14.4-&gt;geofetch) (0.12.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/bnt4me/.local/lib/python3.10/site-packages (from requests&gt;=2.28.1-&gt;geofetch) (3.0.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests&gt;=2.28.1-&gt;geofetch) (3.3)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from requests&gt;=2.28.1-&gt;geofetch) (1.26.18)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python3/dist-packages (from requests&gt;=2.28.1-&gt;geofetch) (2020.6.20)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from rich&gt;=12.5.1-&gt;geofetch) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from rich&gt;=12.5.1-&gt;geofetch) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=12.5.1-&gt;geofetch) (0.1.2)\nRequirement already satisfied: typer&gt;=0.7.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from pephubclient&gt;=0.4.2-&gt;peppy&gt;=0.40.6-&gt;geofetch) (0.9.4)\nRequirement already satisfied: pydantic&gt;2.5.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from pephubclient&gt;=0.4.2-&gt;peppy&gt;=0.40.6-&gt;geofetch) (2.7.3)\nRequirement already satisfied: jsonschema in /home/bnt4me/.local/lib/python3.10/site-packages (from pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (4.23.0)\nRequirement already satisfied: eido in /home/bnt4me/.local/lib/python3.10/site-packages (from pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (0.2.4)\nRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (3.0.3)\nRequirement already satisfied: six&gt;=1.5 in /usr/lib/python3/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.5.3-&gt;geofetch) (1.16.0)\nRequirement already satisfied: attmap&gt;=0.13.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from yacman&gt;=0.9.3-&gt;piper&gt;=0.14.4-&gt;geofetch) (0.13.2)\nRequirement already satisfied: oyaml in /home/bnt4me/.local/lib/python3.10/site-packages (from yacman&gt;=0.9.3-&gt;piper&gt;=0.14.4-&gt;geofetch) (1.0)\nRequirement already satisfied: attrs&gt;=22.2.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from jsonschema-&gt;pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (25.3.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /home/bnt4me/.local/lib/python3.10/site-packages (from jsonschema-&gt;pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (2025.4.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /home/bnt4me/.local/lib/python3.10/site-packages (from jsonschema-&gt;pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (0.36.2)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from jsonschema-&gt;pipestat&gt;=0.11.0-&gt;piper&gt;=0.14.4-&gt;geofetch) (0.24.0)\nRequirement already satisfied: annotated-types&gt;=0.4.0 in /home/bnt4me/.local/lib/python3.10/site-packages (from pydantic&gt;2.5.0-&gt;pephubclient&gt;=0.4.2-&gt;peppy&gt;=0.40.6-&gt;geofetch) (0.6.0)\nRequirement already satisfied: pydantic-core==2.18.4 in /home/bnt4me/.local/lib/python3.10/site-packages (from pydantic&gt;2.5.0-&gt;pephubclient&gt;=0.4.2-&gt;peppy&gt;=0.40.6-&gt;geofetch) (2.18.4)\nRequirement already satisfied: typing-extensions&gt;=4.6.1 in /home/bnt4me/.local/lib/python3.10/site-packages (from pydantic&gt;2.5.0-&gt;pephubclient&gt;=0.4.2-&gt;peppy&gt;=0.40.6-&gt;geofetch) (4.8.0)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/lib/python3/dist-packages (from typer&gt;=0.7.0-&gt;pephubclient&gt;=0.4.2-&gt;peppy&gt;=0.40.6-&gt;geofetch) (8.0.3)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -&gt; \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n</code></pre> <pre><code>geofetch --version\n</code></pre> <pre><code>geofetch 0.12.8\n</code></pre> <p>1) Download SRA files and PEP using GEOfetch</p> <p>Add flags: 1) <code>--add-convert-modifier</code> (To add looper configurations for conversion) 2) <code>--discard-soft</code> (To delete soft files. We don't need them :D)</p> <pre><code>geofetch -i GSE67303 -n red_algae -m `pwd` --add-convert-modifier --discard-soft\n</code></pre> <pre><code>\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:23]\u001b[0m Metadata folder: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m Trying GSE67303 (not a file) as accession...\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m Skipped 0 accessions. Starting now.\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE67303'\u001b[0m\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m Processed 4 samples.\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m Expanding metadata list...\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m Found SRA Project accession: SRP056574\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:24]\u001b[0m Downloading SRP056574 sra metadata\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:25]\u001b[0m Parsing SRA file to download SRR records\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:25]\u001b[0m Getting SRR: SRR1930183  in (GSE67303)\n\n2025-07-10T04:54:26 prefetch.2.11.3: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2025-07-10T04:54:26 prefetch.2.11.3: 1) Downloading 'SRR1930183'...\n2025-07-10T04:54:26 prefetch.2.11.3: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2025-07-10T04:54:26 prefetch.2.11.3:  Downloading via HTTPS...\n2025-07-10T04:54:31 prefetch.2.11.3:  HTTPS download succeed\n2025-07-10T04:54:31 prefetch.2.11.3:  'SRR1930183' is valid\n2025-07-10T04:54:31 prefetch.2.11.3: 1) 'SRR1930183' was downloaded successfully\n2025-07-10T04:54:31 prefetch.2.11.3: 'SRR1930183' has 0 unresolved dependencies\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:31]\u001b[0m Getting SRR: SRR1930184  in (GSE67303)\n\n2025-07-10T04:54:32 prefetch.2.11.3: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2025-07-10T04:54:32 prefetch.2.11.3: 1) Downloading 'SRR1930184'...\n2025-07-10T04:54:32 prefetch.2.11.3: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2025-07-10T04:54:32 prefetch.2.11.3:  Downloading via HTTPS...\n2025-07-10T04:54:36 prefetch.2.11.3:  HTTPS download succeed\n2025-07-10T04:54:36 prefetch.2.11.3:  'SRR1930184' is valid\n2025-07-10T04:54:36 prefetch.2.11.3: 1) 'SRR1930184' was downloaded successfully\n2025-07-10T04:54:36 prefetch.2.11.3: 'SRR1930184' has 0 unresolved dependencies\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:36]\u001b[0m Getting SRR: SRR1930185  in (GSE67303)\n\n2025-07-10T04:54:37 prefetch.2.11.3: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2025-07-10T04:54:37 prefetch.2.11.3: 1) Downloading 'SRR1930185'...\n2025-07-10T04:54:37 prefetch.2.11.3: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2025-07-10T04:54:37 prefetch.2.11.3:  Downloading via HTTPS...\n2025-07-10T04:54:45 prefetch.2.11.3:  HTTPS download succeed\n2025-07-10T04:54:45 prefetch.2.11.3:  'SRR1930185' is valid\n2025-07-10T04:54:45 prefetch.2.11.3: 1) 'SRR1930185' was downloaded successfully\n2025-07-10T04:54:45 prefetch.2.11.3: 'SRR1930185' has 0 unresolved dependencies\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:45]\u001b[0m Getting SRR: SRR1930186  in (GSE67303)\n\n2025-07-10T04:54:46 prefetch.2.11.3: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2025-07-10T04:54:46 prefetch.2.11.3: 1) Downloading 'SRR1930186'...\n2025-07-10T04:54:46 prefetch.2.11.3: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2025-07-10T04:54:46 prefetch.2.11.3:  Downloading via HTTPS...\n2025-07-10T04:54:52 prefetch.2.11.3:  HTTPS download succeed\n2025-07-10T04:54:52 prefetch.2.11.3:  'SRR1930186' is valid\n2025-07-10T04:54:52 prefetch.2.11.3: 1) 'SRR1930186' was downloaded successfully\n2025-07-10T04:54:52 prefetch.2.11.3: 'SRR1930186' has 0 unresolved dependencies\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m Finished processing 1 accession(s)\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m Cleaning soft files ...\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m Creating complete project annotation sheets and config file...\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m \u001b[92mSample annotation sheet: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/GSE67303_PEP/GSE67303_PEP_raw.csv . Saved!\u001b[0m\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m \u001b[92mFile has been saved successfully\u001b[0m\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m Config file: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/GSE67303_PEP/GSE67303_PEP.yaml\n\u001b[1;30m[INFO]\u001b[0m \u001b[32m[00:54:52]\u001b[0m Looper config file: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/looper_config.yaml\n</code></pre> <p>Let's see if files were downloaded:</p> <pre><code>ls\n</code></pre> <pre><code>\u001b[0m\u001b[01;34mfq_folder\u001b[0m                         raw-data-downloading.ipynb  \u001b[01;34mSRR1930185\u001b[0m\nhowto-sra-to-fastq.ipynb          \u001b[01;34mred_algae\u001b[0m                   \u001b[01;34mSRR1930186\u001b[0m\nprocessed-data-downloading.ipynb  \u001b[01;34mSRR1930183\u001b[0m\npython-usage.ipynb                \u001b[01;34mSRR1930184\u001b[0m\n</code></pre> <p>now let's check how does our config file looks like:</p> <pre><code>cat ./red_algae/GSE67303_PEP/GSE67303_PEP.yaml\n</code></pre> <pre><code># Autogenerated by geofetch\n\nname: GSE67303\npep_version: 2.1.0\nsample_table: GSE67303_PEP_raw.csv\n\n\"experiment_metadata\":\n  \"series_contact_address\": \"930 N University Ave\"\n  \"series_contact_city\": \"Ann Arbor\"\n  \"series_contact_country\": \"USA\"\n  \"series_contact_department\": \"Chemistry\"\n  \"series_contact_email\": \"mtardu@umich.edu\"\n  \"series_contact_institute\": \"University of Michigan\"\n  \"series_contact_laboratory\": \"Koutmou Lab\"\n  \"series_contact_name\": \"mehmet,,tardu\"\n  \"series_contact_state\": \"MI\"\n  \"series_contact_zip_postal_code\": \"48109\"\n  \"series_contributor\": \"Mehmet,,Tardu + Ugur,M,Dikbas + Ibrahim,,Baris + Ibrahim,H,Kavakli\"\n  \"series_geo_accession\": \"GSE67303\"\n  \"series_last_update_date\": \"May 15 2019\"\n  \"series_overall_design\": \"Identification of blue light and red light regulated genes\\\n    \\ by deep sequencing in biological duplicates. qRT-PCR was performed to verify\\\n    \\ the RNA-seq results.\"\n  \"series_platform_id\": \"GPL19949\"\n  \"series_platform_organism\": \"Cyanidioschyzon merolae strain 10D\"\n  \"series_platform_taxid\": \"280699\"\n  \"series_pubmed_id\": \"27614431\"\n  \"series_relation\": \"BioProject: https://www.ncbi.nlm.nih.gov/bioproject/PRJNA279462\\\n    \\ + SRA: https://www.ncbi.nlm.nih.gov/sra?term=SRP056574\"\n  \"series_sample_id\": \"GSM1644066 + GSM1644067 + GSM1644068 + GSM1644069\"\n  \"series_sample_organism\": \"Cyanidioschyzon merolae strain 10D\"\n  \"series_sample_taxid\": \"280699\"\n  \"series_status\": \"Public on Sep 01 2016\"\n  \"series_submission_date\": \"Mar 26 2015\"\n  \"series_summary\": \"Light is one of the main environmental cues that affects the\\\n    \\ physiology and behavior of many organisms. The effect of light on genome-wide\\\n    \\ transcriptional regulation has been well-studied in green algae and plants,\\\n    \\ but not in red algae. Cyanidioschyzon merolae is used as a model red algae,\\\n    \\ and is suitable for studies on transcriptomics because of its compact genome\\\n    \\ with a relatively small number of genes. In addition, complete genome sequences\\\n    \\ of the nucleus, mitochondrion, and chloroplast of this organism have been determined.\\\n    \\ Together, these attributes make C. merolae an ideal model organism to study\\\n    \\ the response to light stimuli at the transcriptional and the systems biology\\\n    \\ levels. Previous studies have shown that light significantly affects cell signaling\\\n    \\ in this organism, but there are no reports on its blue light- and red light-mediated\\\n    \\ transcriptional responses. We investigated the direct effects of blue and red\\\n    \\ light at the transcriptional level using RNA-seq. Blue and red light were found\\\n    \\ to regulate 35% of the total genes in C. merolae. Blue light affected the transcription\\\n    \\ of genes involved protein synthesis while red light specifically regulated the\\\n    \\ transcription of genes involved in photosynthesis and DNA repair. Blue or red\\\n    \\ light regulated genes involved in carbon metabolism and pigment biosynthesis.\\\n    \\ Overall, our data showed that red and blue light regulate the majority of the\\\n    \\ cellular, cell division, and repair processes in C. merolae.\"\n  \"series_supplementary_file\": \"ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl/GSE67303_DEG_cuffdiff.xlsx\"\n  \"series_title\": \"RNA-seq analysis of the transcriptional response to blue and red\\\n    \\ light in the extremophilic red alga, Cyanidioschyzon merolae\"\n  \"series_type\": \"Expression profiling by high throughput sequencing\"\n\n\n\nsample_modifiers:\n  append:\n    # Project metadata:\n    sample_treatment_protocol_ch1: \"Cells were exposed to blue-light (15 \u00b5mole m-2s-1) for 30 minutes\"\n    sample_growth_protocol_ch1: \"Cyanidioschyzon merolae cells were grown in 2xMA media\"\n    sample_extract_protocol_ch1: \"Dark kept and blue-light exposed C.merolae cells were removed and RNA was harvested using Trizol reagent. Illumina TruSeq RNA Sample Prep Kit (Cat#RS-122-2001) was used with 1 ug of total RNA for the construction of sequencing libraries., RNA libraries were prepared for sequencing using standard Illumina protocols\"\n    sample_data_processing: \"The purified cDNA library was sequenced on Illumina''s MiSeq sequencing platform following vendor''s instruction for running the instrument., Sequenced reads were trimmed for adaptor sequence, and masked for low-complexity or low-quality sequence, then mapped to Cyanidioschyzon merolae 10D reference genome (assembly ID:ASM9120v1) using TopHat (v2.0.5)., Differential expression analysis was conducted by using cuffdiff tool in cufflink suite (v2.2)\"\n    supplementary_files_format_and_content: \"Excel spreadsheet includes FPKM values for Darkness and Blue-Light exposed samples with p and q values of cuffdiff output.\"\n    # End of project metadata\n\n\n    # Adding sra convert looper pipeline\n    SRR_files: SRA\n\n  derive:\n    attributes: [read1, read2, SRR_files]\n    sources:\n      SRA: \"${SRARAW}/{srr}/{srr}.sra\"\n      FQ: \"${SRAFQ}/{srr}.fastq.gz\"\n      FQ1: \"${SRAFQ}/{srr}_1.fastq.gz\"\n      FQ2: \"${SRAFQ}/{srr}_2.fastq.gz\"\n  imply:\n    - if:\n        organism: \"Mus musculus\"\n      then:\n        genome: mm10\n    - if:\n        organism: \"Homo sapiens\"\n      then:\n        genome: hg38\n    - if:\n        read_type: \"PAIRED\"\n      then:\n        read1: FQ1\n        read2: FQ2\n    - if:\n        read_type: \"SINGLE\"\n      then:\n        read1: FQ1\n</code></pre> <p>To run pipeline, you should set up few enviromental variables: 1) SRARAW - folder where SRA files were downloaded 2) SRAFQ -folder where fastq should be produced 3) CODE - (first you should clone geofetch), and $CODE is where geofetch folder is located</p> <pre><code># Set SRARAW env\nexport SRARAW=`pwd`\n</code></pre> <pre><code># Create folder where you want to store fq\nmkdir fq_folder\n</code></pre> <pre><code># Set SRAFQ env\nexport SRAFQ=`pwd`/fq_folder\n</code></pre>"},{"location":"geofetch/code/howto-sra-to-fastq/#now-install-looper-if-you-dont-have-it","title":"Now install looper if you don't have it","text":"<pre><code># pip install looper\n</code></pre> <pre><code>looper --version\n</code></pre> <pre><code>2.0.1\n\u001b[0m\n</code></pre> <p>Let's check where is looper config file and whats inside:</p> <pre><code>ls red_algae\n</code></pre> <pre><code>\u001b[0m\u001b[01;34mGSE67303_PEP\u001b[0m  looper_config.yaml  \u001b[01;34moutput_dir\u001b[0m\n</code></pre> <pre><code>cat red_algae/looper_config.yaml\n</code></pre> <pre><code>pep_config: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/GSE67303_PEP/GSE67303_PEP.yaml\noutput_dir: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir\npipeline_interfaces:\n  - /home/bnt4me/.local/lib/python3.10/site-packages/geofetch/templates/pipeline_interface_convert.yaml\n</code></pre> <p>Geofetch automatically generated paths to pep_config and pipeline interfaces that are embedded into geofetch</p> <pre><code>looper run --config ./red_algae/looper_config.yaml -p local --output-dir .\n</code></pre> <pre><code>Looper version: 2.0.1\nCommand: run\nUsing default divvy config. You may specify in env var: ['DIVCFG']\nActivating compute package 'local'\n\u001b[36m## [1 of 4] sample: cm_bluelight_rep1; pipeline: sra_convert\u001b[0m\nWriting script to /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_bluelight_rep1.sub\nJob script (n=1; 0.00Gb): /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_bluelight_rep1.sub\nCompute node: alex-laptop\nStart time: 2025-07-10 00:59:02\nUsing outfolder: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930183\nNo pipestat output schema was supplied to PipestatManager.\nInitializing results file '/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930183/stats.yaml'\n### Pipeline run code and environment:\n\n*          Command: `/home/bnt4me/.local/bin/sraconvert --srr /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930183/SRR1930183.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*     Compute host: `alex-laptop`\n*      Working dir: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*        Outfolder: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930183/`\n*         Log file: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930183/sra_convert_log.md`\n*       Start time:  (07-10 00:59:03) elapsed: 0.0 _TIME_\n\n### Version log:\n\n*   Python version: `3.10.12`\n*      Pypiper dir: `/home/bnt4me/.local/lib/python3.10/site-packages/pypiper`\n*  Pypiper version: `0.14.4`\n*     Pipeline dir: `/home/bnt4me/.local/bin`\n* Pipeline version:\n\n### Arguments passed to pipeline:\n\n*          `bamfolder`:  ``\n*        `config_file`:  `sraconvert.yaml`\n*             `format`:  `fastq`\n*           `fqfolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder`\n*           `keep_sra`:  `False`\n*             `logdev`:  `False`\n*               `mode`:  `convert`\n*      `output_parent`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*            `recover`:  `False`\n*        `sample_name`:  `None`\n*             `silent`:  `False`\n*          `srafolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*                `srr`:  `['/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930183/SRR1930183.sra']`\n*          `verbosity`:  `None`\n\n### Initialized Pipestat Object:\n\n* PipestatManager (sra_convert)\n* Backend: File\n*  - results: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930183/stats.yaml\n*  - status: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930183\n* Multiple Pipelines Allowed: False\n* Pipeline name: sra_convert\n* Pipeline type: sample\n* Status Schema key: None\n* Results formatter: default_formatter\n* Results schema source: None\n* Status schema source: None\n* Records count: 2\n* Sample name: DEFAULT_SAMPLE_NAME\n\n\n----------------------------------------\n\nProcessing 1 of 1 files: SRR1930183\nTarget to produce: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder/SRR1930183_1.fastq.gz`\n\n&gt; `fasterq-dump /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930183/SRR1930183.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder` (871210)\n&lt;pre&gt;\nspots read      : 1,068,319\nreads read      : 2,136,638\nreads written   : 2,136,638\n&lt;/pre&gt;\nCommand completed. Elapsed time: 0:00:02. Running peak memory: 0.069GB.  \n  PID: 871210;  Command: fasterq-dump;  Return code: 0; Memory used: 0.069GB\n\nAlready completed files: []\n\n### Pipeline completed. Epilogue\n*        Elapsed time (this run):  0:00:02\n*  Total elapsed time (all runs):  0:00:02\n*         Peak memory (this run):  0.0685 GB\n*        Pipeline completed time: 2025-07-10 00:59:05\nUsing default schema: /home/bnt4me/.local/bin/pipestat_output_schema.yaml\n\u001b[36m## [2 of 4] sample: cm_bluelight_rep2; pipeline: sra_convert\u001b[0m\nWriting script to /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_bluelight_rep2.sub\nJob script (n=1; 0.00Gb): /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_bluelight_rep2.sub\nCompute node: alex-laptop\nStart time: 2025-07-10 00:59:06\nUsing outfolder: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930184\nNo pipestat output schema was supplied to PipestatManager.\nInitializing results file '/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930184/stats.yaml'\n### Pipeline run code and environment:\n\n*          Command: `/home/bnt4me/.local/bin/sraconvert --srr /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930184/SRR1930184.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*     Compute host: `alex-laptop`\n*      Working dir: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*        Outfolder: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930184/`\n*         Log file: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930184/sra_convert_log.md`\n*       Start time:  (07-10 00:59:06) elapsed: 0.0 _TIME_\n\n### Version log:\n\n*   Python version: `3.10.12`\n*      Pypiper dir: `/home/bnt4me/.local/lib/python3.10/site-packages/pypiper`\n*  Pypiper version: `0.14.4`\n*     Pipeline dir: `/home/bnt4me/.local/bin`\n* Pipeline version:\n\n### Arguments passed to pipeline:\n\n*          `bamfolder`:  ``\n*        `config_file`:  `sraconvert.yaml`\n*             `format`:  `fastq`\n*           `fqfolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder`\n*           `keep_sra`:  `False`\n*             `logdev`:  `False`\n*               `mode`:  `convert`\n*      `output_parent`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*            `recover`:  `False`\n*        `sample_name`:  `None`\n*             `silent`:  `False`\n*          `srafolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*                `srr`:  `['/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930184/SRR1930184.sra']`\n*          `verbosity`:  `None`\n\n### Initialized Pipestat Object:\n\n* PipestatManager (sra_convert)\n* Backend: File\n*  - results: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930184/stats.yaml\n*  - status: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930184\n* Multiple Pipelines Allowed: False\n* Pipeline name: sra_convert\n* Pipeline type: sample\n* Status Schema key: None\n* Results formatter: default_formatter\n* Results schema source: None\n* Status schema source: None\n* Records count: 2\n* Sample name: DEFAULT_SAMPLE_NAME\n\n\n----------------------------------------\n\nProcessing 1 of 1 files: SRR1930184\nTarget to produce: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder/SRR1930184_1.fastq.gz`\n\n&gt; `fasterq-dump /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930184/SRR1930184.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder` (871261)\n&lt;pre&gt;\nspots read      : 762,229\nreads read      : 1,524,458\nreads written   : 1,524,458\n&lt;/pre&gt;\nCommand completed. Elapsed time: 0:00:02. Running peak memory: 0.083GB.  \n  PID: 871261;  Command: fasterq-dump;  Return code: 0; Memory used: 0.083GB\n\nAlready completed files: []\n\n### Pipeline completed. Epilogue\n*        Elapsed time (this run):  0:00:02\n*  Total elapsed time (all runs):  0:00:02\n*         Peak memory (this run):  0.0832 GB\n*        Pipeline completed time: 2025-07-10 00:59:08\n\n\nUsing default schema: /home/bnt4me/.local/bin/pipestat_output_schema.yaml\n\u001b[36m## [3 of 4] sample: cm_darkness_rep1; pipeline: sra_convert\u001b[0m\nWriting script to /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_darkness_rep1.sub\nJob script (n=1; 0.00Gb): /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_darkness_rep1.sub\nCompute node: alex-laptop\nStart time: 2025-07-10 00:59:08\nUsing outfolder: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930185\nNo pipestat output schema was supplied to PipestatManager.\nInitializing results file '/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930185/stats.yaml'\n### Pipeline run code and environment:\n\n*          Command: `/home/bnt4me/.local/bin/sraconvert --srr /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930185/SRR1930185.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*     Compute host: `alex-laptop`\n*      Working dir: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*        Outfolder: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930185/`\n*         Log file: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930185/sra_convert_log.md`\n*       Start time:  (07-10 00:59:09) elapsed: 0.0 _TIME_\n\n### Version log:\n\n*   Python version: `3.10.12`\n*      Pypiper dir: `/home/bnt4me/.local/lib/python3.10/site-packages/pypiper`\n*  Pypiper version: `0.14.4`\n*     Pipeline dir: `/home/bnt4me/.local/bin`\n* Pipeline version:\n\n### Arguments passed to pipeline:\n\n*          `bamfolder`:  ``\n*        `config_file`:  `sraconvert.yaml`\n*             `format`:  `fastq`\n*           `fqfolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder`\n*           `keep_sra`:  `False`\n*             `logdev`:  `False`\n*               `mode`:  `convert`\n*      `output_parent`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*            `recover`:  `False`\n*        `sample_name`:  `None`\n*             `silent`:  `False`\n*          `srafolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*                `srr`:  `['/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930185/SRR1930185.sra']`\n*          `verbosity`:  `None`\n\n### Initialized Pipestat Object:\n\n* PipestatManager (sra_convert)\n* Backend: File\n*  - results: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930185/stats.yaml\n*  - status: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930185\n* Multiple Pipelines Allowed: False\n* Pipeline name: sra_convert\n* Pipeline type: sample\n* Status Schema key: None\n* Results formatter: default_formatter\n* Results schema source: None\n* Status schema source: None\n* Records count: 2\n* Sample name: DEFAULT_SAMPLE_NAME\n\n\n----------------------------------------\n\nProcessing 1 of 1 files: SRR1930185\nTarget to produce: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder/SRR1930185_1.fastq.gz`\n\n&gt; `fasterq-dump /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930185/SRR1930185.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder` (871306)\n&lt;pre&gt;\nspots read      : 1,707,508\nreads read      : 3,415,016\nreads written   : 3,415,016\n&lt;/pre&gt;\nCommand completed. Elapsed time: 0:00:04. Running peak memory: 0.07GB.  \n  PID: 871306;  Command: fasterq-dump;  Return code: 0; Memory used: 0.07GB\n\nAlready completed files: []\n\n### Pipeline completed. Epilogue\n*        Elapsed time (this run):  0:00:04\n*  Total elapsed time (all runs):  0:00:04\n*         Peak memory (this run):  0.0701 GB\n*        Pipeline completed time: 2025-07-10 00:59:13\nUsing default schema: /home/bnt4me/.local/bin/pipestat_output_schema.yaml\n\u001b[36m## [4 of 4] sample: cm_darkness_rep2; pipeline: sra_convert\u001b[0m\nWriting script to /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_darkness_rep2.sub\nJob script (n=1; 0.00Gb): /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/submission/sra_convert_cm_darkness_rep2.sub\nCompute node: alex-laptop\nStart time: 2025-07-10 00:59:13\nUsing outfolder: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930186\nNo pipestat output schema was supplied to PipestatManager.\nInitializing results file '/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930186/stats.yaml'\n### Pipeline run code and environment:\n\n*          Command: `/home/bnt4me/.local/bin/sraconvert --srr /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930186/SRR1930186.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*     Compute host: `alex-laptop`\n*      Working dir: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*        Outfolder: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930186/`\n*         Log file: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930186/sra_convert_log.md`\n*       Start time:  (07-10 00:59:14) elapsed: 0.0 _TIME_\n\n### Version log:\n\n*   Python version: `3.10.12`\n*      Pypiper dir: `/home/bnt4me/.local/lib/python3.10/site-packages/pypiper`\n*  Pypiper version: `0.14.4`\n*     Pipeline dir: `/home/bnt4me/.local/bin`\n* Pipeline version:\n\n### Arguments passed to pipeline:\n\n*          `bamfolder`:  ``\n*        `config_file`:  `sraconvert.yaml`\n*             `format`:  `fastq`\n*           `fqfolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder`\n*           `keep_sra`:  `False`\n*             `logdev`:  `False`\n*               `mode`:  `convert`\n*      `output_parent`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline`\n*            `recover`:  `False`\n*        `sample_name`:  `None`\n*             `silent`:  `False`\n*          `srafolder`:  `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks`\n*                `srr`:  `['/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930186/SRR1930186.sra']`\n*          `verbosity`:  `None`\n\n### Initialized Pipestat Object:\n\n* PipestatManager (sra_convert)\n* Backend: File\n*  - results: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930186/stats.yaml\n*  - status: /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/red_algae/output_dir/results_pipeline/SRR1930186\n* Multiple Pipelines Allowed: False\n* Pipeline name: sra_convert\n* Pipeline type: sample\n* Status Schema key: None\n* Results formatter: default_formatter\n* Results schema source: None\n* Status schema source: None\n* Records count: 2\n* Sample name: DEFAULT_SAMPLE_NAME\n\n\n----------------------------------------\n\nProcessing 1 of 1 files: SRR1930186\nTarget to produce: `/home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder/SRR1930186_1.fastq.gz`\n\n&gt; `fasterq-dump /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/SRR1930186/SRR1930186.sra -O /home/bnt4me/virginia/repos/pepspec/docs/geofetch/notebooks/fq_folder` (871369)\n&lt;pre&gt;\nspots read      : 1,224,029\nreads read      : 2,448,058\nreads written   : 2,448,058\n&lt;/pre&gt;\nCommand completed. Elapsed time: 0:00:02. Running peak memory: 0.083GB.  \n  PID: 871369;  Command: fasterq-dump;  Return code: 0; Memory used: 0.083GB\n\nAlready completed files: []\n\n### Pipeline completed. Epilogue\n*        Elapsed time (this run):  0:00:02\n*  Total elapsed time (all runs):  0:00:02\n*         Peak memory (this run):  0.0832 GB\n*        Pipeline completed time: 2025-07-10 00:59:16\nUsing default schema: /home/bnt4me/.local/bin/pipestat_output_schema.yaml\n\n\n\nLooper finished\nSamples valid for job generation: 4 of 4\n\u001b[0m\n</code></pre>"},{"location":"geofetch/code/howto-sra-to-fastq/#check-if-everything-worked","title":"Check if everything worked:","text":"<pre><code>cd fq_folder\n</code></pre> <pre><code>ls\n</code></pre> <pre><code>SRR1930183_1.fastq  SRR1930184_1.fastq  SRR1930185_1.fastq  SRR1930186_1.fastq\nSRR1930183_2.fastq  SRR1930184_2.fastq  SRR1930185_2.fastq  SRR1930186_2.fastq\n</code></pre> <p>Everything was executed sucessfully and SRA files were converted into fastq files</p>"},{"location":"geofetch/code/processed-data-downloading/","title":"geofetch tutorial for processed data","text":"<p>The GSE185701 data set has about 355 Mb of processed data that contains 57 Supplementary files, so it's a quick download for a test case. Let's take a quick peek at the geofetch version:</p> <pre><code>geofetch --version\n</code></pre> <pre><code>geofetch 0.10.1\n</code></pre> <p>To see your CLI options, invoke <code>geofetch -h</code>:</p> <p>Calling geofetch will do 4 tasks: </p> <ol> <li>download all or filtered processed files from <code>GSE#####</code> into your geo folder.</li> <li>download all metadata from GEO and store in your metadata folder.</li> <li>produce a PEP-compatible sample table, <code>PROJECT_NAME_sample_processed.csv</code> and <code>PROJECT_NAME_series_processed.csv</code>, in your metadata folder.</li> <li>produce a PEP-compatible project configuration file, <code>PROJECT_NAME_sample_processed.yaml</code> and <code>PROJECT_NAME_series_processed.yaml</code>, in your metadata folder.</li> </ol> <p>Complete details about geofetch outputs is cataloged in the metadata outputs reference.</p> <p>from IPython.core.display import SVG SVG(filename='logo.svg')</p> <p></p>"},{"location":"geofetch/code/processed-data-downloading/#download-the-data","title":"Download the data","text":"<p>First, create the metadata for processed data (by adding --processed and --just-metadata):</p> <pre><code>geofetch -i GSE185701 --processed -n bright_test --just-metadata\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test\nTrying GSE185701 (not a file) as accession...\nSkipped 0 accessions. Starting now.\n\u001b[38;5;200mProcessing accession 1 of 1: 'GSE185701'\u001b[0m\n--2022-07-08 12:34:57--  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gse&amp;acc=GSE185701&amp;form=text&amp;view=full\nResolving www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)... 2607:f220:41e:4290::110, 130.14.29.110\nConnecting to www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)|2607:f220:41e:4290::110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [geo/text]\nSaving to: \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/GSE185701_GSE.soft\u2019\n\n/home/bnt4me/Virgin     [ &lt;=&gt;                ]   2.82K  --.-KB/s    in 0s\n\n2022-07-08 12:34:57 (973 MB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/GSE185701_GSE.soft\u2019 saved [2885]\n\n--2022-07-08 12:34:57--  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gsm&amp;acc=GSE185701&amp;form=text&amp;view=full\nResolving www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)... 2607:f220:41e:4290::110, 130.14.29.110\nConnecting to www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)|2607:f220:41e:4290::110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [geo/text]\nSaving to: \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/GSE185701_GSM.soft\u2019\n\n/home/bnt4me/Virgin     [  &lt;=&gt;               ]  39.51K   132KB/s    in 0.3s\n\n2022-07-08 12:34:58 (132 KB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/GSE185701_GSM.soft\u2019 saved [40454]\n\n\u001b[38;5;242m\n--2022-07-08 12:34:58--  ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE185nnn/GSE185701/suppl/filelist.txt\n           =&gt; \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/GSE185701_file_list.txt\u2019\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 2607:f220:41e:250::10, 2607:f220:41e:250::7, 165.112.9.229, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|2607:f220:41e:250::10|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /geo/series/GSE185nnn/GSE185701/suppl ... done.\n==&gt; SIZE filelist.txt ... 794\n==&gt; EPSV ... done.    ==&gt; RETR filelist.txt ... done.\nLength: 794 (unauthoritative)\n\nfilelist.txt        100%[===================&gt;]     794  --.-KB/s    in 0s\n\n2022-07-08 12:34:58 (219 MB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/GSE185701_file_list.txt\u2019 saved [794]\n\n\u001b[38;5;242m0\u001b[0m\n\u001b[0m\nTotal number of processed SAMPLES files found is: 8\nTotal number of processed SERIES files found is: 1\nExpanding metadata list...\nExpanding metadata list...\nFinished processing 1 accession(s)\nUnifying and saving of metadata... \n\u001b[92mFile /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/PEP_samples/GSE185701_samples.csv has been saved successfully\u001b[0m\n  Config file: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/bright_test/PEP_samples/GSE185701_samples.yaml\n</code></pre> <pre><code>ls bright_test\n</code></pre> <pre><code>GSE185701_file_list.txt  GSE185701_GSE.soft  GSE185701_GSM.soft  \u001b[0m\u001b[01;34mPEP_samples\u001b[0m\n</code></pre> <p>The <code>.soft</code> files are the direct output from GEO, which contain all the metadata as stored by GEO, for both the experiment (<code>_GSE</code>) and for the individual samples (<code>_GSM</code>). Geofetch also produces a <code>csv</code> file with the SRA metadata. The filtered version (ending in <code>_filt</code>) would contain only the specified subset of the samples if we didn't request them all, but in this case, since we only gave an accession, it is identical to the complete file. Additionally, file_list.txt is downloaded, that contains information about size, type and creation date of all sample files.</p> <p>Finally, there are the 2 files that make up the PEP: the <code>_config.yaml</code> file and the <code>_annotation.csv</code> file (for samples and series). Let's see what's in these files now.</p> <pre><code>cat bright_test/PEP_samples/GSE185701_samples.yaml\n</code></pre> <pre><code># Autogenerated by geofetch\n\npep_version: 2.1.0\nproject_name: GSE185701\nsample_table: GSE185701_samples.csv\n\nsample_modifiers:\n  append:\n    output_file_path: FILES\n    sample_growth_protocol_ch1: Huh 7 was cultured in Dulbecco\u2019s modified Eagle\u2019s medium (DMEM) (Invitrogen, Carlsbad, CA, USA) containing 10% fetal bovine serum (FBS) (HyClone, Logan, UT, USA) and antibiotics (penicillin and streptomycin, Invitrogen) at 37 \u00b0C in 5% CO2.\n\n  derive:\n    attributes: [output_file_path]\n    sources:\n      FILES: /{gse}/{file}\n</code></pre> <p>There are few important things to note in this file:</p> <ul> <li>First, see in the PEP that <code>sample_table</code> points to the csv file produced by geofetch.</li> <li>Second: output_file_path is location of all the files. </li> <li>Third: sample_modifier Sample_growth_protocol_ch1 is constant sample character and is larger then 50 characters so it is deleted from csv file. For large project it can significantly reduced size of the metadata</li> </ul> <p>Now let's look at the first 100 characters of the csv file:</p> <pre><code>cut -c -100 bright_test/PEP_samples/GSE185701_samples.csv\n</code></pre> <pre><code>sample_taxid_ch1,sample_geo_accession,sample_channel_count,sample_instrument_model,biosample,supplem\n9606,GSM5621756,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223730,wig files were gen\n9606,GSM5621756,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223730,wig files were gen\n9606,GSM5621758,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223732,wig files were gen\n9606,GSM5621758,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223732,wig files were gen\n9606,GSM5621760,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223728,wig files were gen\n9606,GSM5621760,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223728,wig files were gen\n9606,GSM5621761,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223729,wig files were gen\n9606,GSM5621761,1,HiSeq X Ten,https://www.ncbi.nlm.nih.gov/biosample/SAMN22223729,wig files were gen\n</code></pre> <p>Now let's download the actual data. This time we will will be downloading data from the GSE185701 data set .</p> <p>Let's additionally add few arguments:</p> <ul> <li>geo-folder (required) - path to the location where processed files have to be saved</li> <li>filter argument, to download only bed files  (--filter \".Bed.gz$\")</li> <li>data-source argument, to download files only from sample location (--data-source samples)</li> </ul> <pre><code>geofetch -i GSE185701 --processed -n bright_test --filter \".bed.gz$\" --data-source samples \\\n--geo-folder /home/bnt4me/Virginia/for_docs/geo\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter\nTrying GSE185701 (not a file) as accession...\nSkipped 0 accessions. Starting now.\n\u001b[38;5;200mProcessing accession 1 of 1: 'GSE185701'\u001b[0m\n--2022-07-08 12:36:16--  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gse&amp;acc=GSE185701&amp;form=text&amp;view=full\nResolving www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)... 2607:f220:41e:4290::110, 130.14.29.110\nConnecting to www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)|2607:f220:41e:4290::110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [geo/text]\nSaving to: \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/GSE185701_GSE.soft\u2019\n\n/home/bnt4me/Virgin     [ &lt;=&gt;                ]   2.82K  --.-KB/s    in 0s\n\n2022-07-08 12:36:16 (245 MB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/GSE185701_GSE.soft\u2019 saved [2885]\n\n--2022-07-08 12:36:16--  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gsm&amp;acc=GSE185701&amp;form=text&amp;view=full\nResolving www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)... 2607:f220:41e:4290::110, 130.14.29.110\nConnecting to www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)|2607:f220:41e:4290::110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [geo/text]\nSaving to: \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/GSE185701_GSM.soft\u2019\n\n/home/bnt4me/Virgin     [ &lt;=&gt;                ]  39.51K  --.-KB/s    in 0.1s\n\n2022-07-08 12:36:16 (269 KB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/GSE185701_GSM.soft\u2019 saved [40454]\n\n\u001b[38;5;242m\n--2022-07-08 12:36:16--  ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE185nnn/GSE185701/suppl/filelist.txt\n           =&gt; \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/GSE185701_file_list.txt\u2019\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 2607:f220:41e:250::12, 2607:f220:41e:250::13, 130.14.250.13, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|2607:f220:41e:250::12|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /geo/series/GSE185nnn/GSE185701/suppl ... done.\n==&gt; SIZE filelist.txt ... 794\n==&gt; EPSV ... done.    ==&gt; RETR filelist.txt ... done.\nLength: 794 (unauthoritative)\n\nfilelist.txt        100%[===================&gt;]     794  --.-KB/s    in 0s\n\n2022-07-08 12:36:17 (2.55 MB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/GSE185701_file_list.txt\u2019 saved [794]\n\n\u001b[38;5;242m0\u001b[0m\n\u001b[0m\nTotal number of processed SAMPLES files found is: 8\n\u001b[32mTotal number of files after filter is: 4 \u001b[0m\nTotal number of processed SERIES files found is: 1\n\u001b[32mTotal number of files after filter is: 0 \u001b[0m\nExpanding metadata list...\nExpanding metadata list...\n\u001b[38;5;242m\n--2022-07-08 12:36:17--  ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM5621nnn/GSM5621756/suppl/GSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz\n           =&gt; \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz\u2019\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 2607:f220:41e:250::13, 2607:f220:41e:250::12, 165.112.9.229, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|2607:f220:41e:250::13|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /geo/samples/GSM5621nnn/GSM5621756/suppl ... done.\n==&gt; SIZE GSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz ... 785486\n==&gt; EPSV ... done.    ==&gt; RETR GSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz ... done.\nLength: 785486 (767K) (unauthoritative)\n\nGSM5621756_ChIPseq_ 100%[===================&gt;] 767.08K  1.64MB/s    in 0.5s\n\n2022-07-08 12:36:19 (1.64 MB/s) - \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz\u2019 saved [785486]\n\n\u001b[38;5;242m0\u001b[0m\n\u001b[0m\n\u001b[92mFile /home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz has been downloaded successfully\u001b[0m\n\u001b[38;5;242m\n--2022-07-08 12:36:19--  ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM5621nnn/GSM5621758/suppl/GSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz\n           =&gt; \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz\u2019\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 2607:f220:41e:250::13, 2607:f220:41e:250::12, 165.112.9.229, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|2607:f220:41e:250::13|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /geo/samples/GSM5621nnn/GSM5621758/suppl ... done.\n==&gt; SIZE GSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz ... 784432\n==&gt; EPSV ... done.    ==&gt; RETR GSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz ... done.\nLength: 784432 (766K) (unauthoritative)\n\nGSM5621758_ChIPseq_ 100%[===================&gt;] 766.05K  1.03MB/s    in 0.7s\n\n2022-07-08 12:36:20 (1.03 MB/s) - \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz\u2019 saved [784432]\n\n\u001b[38;5;242m0\u001b[0m\n\u001b[0m\n\u001b[92mFile /home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz has been downloaded successfully\u001b[0m\n\u001b[38;5;242m\n--2022-07-08 12:36:21--  ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM5621nnn/GSM5621760/suppl/GSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz\n           =&gt; \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz\u2019\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 2607:f220:41e:250::13, 2607:f220:41e:250::12, 165.112.9.229, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|2607:f220:41e:250::13|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /geo/samples/GSM5621nnn/GSM5621760/suppl ... done.\n==&gt; SIZE GSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz ... 163441\n==&gt; EPSV ... done.    ==&gt; RETR GSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz ... done.\nLength: 163441 (160K) (unauthoritative)\n\nGSM5621760_CUTTag_H 100%[===================&gt;] 159.61K   816KB/s    in 0.2s\n\n2022-07-08 12:36:21 (816 KB/s) - \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz\u2019 saved [163441]\n\n\u001b[38;5;242m0\u001b[0m\n\u001b[0m\n\u001b[92mFile /home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz has been downloaded successfully\u001b[0m\n\u001b[38;5;242m\n--2022-07-08 12:36:22--  ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM5621nnn/GSM5621761/suppl/GSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz\n           =&gt; \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz\u2019\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 2607:f220:41e:250::13, 2607:f220:41e:250::12, 165.112.9.229, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|2607:f220:41e:250::13|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n==&gt; TYPE I ... done.  ==&gt; CWD (1) /geo/samples/GSM5621nnn/GSM5621761/suppl ... done.\n==&gt; SIZE GSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz ... 117250\n==&gt; EPSV ... done.    ==&gt; RETR GSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz ... done.\nLength: 117250 (115K) (unauthoritative)\n\nGSM5621761_CUTTag_H 100%[===================&gt;] 114.50K   318KB/s    in 0.4s\n\n2022-07-08 12:36:23 (318 KB/s) - \u2018/home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz\u2019 saved [117250]\n\n\u001b[38;5;242m0\u001b[0m\n\u001b[0m\n\u001b[92mFile /home/bnt4me/Virginia/for_docs/geo/GSE185701/GSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz has been downloaded successfully\u001b[0m\nFinished processing 1 accession(s)\nUnifying and saving of metadata... \n\u001b[92mFile /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/PEP_samples/GSE185701_samples.csv has been saved successfully\u001b[0m\n  Config file: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/PEP_samples/GSE185701_samples.yaml\n</code></pre> <p>Now lets list the folder to see what data is there. And let's see what's in pep files now.</p> <pre><code>ls /home/bnt4me/Virginia/for_docs/geo/GSE185701\n</code></pre> <pre><code>\u001b[0m\u001b[01;31mGSM5621756_ChIPseq_Huh7_siNC_H3K27ac_summits.bed.gz\u001b[0m\n\u001b[01;31mGSM5621758_ChIPseq_Huh7_siDHX37_H3K27ac_summits.bed.gz\u001b[0m\n\u001b[01;31mGSM5621760_CUTTag_Huh7_DHX37_summits.bed.gz\u001b[0m\n\u001b[01;31mGSM5621761_CUTTag_Huh7_PLRG1_summits.bed.gz\u001b[0m\n</code></pre> <pre><code>cut -c -100 cat PEP_samples/GSE185701_samples.csv\n</code></pre> <pre><code>cut: cat: No such file or directory\nsample_platform_id,sample_library_strategy,sample_contact_country,sample_contact_name,sample_contact\nGPL20795,ChIP-Seq,China,\"Xianghuo,,He\",Shanghai,HCC,\"transfected with siNC using Lipofectamine RNAiM\nGPL20795,ChIP-Seq,China,\"Xianghuo,,He\",Shanghai,HCC,\"transfected with siDHX37 using Lipofectamine RN\nGPL20795,OTHER,China,\"Xianghuo,,He\",Shanghai,HCC,\"transfected with Flag-DHX37 lentivirus, renew the \nGPL20795,OTHER,China,\"Xianghuo,,He\",Shanghai,HCC,untreated,SRA,Huh 7,hg38,Homo sapiens,HiSeq X Ten,h\n</code></pre> <pre><code>cat PEP_samples/GSE185701_samples.yaml\n</code></pre> <pre><code># Autogenerated by geofetch\n\npep_version: 2.1.0\nproject_name: GSE185701\nsample_table: GSE185701_samples.csv\n\nsample_modifiers:\n  append:\n    output_file_path: FILES\n    sample_growth_protocol_ch1: Huh 7 was cultured in Dulbecco\u2019s modified Eagle\u2019s medium (DMEM) (Invitrogen, Carlsbad, CA, USA) containing 10% fetal bovine serum (FBS) (HyClone, Logan, UT, USA) and antibiotics (penicillin and streptomycin, Invitrogen) at 37 \u00b0C in 5% CO2.\n\n  derive:\n    attributes: [output_file_path]\n    sources:\n      FILES: /home/bnt4me/Virginia/for_docs/geo/{gse}/{file}\n</code></pre> <p>Now we have easy access to this data by using peppy package in python or pepr in r in further analysis </p>"},{"location":"geofetch/code/python-api/","title":"API","text":""},{"location":"geofetch/code/python-api/#package-geofetch-documentation","title":"Package <code>geofetch</code> Documentation","text":"<p>Package-level data </p>"},{"location":"geofetch/code/python-api/#class-finder","title":"Class <code>Finder</code>","text":"<p>Class for finding GSE accessions in special period of time. Additionally, user can add specific filters for the search, while initialization of the class</p> <pre><code>def __init__(self, filters: str=None, retmax: int=10000000)\n</code></pre>"},{"location":"geofetch/code/python-api/#parameters","title":"Parameters:","text":"<ul> <li><code>filters</code> (``):  filters that have to be added to the query.Filter Patterns can be found here: https://www.ncbi.nlm.nih.gov/books/NBK3837/#EntrezHelp.Using_the_Advanced_Search_Pag</li> <li><code>retmax</code> (``):  maximum number of retrieved accessions.</li> </ul> <pre><code>def find_differences(old_list: list, new_list: list) -&gt; list\n</code></pre> <p>Compare 2 lists and search for elements that are not in old list</p>"},{"location":"geofetch/code/python-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>old_list</code> (``):  old list of elements</li> <li><code>new_list</code> (``):  new list of elements</li> </ul>"},{"location":"geofetch/code/python-api/#returns","title":"Returns:","text":"<ul> <li>``:  list of elements that are not in old list but are in new_list</li> </ul> <pre><code>def generate_file(self, file_path: str, gse_list: list=None)\n</code></pre> <p>Save the list of GSE accessions stored in this Finder object to a given file</p>"},{"location":"geofetch/code/python-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>file_path</code> (``):  root to the file where gse accessions have to be saved</li> <li><code>gse_list</code> (``):  list of gse accessions</li> </ul>"},{"location":"geofetch/code/python-api/#returns_1","title":"Returns:","text":"<ul> <li>``:  NoReturn</li> </ul> <pre><code>def get_gse_all(self) -&gt; list\n</code></pre> <p>Get list of all gse accession available in GEO</p>"},{"location":"geofetch/code/python-api/#returns_2","title":"Returns:","text":"<ul> <li>``:  list of gse accession</li> </ul> <pre><code>def get_gse_by_date(self, start_date: str, end_date: str=None) -&gt; list\n</code></pre> <p>Search gse accessions by providing start date and end date. By default, the last date is today.</p>"},{"location":"geofetch/code/python-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>start_date</code> (``):  'YYYY/MM/DD']</li> <li><code>end_date</code> (``):  'YYYY/MM/DD']</li> </ul>"},{"location":"geofetch/code/python-api/#returns_3","title":"Returns:","text":"<ul> <li>``:  list of gse accessions</li> </ul> <pre><code>def get_gse_by_day_count(self, n_days: int=1) -&gt; list\n</code></pre> <p>Get list of gse accessions that were uploaded or updated in last X days</p>"},{"location":"geofetch/code/python-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>n_days</code> (``):  number of days from now [e.g. 5]</li> </ul>"},{"location":"geofetch/code/python-api/#returns_4","title":"Returns:","text":"<ul> <li>``:  list of gse accession</li> </ul> <pre><code>def get_gse_id_by_query(self, url: str) -&gt; list\n</code></pre> <p>Run esearch (ncbi search tool) by specifying URL and retrieve gse list result</p>"},{"location":"geofetch/code/python-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>url</code> (``):  url of the query</li> </ul>"},{"location":"geofetch/code/python-api/#returns_5","title":"Returns:","text":"<ul> <li>``:  list of gse ids</li> </ul> <pre><code>def get_gse_last_3_month(self) -&gt; list\n</code></pre> <p>Get list of gse accession that were uploaded or updated in last 3 month</p>"},{"location":"geofetch/code/python-api/#returns_6","title":"Returns:","text":"<ul> <li>``:  list of gse accession</li> </ul> <pre><code>def get_gse_last_week(self) -&gt; list\n</code></pre> <p>Get list of gse accession that were uploaded or updated in last week</p>"},{"location":"geofetch/code/python-api/#returns_7","title":"Returns:","text":"<ul> <li>``:  list of gse accession</li> </ul> <pre><code>def uid_to_gse(uid: str) -&gt; str\n</code></pre> <p>UID to GES accession converter</p>"},{"location":"geofetch/code/python-api/#parameters_6","title":"Parameters:","text":"<ul> <li><code>uid</code> (``):  uid string (Unique Identifier Number in GEO)</li> </ul>"},{"location":"geofetch/code/python-api/#returns_8","title":"Returns:","text":"<ul> <li>``:  GSE id string</li> </ul>"},{"location":"geofetch/code/python-api/#class-geofetcher","title":"Class <code>Geofetcher</code>","text":"<p>Class to download or get projects, metadata, data from GEO and SRA</p> <pre><code>def __init__(self, name: str='', metadata_root: str='', metadata_folder: str='', just_metadata: bool=False, refresh_metadata: bool=False, config_template: str=None, pipeline_samples: str=None, pipeline_project: str=None, skip: int=0, acc_anno: bool=False, use_key_subset: bool=False, processed: bool=False, data_source: str='samples', filter: str=None, filter_size: str=None, geo_folder: str='.', split_experiments: bool=False, bam_folder: str='', fq_folder: str='', sra_folder: str='', bam_conversion: bool=False, picard_path: str='', input: str=None, const_limit_project: int=50, const_limit_discard: int=1000, attr_limit_truncate: int=500, max_soft_size: str='1GB', discard_soft: bool=False, add_dotfile: bool=False, disable_progressbar: bool=False, add_convert_modifier: bool=False, opts=None, max_prefetch_size=None, **kwargs)\n</code></pre> <p>Constructor</p>"},{"location":"geofetch/code/python-api/#parameters_7","title":"Parameters:","text":"<ul> <li><code>input</code> (``):  GSEnumber or path to the input file</li> <li><code>name</code> (``):  Specify a project name. Defaults to GSE number or name of accessions file name</li> <li><code>metadata_root</code> (``):   Specify a parent folder location to store metadata.The project name will be added as a subfolder [Default: $SRAMETA:]</li> <li><code>metadata_folder</code> (``):  Specify an absolute folder location to store metadata. No subfolder will be added.Overrides value of --metadata-root [Default: Not used (--metadata-root is used by default)]</li> <li><code>just_metadata</code> (``):  If set, don't actually run downloads, just create metadata</li> <li><code>refresh_metadata</code> (``):  If set, re-download metadata even if it exists.</li> <li><code>config_template</code> (``):  Project config yaml file template.</li> <li><code>pipeline_samples</code> (``):  Specify one or more filepaths to SAMPLES pipeline interface yaml files.These will be added to the project config file to make it immediately compatible with looper. [Default: null]</li> <li><code>pipeline_project</code> (``):  Specify one or more filepaths to PROJECT pipeline interface yaml files.These will be added to the project config file to make it immediately compatible with looper. [Default: null]</li> <li><code>acc_anno</code> (``):   Produce annotation sheets for each accession.Project combined PEP for the whole project won't be produced.</li> <li><code>discard_soft</code> (``):  Create project without downloading soft files on the disc</li> <li><code>add_dotfile</code> (``):  Add .pep.yaml file that points .yaml PEP file</li> <li><code>disable_progressbar</code> (``):  Set true to disable progressbar</li> </ul> <pre><code>def fetch_all(self, input: str, name: str=None) -&gt; Union[NoReturn, peppy.project.Project]\n</code></pre> <p>Main function driver/workflow Function that search, filters, downloads and save data and metadata from  GEO and SRA</p>"},{"location":"geofetch/code/python-api/#parameters_8","title":"Parameters:","text":"<ul> <li><code>input</code> (``):  GSE or input file with gse's</li> <li><code>name</code> (``):  Name of the project</li> </ul>"},{"location":"geofetch/code/python-api/#returns_9","title":"Returns:","text":"<ul> <li>``:  NoReturn or peppy Project</li> </ul> <pre><code>def fetch_processed_one(self, gse_file_content: list, gsm_file_content: list, gsm_filter_list: dict) -&gt; Tuple\n</code></pre> <p>Fetche one processed GSE project and return its metadata</p>"},{"location":"geofetch/code/python-api/#parameters_9","title":"Parameters:","text":"<ul> <li><code>gsm_file_content</code> (``):  gse soft file content</li> <li><code>gse_file_content</code> (``):  gsm soft file content</li> <li><code>gsm_filter_list</code> (``):  list of gsm that have to be downloaded</li> </ul>"},{"location":"geofetch/code/python-api/#returns_10","title":"Returns:","text":"<ul> <li>``:  Tuple of project list of gsm samples and gse samples</li> </ul> <pre><code>def get_projects(self, input: str, just_metadata: bool=True, discard_soft: bool=True) -&gt; dict\n</code></pre> <p>Function for fetching projects from GEO|SRA and receiving peppy project</p>"},{"location":"geofetch/code/python-api/#parameters_10","title":"Parameters:","text":"<ul> <li><code>input</code> (``):  GSE number, or path to file of GSE numbers</li> <li><code>just_metadata</code> (``):  process only metadata</li> <li><code>discard_soft</code> (``):   clean run, without downloading soft files</li> </ul>"},{"location":"geofetch/code/python-api/#returns_11","title":"Returns:","text":"<ul> <li>``:  peppy project or list of project, if acc_anno is set.</li> </ul> <p>Version Information: <code>geofetch</code> v0.12.6, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"geofetch/code/python-usage/","title":"Tutorial of usage geofetch as python package","text":"<p>\u266a\u266b\u2022\u266a\u266a\u266b\u2022\u266a\u266a\u266b\u2022\u266a\u266a\u266b\u2022\u266a\u266a\u266b*</p> <p>Geofetch provides python fuctions to fetch metadata and metadata from GEO and SRA by using python language. <code>get_project</code> function returns dictionary of peppy projects that were found using filters and input you specified.  peppy is a Python package that provides an API for handling standardized project and sample metadata. </p> <p>More information you can get here:</p> <p>http://peppy.databio.org/en/latest/</p> <p>http://pep.databio.org/en/2.0.0/</p>"},{"location":"geofetch/code/python-usage/#first-lets-import-geofetch","title":"First let's import geofetch","text":"<pre><code>from geofetch import Geofetcher\n</code></pre>"},{"location":"geofetch/code/python-usage/#initiate-geofetch-object-by-specifing-parameters-that-you-want-to-use-for-downloading-metadatadata","title":"Initiate Geofetch object by specifing parameters that you want to use for downloading metadata/data","text":"<p>1) If you won't specify any parameters, defaul parameters will be used</p> <pre><code>geof = Geofetcher()\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/project_name\n</code></pre> <p>2) To download processed data with samples and series specify this two arguments:</p> <pre><code>geof = Geofetcher(processed=True, data_source=\"all\")\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/project_name\n</code></pre> <p>3) To tune project parameter, where metadata should be stored use next parameters:</p> <pre><code>geof = Geofetcher(processed=True, data_source=\"all\", const_limit_project = 20, const_limit_discard = 500, attr_limit_truncate = 10000 )\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/project_name\n</code></pre> <p>4) To add more filter of other options see documentation</p>"},{"location":"geofetch/code/python-usage/#run-geofetch","title":"Run Geofetch","text":""},{"location":"geofetch/code/python-usage/#by-default","title":"By default:","text":"<p>1) No actual data will be downloaded (just_metadata=True)</p> <p>2) No soft files will be saved on the disc (discard_soft=True)</p> <pre><code>projects = geof.get_projects(\"GSE95654\")\n</code></pre> <pre><code>Trying GSE95654 (not a file) as accession...\nTrying GSE95654 (not a file) as accession...\n\n\n\nOutput()\n\n\nSkipped 0 accessions. Starting now.\n\u001b[38;5;200mProcessing accession 1 of 1: 'GSE95654'\u001b[0m\n\nTotal number of processed SAMPLES files found is: 40\nTotal number of processed SERIES files found is: 0\nExpanding metadata list...\nExpanding metadata list...\n</code></pre> <pre></pre> <pre>\n</pre> <pre><code>Finished processing 1 accession(s)\nCleaning soft files ...\nUnifying and saving of metadata...\n\n\n\nOutput()\n</code></pre> <pre></pre> <pre>\n</pre> <pre>\n</pre> <pre><code>Output()\n</code></pre> <pre></pre> <pre>\n</pre> <pre>\n</pre> <pre><code>No files found. No data to save. File /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/project_name/GSE95654_series/GSE95654_series.csv won't be created\n</code></pre> <p>Check if projects were created by checking dict keys:</p> <pre><code>projects.keys()\n</code></pre> <pre><code>dict_keys(['GSE95654_samples'])\n</code></pre> <p>project for smaples was created! Now let's look into it.</p> <p>* the values of the dictionary are peppy projects. More information about peppy Project you can find in the documentation: http://peppy.databio.org/en/latest/</p> <pre><code>len(projects['GSE95654_samples'].samples)\n</code></pre> <pre><code>40\n</code></pre> <p>We got 40 samples from GSE95654 project. If you want to check if it's correct information go into: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE95654</p> <p>Now let's see actuall data. first 15 project and 5 clolumns:</p> <pre><code>projects['GSE95654_samples'].sample_table.iloc[:15 , :5]\n</code></pre> sample_name sample_library_strategy genome_build tissue sample_organism_ch1 sample_name RRBS_on_CRC_patient_8 RRBS_on_CRC_patient_8 Bisulfite-Seq hg19 primary tumor Homo sapiens RRBS_on_adjacent_normal_colon_patient_8 RRBS_on_adjacent_normal_colon_patient_8 Bisulfite-Seq hg19 adjacent normal colon Homo sapiens RRBS_on_CRC_patient_32 RRBS_on_CRC_patient_32 Bisulfite-Seq hg19 primary tumor Homo sapiens RRBS_on_adjacent_normal_colon_patient_32 RRBS_on_adjacent_normal_colon_patient_32 Bisulfite-Seq hg19 adjacent normal colon Homo sapiens RRBS_on_CRC_patient_41 RRBS_on_CRC_patient_41 Bisulfite-Seq hg19 primary tumor Homo sapiens RRBS_on_adjacent_normal_colon_patient_41 RRBS_on_adjacent_normal_colon_patient_41 Bisulfite-Seq hg19 adjacent normal colon Homo sapiens RRBS_on_CRC_patient_42 RRBS_on_CRC_patient_42 Bisulfite-Seq hg19 primary tumor Homo sapiens RRBS_on_adjacent_normal_colon_patient_42 RRBS_on_adjacent_normal_colon_patient_42 Bisulfite-Seq hg19 adjacent normal colon Homo sapiens RRBS_on_ACF_patient_173 RRBS_on_ACF_patient_173 Bisulfite-Seq hg19 aberrant crypt foci Homo sapiens RRBS_on_ACF_patient_515 RRBS_on_ACF_patient_515 Bisulfite-Seq hg19 aberrant crypt foci Homo sapiens RRBS_on_normal_crypts_patient_139 RRBS_on_normal_crypts_patient_139 Bisulfite-Seq hg19 normal colonic crypt Homo sapiens RRBS_on_ACF_patient_143 RRBS_on_ACF_patient_143 Bisulfite-Seq hg19 aberrant crypt foci Homo sapiens RRBS_on_normal_crypts_patient_143 RRBS_on_normal_crypts_patient_143 Bisulfite-Seq hg19 normal colonic crypt Homo sapiens RRBS_on_normal_crypts_patient_165 RRBS_on_normal_crypts_patient_165 Bisulfite-Seq hg19 normal colonic crypt Homo sapiens RRBS_on_ACF_patient_165 RRBS_on_ACF_patient_165 Bisulfite-Seq hg19 aberrant crypt foci Homo sapiens"},{"location":"geofetch/code/raw-data-downloading/","title":"geofetch tutorial for raw data","text":"<p>The GSE67303 data set has about 250 mb of data across 4 samples, so it's a quick download for a test case. Let's take a quick peek at the geofetch version:</p> <pre><code>geofetch --version\n</code></pre> <pre><code>geofetch 0.10.1\n</code></pre> <p>To see your CLI options, invoke <code>geofetch -h</code>:</p> <pre><code>geofetch -h\n</code></pre> <pre><code>usage: geofetch [-h] [-V] -i INPUT [-n NAME] [-m METADATA_ROOT]\n                [-u METADATA_FOLDER] [--just-metadata] [-r]\n                [--config-template CONFIG_TEMPLATE]\n                [--pipeline-samples PIPELINE_SAMPLES]\n                [--pipeline-project PIPELINE_PROJECT] [-k SKIP] [--acc-anno]\n                [--discard-soft] [--const-limit-project CONST_LIMIT_PROJECT]\n                [--const-limit-discard CONST_LIMIT_DISCARD]\n                [--attr-limit-truncate ATTR_LIMIT_TRUNCATE] [--add-dotfile]\n                [-p] [--data-source {all,samples,series}] [--filter FILTER]\n                [--filter-size FILTER_SIZE] [-g GEO_FOLDER] [-x]\n                [-b BAM_FOLDER] [-f FQ_FOLDER] [--use-key-subset] [--silent]\n                [--verbosity V] [--logdev]\n\nAutomatic GEO and SRA data downloader\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -V, --version         show program's version number and exit\n  -i INPUT, --input INPUT\n                        required: a GEO (GSE) accession, or a file with a list\n                        of GSE numbers\n  -n NAME, --name NAME  Specify a project name. Defaults to GSE number\n  -m METADATA_ROOT, --metadata-root METADATA_ROOT\n                        Specify a parent folder location to store metadata.\n                        The project name will be added as a subfolder\n                        [Default: $SRAMETA:]\n  -u METADATA_FOLDER, --metadata-folder METADATA_FOLDER\n                        Specify an absolute folder location to store metadata.\n                        No subfolder will be added. Overrides value of\n                        --metadata-root [Default: Not used (--metadata-root is\n                        used by default)]\n  --just-metadata       If set, don't actually run downloads, just create\n                        metadata\n  -r, --refresh-metadata\n                        If set, re-download metadata even if it exists.\n  --config-template CONFIG_TEMPLATE\n                        Project config yaml file template.\n  --pipeline-samples PIPELINE_SAMPLES\n                        Optional: Specify one or more filepaths to SAMPLES\n                        pipeline interface yaml files. These will be added to\n                        the project config file to make it immediately\n                        compatible with looper. [Default: null]\n  --pipeline-project PIPELINE_PROJECT\n                        Optional: Specify one or more filepaths to PROJECT\n                        pipeline interface yaml files. These will be added to\n                        the project config file to make it immediately\n                        compatible with looper. [Default: null]\n  -k SKIP, --skip SKIP  Skip some accessions. [Default: no skip].\n  --acc-anno            Optional: Produce annotation sheets for each\n                        accession. Project combined PEP for the whole project\n                        won't be produced.\n  --discard-soft        Optional: After creation of PEP files, all soft and\n                        additional files will be deleted\n  --const-limit-project CONST_LIMIT_PROJECT\n                        Optional: Limit of the number of the constant sample\n                        characters that should not be in project yaml.\n                        [Default: 50]\n  --const-limit-discard CONST_LIMIT_DISCARD\n                        Optional: Limit of the number of the constant sample\n                        characters that should not be discarded [Default: 250]\n  --attr-limit-truncate ATTR_LIMIT_TRUNCATE\n                        Optional: Limit of the number of sample characters.Any\n                        attribute with more than X characters will truncate to\n                        the first X, where X is a number of characters\n                        [Default: 500]\n  --add-dotfile         Optional: Add .pep.yaml file that points .yaml PEP\n                        file\n  --silent              Silence logging. Overrides verbosity.\n  --verbosity V         Set logging level (1-5 or logging module level name)\n  --logdev              Expand content of logging message format.\n\nprocessed:\n  -p, --processed       Download processed data [Default: download raw data].\n  --data-source {all,samples,series}\n                        Optional: Specifies the source of data on the GEO\n                        record to retrieve processed data, which may be\n                        attached to the collective series entity, or to\n                        individual samples. Allowable values are: samples,\n                        series or both (all). Ignored unless 'processed' flag\n                        is set. [Default: samples]\n  --filter FILTER       Optional: Filter regex for processed filenames\n                        [Default: None].Ignored unless 'processed' flag is\n                        set.\n  --filter-size FILTER_SIZE\n                        Optional: Filter size for processed files that are\n                        stored as sample repository [Default: None]. Works\n                        only for sample data. Supported input formats : 12B,\n                        12KB, 12MB, 12GB. Ignored unless 'processed' flag is\n                        set.\n  -g GEO_FOLDER, --geo-folder GEO_FOLDER\n                        Optional: Specify a location to store processed GEO\n                        files. Ignored unless 'processed' flag is\n                        set.[Default: $GEODATA:]\n\nraw:\n  -x, --split-experiments\n                        Split SRR runs into individual samples. By default,\n                        SRX experiments with multiple SRR Runs will have a\n                        single entry in the annotation table, with each run as\n                        a separate row in the subannotation table. This\n                        setting instead treats each run as a separate sample\n  -b BAM_FOLDER, --bam-folder BAM_FOLDER\n                        Optional: Specify folder of bam files. Geofetch will\n                        not download sra files when corresponding bam files\n                        already exist. [Default: $SRABAM:]\n  -f FQ_FOLDER, --fq-folder FQ_FOLDER\n                        Optional: Specify folder of fastq files. Geofetch will\n                        not download sra files when corresponding fastq files\n                        already exist. [Default: $SRAFQ:]\n  --use-key-subset      Use just the keys defined in this module when writing\n                        out metadata.\n</code></pre> <p>Calling geofetch will do 4 tasks: </p> <ol> <li>download all <code>.sra</code> files from <code>GSE#####</code> into your SRA folder (wherever you have configured <code>sratools</code> to stick data).</li> <li>download all metadata from GEO and SRA and store in your metadata folder.</li> <li>produce a PEP-compatible sample table, <code>PROJECT_NAME_annotation.csv</code>, in your metadata folder.</li> <li>produce a PEP-compatible project configuration file, <code>PROJECT_NAME_config.yaml</code>, in your metadata folder.</li> </ol> <p>Complete details about geofetch outputs is cataloged in the metadata outputs reference.</p>"},{"location":"geofetch/code/raw-data-downloading/#download-the-data","title":"Download the data","text":"<p>First, create the metadata:</p> <pre><code>geofetch -i GSE67303 -n red_algae -m `pwd` --just-metadata\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae\nTrying GSE67303 (not a file) as accession...\nSkipped 0 accessions. Starting now.\n\u001b[38;5;200mProcessing accession 1 of 1: 'GSE67303'\u001b[0m\n--2022-07-08 12:39:24--  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gse&amp;acc=GSE67303&amp;form=text&amp;view=full\nResolving www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)... 2607:f220:41e:4290::110, 130.14.29.110\nConnecting to www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)|2607:f220:41e:4290::110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [geo/text]\nSaving to: \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_GSE.soft\u2019\n\n/home/bnt4me/Virgin     [ &lt;=&gt;                ]   3.19K  --.-KB/s    in 0s\n\n2022-07-08 12:39:24 (134 MB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_GSE.soft\u2019 saved [3266]\n\n--2022-07-08 12:39:24--  https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gsm&amp;acc=GSE67303&amp;form=text&amp;view=full\nResolving www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)... 2607:f220:41e:4290::110, 130.14.29.110\nConnecting to www.ncbi.nlm.nih.gov (www.ncbi.nlm.nih.gov)|2607:f220:41e:4290::110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [geo/text]\nSaving to: \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_GSM.soft\u2019\n\n/home/bnt4me/Virgin     [ &lt;=&gt;                ]  10.70K  --.-KB/s    in 0.05s\n\n2022-07-08 12:39:24 (218 KB/s) - \u2018/home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_GSM.soft\u2019 saved [10956]\n\nProcessed 4 samples.\nFound SRA Project accession: SRP056574\nDownloading SRP056574 sra metadata\nParsing SRA file to download SRR records\nsample_name does not exist, creating new...\nGetting SRR: SRR1930183 (SRX969073)\nDry run (no raw data will be download)\nsample_name does not exist, creating new...\nGetting SRR: SRR1930184 (SRX969074)\nDry run (no raw data will be download)\nsample_name does not exist, creating new...\nGetting SRR: SRR1930185 (SRX969075)\nDry run (no raw data will be download)\nsample_name does not exist, creating new...\nGetting SRR: SRR1930186 (SRX969076)\nDry run (no raw data will be download)\nFinished processing 1 accession(s)\nCreating complete project annotation sheets and config file...\nSample annotation sheet: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_annotation.csv\nWriting: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_annotation.csv\n  Config file: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_config.yaml\n</code></pre> <p>The <code>-m</code> parameter specifies to use the current directory, storing the data according to the name (<code>-n</code>) parameter. So, we'll now have a <code>red_alga</code> subfolder, where the results will be saved. Inside that folder you'll see the output of the command:</p> <pre><code>ls red_algae\n</code></pre> <pre><code>GSE67303_annotation.csv  GSE67303_GSE.soft  GSE67303_SRA.csv\nGSE67303_config.yaml     GSE67303_GSM.soft\n</code></pre> <p>The <code>.soft</code> files are the direct output from GEO, which contain all the metadata as stored by GEO, for both the experiment (<code>_GSE</code>) and for the individual samples (<code>_GSM</code>). Geofetch also produces a <code>csv</code> file with the SRA metadata. The filtered version (ending in <code>_filt</code>) would contain only the specified subset of the samples if we didn't request them all, but in this case, since we only gave an accession, it is identical to the complete file.</p> <p>Finally, there are the 2 files that make up the PEP: the <code>_config.yaml</code> file and the <code>_annotation.csv</code> file. Let's see what's in these files now.</p> <pre><code>cat red_algae/GSE67303_config.yaml\n</code></pre> <pre><code># Autogenerated by geofetch\n\nname: GSE67303\npep_version: 2.1.0\nsample_table: GSE67303_annotation.csv\nsubsample_table: null\n\nlooper:\n  output_dir: GSE67303\n  pipeline_interfaces: {pipeline_interfaces}\n\nsample_modifiers:\n  append:\n    Sample_growth_protocol_ch1: Cyanidioschyzon merolae cells were grown in 2xMA media\n    Sample_data_processing: Supplementary_files_format_and_content: Excel spreadsheet includes FPKM values for Darkness and Blue-Light exposed samples with p and q values of cuffdiff output.\n    Sample_extract_protocol_ch1: RNA libraries were prepared for sequencing using standard Illumina protocols\n    Sample_treatment_protocol_ch1: Cells were exposed to blue-light (15 \u00b5mole m-2s-1) for 30 minutes\n    SRR_files: SRA\n\n  derive:\n    attributes: [read1, read2, SRR_files]\n    sources:\n      SRA: \"${SRABAM}/{SRR}.bam\"\n      FQ: \"${SRAFQ}/{SRR}.fastq.gz\"\n      FQ1: \"${SRAFQ}/{SRR}_1.fastq.gz\"\n      FQ2: \"${SRAFQ}/{SRR}_2.fastq.gz\"      \n  imply:\n    - if: \n        organism: \"Mus musculus\"\n      then:\n        genome: mm10\n    - if: \n        organism: \"Homo sapiens\"\n      then:\n        genome: hg38          \n    - if: \n        read_type: \"PAIRED\"\n      then:\n        read1: FQ1\n        read2: FQ2          \n    - if: \n        read_type: \"SINGLE\"\n      then:\n        read1: FQ1\n\nproject_modifiers:\n  amend:\n    sra_convert:\n      looper:\n        results_subdir: sra_convert_results\n      sample_modifiers:\n        append:\n          SRR_files: SRA\n          pipeline_interfaces: ${CODE}/geofetch/pipeline_interface_convert.yaml\n        derive:\n          attributes: [read1, read2, SRR_files]\n          sources:\n            SRA: \"${SRARAW}/{SRR}.sra\"\n            FQ: \"${SRAFQ}/{SRR}.fastq.gz\"\n            FQ1: \"${SRAFQ}/{SRR}_1.fastq.gz\"\n            FQ2: \"${SRAFQ}/{SRR}_2.fastq.gz\"\n</code></pre> <p>There are two important things to note in his file: First, see in the PEP that <code>sample_table</code> points to the csv file produced by geofetch. Second, look at the amendment called <code>sra_convert</code>. This adds a pipeline interface to the sra conversion pipeline, and adds derived attributes for SRA files and fastq files that rely on environment variables called <code>$SRARAW</code> and <code>$SRAFQ</code>. These environment variables should point to folders where you store your raw .sra files and the converted fastq files.</p> <p>Now let's look at the first 100 characters of the csv file:</p> <pre><code>cut -c -100 red_algae/GSE67303_annotation.csv\n</code></pre> <pre><code>sample_name,protocol,organism,read_type,data_source,SRR,SRX,Sample_title,Sample_geo_accession,Sample\nCm_BlueLight_Rep1,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930183,SRX969073,Cm_BlueLig\nCm_BlueLight_Rep2,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930184,SRX969074,Cm_BlueLig\nCm_Darkness_Rep1,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930185,SRX969075,Cm_Darkness\nCm_Darkness_Rep2,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930186,SRX969076,Cm_Darkness\n</code></pre> <p>Now let's download the actual data.</p> <pre><code>geofetch -i GSE67303 -n red_algae -m `pwd`\n</code></pre> <pre><code>Metadata folder: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae\nTrying GSE67303 (not a file) as accession...\nSkipped 0 accessions. Starting now.\n\u001b[38;5;200mProcessing accession 1 of 1: 'GSE67303'\u001b[0m\nFound previous GSE file: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_GSE.soft\nFound previous GSM file: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_GSM.soft\nProcessed 4 samples.\nFound SRA Project accession: SRP056574\nFound SRA metadata, opening..\nParsing SRA file to download SRR records\nsample_name does not exist, creating new...\nGetting SRR: SRR1930183 (SRX969073)\n\n2022-07-08T16:40:20 prefetch.2.11.2: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2022-07-08T16:40:20 prefetch.2.11.2: 1) Downloading 'SRR1930183'...\n2022-07-08T16:40:20 prefetch.2.11.2: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2022-07-08T16:40:20 prefetch.2.11.2:  Downloading via HTTPS...\n2022-07-08T16:41:28 prefetch.2.11.2:  HTTPS download succeed\n2022-07-08T16:41:28 prefetch.2.11.2:  'SRR1930183' is valid\n2022-07-08T16:41:28 prefetch.2.11.2: 1) 'SRR1930183' was downloaded successfully\n2022-07-08T16:41:28 prefetch.2.11.2: 'SRR1930183' has 0 unresolved dependencies\nsample_name does not exist, creating new...\nGetting SRR: SRR1930184 (SRX969074)\n\n2022-07-08T16:41:39 prefetch.2.11.2: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2022-07-08T16:41:40 prefetch.2.11.2: 1) Downloading 'SRR1930184'...\n2022-07-08T16:41:40 prefetch.2.11.2: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2022-07-08T16:41:40 prefetch.2.11.2:  Downloading via HTTPS...\n2022-07-08T16:42:43 prefetch.2.11.2:  HTTPS download succeed\n2022-07-08T16:42:43 prefetch.2.11.2:  'SRR1930184' is valid\n2022-07-08T16:42:43 prefetch.2.11.2: 1) 'SRR1930184' was downloaded successfully\n2022-07-08T16:42:43 prefetch.2.11.2: 'SRR1930184' has 0 unresolved dependencies\nsample_name does not exist, creating new...\nGetting SRR: SRR1930185 (SRX969075)\n\n2022-07-08T16:42:54 prefetch.2.11.2: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2022-07-08T16:42:55 prefetch.2.11.2: 1) Downloading 'SRR1930185'...\n2022-07-08T16:42:55 prefetch.2.11.2: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2022-07-08T16:42:55 prefetch.2.11.2:  Downloading via HTTPS...\n2022-07-08T16:45:00 prefetch.2.11.2:  HTTPS download succeed\n2022-07-08T16:45:00 prefetch.2.11.2:  'SRR1930185' is valid\n2022-07-08T16:45:00 prefetch.2.11.2: 1) 'SRR1930185' was downloaded successfully\n2022-07-08T16:45:00 prefetch.2.11.2: 'SRR1930185' has 0 unresolved dependencies\nsample_name does not exist, creating new...\nGetting SRR: SRR1930186 (SRX969076)\n\n2022-07-08T16:45:11 prefetch.2.11.2: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n2022-07-08T16:45:12 prefetch.2.11.2: 1) Downloading 'SRR1930186'...\n2022-07-08T16:45:12 prefetch.2.11.2: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n2022-07-08T16:45:12 prefetch.2.11.2:  Downloading via HTTPS...\n2022-07-08T16:46:49 prefetch.2.11.2:  HTTPS download succeed\n2022-07-08T16:46:49 prefetch.2.11.2:  'SRR1930186' is valid\n2022-07-08T16:46:49 prefetch.2.11.2: 1) 'SRR1930186' was downloaded successfully\n2022-07-08T16:46:49 prefetch.2.11.2: 'SRR1930186' has 0 unresolved dependencies\nFinished processing 1 accession(s)\nCreating complete project annotation sheets and config file...\nSample annotation sheet: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_annotation.csv\nWriting: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_annotation.csv\n  Config file: /home/bnt4me/Virginia/repos/geof2/geofetch/docs_jupyter/red_algae/GSE67303_config.yaml\n</code></pre>"},{"location":"geofetch/code/raw-data-downloading/#finalize-the-project-config-and-sample-annotation","title":"Finalize the project config and sample annotation","text":"<p>That's basically it! <code>geofetch</code> will have produced a general-purpose PEP for you, but you'll need to modify it for whatever purpose you have. For example, one common thing is to link to the pipeline you want to use by adding a <code>pipeline_interface</code> to the project config file. You may also need to adjust the <code>sample_annotation</code> file to make sure you have the right column names and attributes needed by the pipeline you're using. GEO submitters are notoriously bad at getting the metadata correct.</p>"},{"location":"geofetch/code/raw-data-downloading/#selecting-samples-to-download","title":"Selecting samples to download.","text":"<p>By default, <code>geofetch</code> downloads all the data for one accession of interest. If you need more fine-grained control, either because you have multiple accessions or you need a subset of samples within them, you can use the file-based sample specification.</p>"},{"location":"geofetch/code/raw-data-downloading/#tips","title":"Tips","text":"<ul> <li> <p>Set an environment variable for <code>$SRABAM</code> (where <code>.bam</code> files will live), and <code>geofetch</code> will check to see if you have an already-converted bamfile there before issuing the command to download the <code>sra</code> file. In this way, you can delete old <code>sra</code> files after conversion and not have to worry about re-downloading them. </p> </li> <li> <p>The config template uses an environment variable <code>$SRARAW</code> for where <code>.sra</code> files will live. If you set this variable to the same place you instructed <code>sratoolkit</code> to download <code>sra</code> files, you won't have to tweak the config file. For more information refer to the <code>sratools</code> page.</p> </li> </ul> <p>You can find a complete example of using <code>geofetch</code> for RNA-seq data. </p>"},{"location":"geofetch/code/usage/","title":"usage reference","text":"<p><code>geofetch</code> command-line usage instructions:</p> <p><code>geofetch --help</code></p> <pre><code>usage: geofetch [&lt;args&gt;]\n\nThe example how to use geofetch (to download GSE573030 just metadata):\n    geofetch -i GSE67303 -m &lt;folder&gt; --just-metadata\n\nTo download all processed data of GSE57303:\n    geofetch -i GSE67303 --processed --geo-folder &lt;folder&gt; -m &lt;folder&gt;\n\nAutomatic GEO and SRA data downloader\n\noptions:\n  -h, --help            show this help message and exit\n  -V, --version         show program's version number and exit\n  -i INPUT, --input INPUT\n                        required: a GEO (GSE) accession, or a file with a list\n                        of GSE numbers\n  -n NAME, --name NAME  Specify a project name. Defaults to GSE number\n  -m METADATA_ROOT, --metadata-root METADATA_ROOT\n                        Specify a parent folder location to store metadata.\n                        The project name will be added as a subfolder\n                        [Default: $SRAMETA:]\n  -u METADATA_FOLDER, --metadata-folder METADATA_FOLDER\n                        Specify an absolute folder location to store metadata.\n                        No subfolder will be added. Overrides value of\n                        --metadata-root.\n  --just-metadata       If set, don't actually run downloads, just create\n                        metadata\n  -r, --refresh-metadata\n                        If set, re-download metadata even if it exists.\n  --config-template CONFIG_TEMPLATE\n                        Project config yaml file template.\n  --pipeline-samples PIPELINE_SAMPLES\n                        Optional: Specify one or more filepaths to SAMPLES\n                        pipeline interface yaml files. These will be added to\n                        the project config file to make it immediately\n                        compatible with looper. [Default: null]\n  --pipeline-project PIPELINE_PROJECT\n                        Optional: Specify one or more filepaths to PROJECT\n                        pipeline interface yaml files. These will be added to\n                        the project config file to make it immediately\n                        compatible with looper. [Default: null]\n  --disable-progressbar\n                        Optional: Disable progressbar\n  -k SKIP, --skip SKIP  Skip some accessions. [Default: no skip].\n  --acc-anno            Optional: Produce annotation sheets for each\n                        accession. Project combined PEP for the whole project\n                        won't be produced.\n  --discard-soft        Optional: After creation of PEP files, all .soft files\n                        will be deleted\n  --const-limit-project CONST_LIMIT_PROJECT\n                        Optional: Limit of the number of the constant sample\n                        characters that should not be in project yaml.\n                        [Default: 50]\n  --const-limit-discard CONST_LIMIT_DISCARD\n                        Optional: Limit of the number of the constant sample\n                        characters that should not be discarded [Default: 250]\n  --attr-limit-truncate ATTR_LIMIT_TRUNCATE\n                        Optional: Limit of the number of sample characters.Any\n                        attribute with more than X characters will truncate to\n                        the first X, where X is a number of characters\n                        [Default: 500]\n  --add-dotfile         Optional: Add .pep.yaml file that points .yaml PEP\n                        file\n  --max-soft-size MAX_SOFT_SIZE\n                        Optional: Max size of soft file. [Default: 1GB].\n                        Supported input formats : 12B, 12KB, 12MB, 12GB.\n  --max-prefetch-size MAX_PREFETCH_SIZE\n                        Argument to pass to prefetch program's --max-size\n                        option, if prefetch will be used in this run of\n                        geofetch; for reference: https://github.com/ncbi/sra-\n                        tools/wiki/08.-prefetch-and-fasterq-dump#check-the-\n                        maximum-size-limit-of-the-prefetch-tool\n  --silent              Silence logging. Overrides verbosity.\n  --verbosity V         Set logging level (1-5 or logging module level name)\n  --logdev              Expand content of logging message format.\n\nprocessed:\n  -p, --processed       Download processed data [Default: download raw data].\n  --data-source {all,samples,series}\n                        Optional: Specifies the source of data on the GEO\n                        record to retrieve processed data, which may be\n                        attached to the collective series entity, or to\n                        individual samples. Allowable values are: samples,\n                        series or both (all). Ignored unless 'processed' flag\n                        is set. [Default: samples]\n  --filter FILTER       Optional: Filter regex for processed filenames\n                        [Default: None].Ignored unless 'processed' flag is\n                        set.\n  --filter-size FILTER_SIZE\n                        Optional: Filter size for processed files that are\n                        stored as sample repository [Default: None]. Works\n                        only for sample data. Supported input formats : 12B,\n                        12KB, 12MB, 12GB. Ignored unless 'processed' flag is\n                        set.\n  -g GEO_FOLDER, --geo-folder GEO_FOLDER\n                        Optional: Specify a location to store processed GEO\n                        files. Ignored unless 'processed' flag is\n                        set.[Default: $GEODATA:]\n\nraw:\n  -x, --split-experiments\n                        Split SRR runs into individual samples. By default,\n                        SRX experiments with multiple SRR Runs will have a\n                        single entry in the annotation table, with each run as\n                        a separate row in the subannotation table. This\n                        setting instead treats each run as a separate sample\n  -b BAM_FOLDER, --bam-folder BAM_FOLDER\n                        Optional: Specify folder of bam files. Geofetch will\n                        not download sra files when corresponding bam files\n                        already exist. [Default: $SRABAM:]\n  -f FQ_FOLDER, --fq-folder FQ_FOLDER\n                        Optional: Specify folder of fastq files. Geofetch will\n                        not download sra files when corresponding fastq files\n                        already exist. [Default: $SRAFQ:]\n  --use-key-subset      Use just the keys defined in this module when writing\n                        out metadata.\n  --add-convert-modifier\n                        Add looper SRA convert modifier to config file.\n</code></pre>"},{"location":"looper/","title":"Looper","text":""},{"location":"looper/#what-is-looper","title":"What is looper?","text":"<p>Looper is a job submitting engine. Looper deploys arbitrary shell commands for each sample in a standard PEP project. You can think of looper as providing a single user interface to running, monitoring, and managing all of your sample-intensive research projects the same way, regardless of data type or pipeline used.</p>"},{"location":"looper/#what-makes-looper-better","title":"What makes looper better?","text":"<p>Looper decouples job handling from the pipeline process. In a typical pipeline, job handling (managing how individual jobs are submitted to a cluster) is delicately intertwined with actual pipeline commands (running the actual code for a single compute job). In contrast, the looper approach is modular: looper only manages job submission. This approach leads to several advantages compared with the traditional integrated approach:</p> <ol> <li>pipelines do not need to independently re-implement job handling code, which is shared.</li> <li>every project uses a universal structure, so datasets can move from one pipeline to another.</li> <li>users must learn only a single interface that works with any project for any pipeline.</li> <li>running just one or two samples/jobs is simpler, and does not require a  distributed compute environment.</li> </ol>"},{"location":"looper/#features-at-a-glance","title":"Features at-a-glance","text":"<p> Modular approach to job handling. Looper completely divides job handling from pipeline processing. Now,  pipelines no longer need to worry about sample metadata parsing.</p> <p> The power of standard PEP format. <code>Looper</code> inherits benefits of PEP format: For example, you only need to learn 1 way to format your project metadata, and it will work with any pipeline. PEP provides subprojects,  programmatic modifiers, loading in R with  pepr or Python with peppy.</p> <p> Universal parallelization implementation. Looper's sample-level parallelization applies to all pipelines, so individual pipelines do not have to re-implement parallelization by sample. <code>Looper</code> also handles cluster submission on any cluster resource manager (SLURM, SGE, etc.), so your pipeline doesn't need to manage that, either.</p> <p> Flexible pipelines. Use looper with any pipeline, any library, in any domain. As long as you can run your pipeline with a shell command, looper can run it for you.</p> <p> Job completion monitoring. Looper is job-aware and will not submit new jobs for samples that are already running or finished, making it easy to add new samples to existing projects, or re-run failed samples.</p> <p> Flexible resources. Looper has an easy-to-use resource requesting scheme. With a few lines to define CPU, memory, clock time, or anything else, pipeline authors can specify different computational resources depending on the size of the input sample and pipeline to run. Or, just use a default if you don't want to mess with setup.</p> <p> Command line interface. Looper uses a command-line interface so you have total power at your fingertips.</p> <p> Beautiful linked result reports. Looper automatically creates an internally linked, portable HTML report highlighting all results for your pipeline, for every pipeline. For an html report example see: PEPATAC Gold Summary</p>"},{"location":"looper/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"looper/changelog/#203-2025-09-23","title":"[2.0.3] -- 2025-09-23","text":""},{"location":"looper/changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed #543</li> <li>Fixed #547</li> <li>Fixed #548</li> </ul>"},{"location":"looper/changelog/#202-2025-09-22","title":"[2.0.2] -- 2025-09-22","text":""},{"location":"looper/changelog/#changed","title":"Changed","text":"<ul> <li>Remove veracitools dependency from the requirements.</li> </ul>"},{"location":"looper/changelog/#201-2025-03-05","title":"[2.0.1] -- 2025-03-05","text":""},{"location":"looper/changelog/#changed_1","title":"Changed","text":"<ul> <li>update ubiquerg&gt;=0.8.1</li> </ul>"},{"location":"looper/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>#541</li> </ul>"},{"location":"looper/changelog/#200-2025-01-16","title":"[2.0.0] -- 2025-01-16","text":"<p>This release breaks backwards compatibility for Looper versions &lt; 2.0.0</p>"},{"location":"looper/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>divvy init #520</li> <li>replaced deprecated PEPHubClient function, <code>_load_raw_pep</code> with <code>.load_raw_pep</code></li> <li>looper cli parameters now take priority as originally intended #518</li> <li>fix divvy inspect</li> <li>remove printed dictionary at looper finish #511</li> <li>fix #536</li> <li>fix #522</li> <li>fix #537</li> <li>fix #534</li> </ul>"},{"location":"looper/changelog/#changed_2","title":"Changed","text":"<ul> <li><code>--looper-config</code> is now <code>--config</code>, <code>-c</code>. #455</li> <li>A pipeline interface now consolidates a <code>sample_interface</code> and a <code>project_interface</code> #493</li> <li>Updated documentation for Looper 2.0.0, removing previous versions pepspec PR #34</li> <li>remove position based argument for divvy config, must use --config or run as default config</li> </ul>"},{"location":"looper/changelog/#added","title":"Added","text":"<ul> <li><code>looper init</code> tutorial #466</li> <li>looper config allows for <code>pephub_path</code> in pipestat config section of <code>.looper.yaml</code> #519</li> <li>improve error messaging for bad/malformed looper configurations #515</li> <li>add shortform argument for --package (alias is now -p)</li> </ul>"},{"location":"looper/changelog/#191-2024-07-18","title":"[1.9.1] -- 2024-07-18","text":""},{"location":"looper/changelog/#changed_3","title":"Changed","text":"<ul> <li>ensure peppy requirement peppy&gt;=0.40.0,&lt;=0.40.2</li> </ul>"},{"location":"looper/changelog/#190-2024-06-26","title":"[1.9.0] -- 2024-06-26","text":""},{"location":"looper/changelog/#added_1","title":"Added","text":"<ul> <li>user can now add cli modifiers to looper config instead of PEP project #270</li> <li>pipeline interfaces no longer must be nested under sample and project keys within looper config file #465</li> <li>var_templates can now be hierarchical #334</li> <li>looper can now gracefully halt spawned subprocesses when the user sends a keyboard interrupt #37</li> </ul>"},{"location":"looper/changelog/#181-2024-06-05","title":"[1.8.1] -- 2024-06-05","text":""},{"location":"looper/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>added <code>-v</code> and <code>--version</code> to the CLI</li> <li>fixed running project level with <code>--project</code> argument</li> </ul>"},{"location":"looper/changelog/#180-2024-06-04","title":"[1.8.0] -- 2024-06-04","text":""},{"location":"looper/changelog/#added_2","title":"Added","text":"<ul> <li>looper destroy now destroys individual results when pipestat is configured: https://github.com/pepkit/looper/issues/469</li> <li>comprehensive smoketests: https://github.com/pepkit/looper/issues/464</li> <li>allow rerun to work on both failed or waiting flags: https://github.com/pepkit/looper/issues/463</li> </ul>"},{"location":"looper/changelog/#changed_4","title":"Changed","text":"<ul> <li>Migrated <code>argparse</code> CLI definition to a pydantic basis for all commands. See: https://github.com/pepkit/looper/issues/438</li> <li>during project load, check if PEP file path is a file first, then check if it is a registry path: https://github.com/pepkit/looper/issues/456 </li> <li>Looper now uses FutureYamlConfigManager due to the yacman refactor v0.9.3: https://github.com/pepkit/looper/issues/452</li> </ul>"},{"location":"looper/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>inferring project name when loading PEP from csv: https://github.com/pepkit/looper/issues/484</li> <li>fix inconsistency resolving pipeline interface paths if multiple paths are supplied: https://github.com/pepkit/looper/issues/474</li> <li>fix bug with checking for completed flags: https://github.com/pepkit/looper/issues/470</li> <li>fix looper destroy not properly destroying all related files: https://github.com/pepkit/looper/issues/468</li> <li>looper rerun now only runs failed jobs as intended: https://github.com/pepkit/looper/issues/467 </li> <li>looper inspect now inspects the looper config: https://github.com/pepkit/looper/issues/462</li> <li>Load PEP from CSV: https://github.com/pepkit/looper/issues/456</li> <li>looper now works with sample_table_index https://github.com/pepkit/looper/issues/458</li> </ul>"},{"location":"looper/changelog/#171-2024-05-28","title":"[1.7.1] -- 2024-05-28","text":""},{"location":"looper/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>pin pipestat version to be between pipestat&gt;=0.8.0,&lt;0.9.0  https://github.com/pepkit/looper/issues/494</li> </ul>"},{"location":"looper/changelog/#170-2024-01-26","title":"[1.7.0] -- 2024-01-26","text":""},{"location":"looper/changelog/#added_3","title":"Added","text":"<ul> <li><code>--portable</code> flag to <code>looper report</code> to create a portable version of the html report</li> <li><code>--lump-j</code> allows grouping samples into a defined number of jobs</li> </ul>"},{"location":"looper/changelog/#changed_5","title":"Changed","text":"<ul> <li><code>--lumpn</code> is now <code>--lump-n</code></li> <li><code>--lump</code> is now <code>--lump-s</code></li> <li></li> </ul>"},{"location":"looper/changelog/#160-2023-12-22","title":"[1.6.0] -- 2023-12-22","text":""},{"location":"looper/changelog/#added_4","title":"Added","text":"<ul> <li><code>looper link</code> creates symlinks for results grouped by record_identifier. It requires pipestat to be configured. #72</li> <li>basic tab completion. </li> </ul>"},{"location":"looper/changelog/#changed_6","title":"Changed","text":"<ul> <li>looper now works with pipestat v0.6.0 and greater.</li> <li><code>looper table</code>, <code>check</code> now use pipestat and therefore require pipestat configuration. #390</li> <li>changed how looper configures pipestat #411</li> <li>initializing pipeline interface also writes an example <code>output_schema.yaml</code> and <code>count_lines.sh</code> pipeline</li> </ul>"},{"location":"looper/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>filtering via attributes that are integers.</li> </ul>"},{"location":"looper/changelog/#151-2023-08-14","title":"[1.5.1] -- 2023-08-14","text":""},{"location":"looper/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>fix <code>looper table</code> failing without <code>sample.protocol</code></li> </ul>"},{"location":"looper/changelog/#changed_7","title":"Changed","text":"<ul> <li>correct <code>--looper_config</code> to <code>--looper-config</code></li> </ul>"},{"location":"looper/changelog/#150-2023-08-09","title":"[1.5.0] -- 2023-08-09","text":""},{"location":"looper/changelog/#added_5","title":"Added","text":"<ul> <li>ability to use PEPs from PEPhub without downloading project #341</li> <li>ability to specify pipeline interfaces inside looper config Looper Config</li> <li>divvy re-integrated in looper</li> <li>divvy inspect -p package</li> <li>Looper will now check that the command path provided in the pipeline interface is callable before submitting.</li> </ul>"},{"location":"looper/changelog/#changed_8","title":"Changed","text":"<ul> <li>initialization of generic pipeline interface available using subcommand <code>init-piface</code></li> <li><code>looper report</code> will now use pipestat to generate browsable HTML reports if pipestat is configured.</li> <li>looper now works with pipestat v0.5.0.</li> <li>Removed --toggle-key functionality. </li> <li>Allow for user to input single integer value for --sel-incl or --sel-excl</li> </ul>"},{"location":"looper/changelog/#143-2023-08-01","title":"[1.4.3] -- 2023-08-01","text":""},{"location":"looper/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Fix regression for var_templates expansion.</li> </ul>"},{"location":"looper/changelog/#142-2023-07-31","title":"[1.4.2] -- 2023-07-31","text":""},{"location":"looper/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Fix for expanding paths properly.</li> </ul>"},{"location":"looper/changelog/#141-2023-06-22","title":"[1.4.1] -- 2023-06-22","text":""},{"location":"looper/changelog/#140-2023-04-24","title":"[1.4.0] -- 2023-04-24","text":""},{"location":"looper/changelog/#added_6","title":"Added","text":"<ul> <li>preliminary support for pipestat.</li> <li>ability to skip samples using  <code>-k</code> or <code>--skip</code> #367</li> <li>ability to input a range into <code>limit</code> and <code>skip</code>#367</li> <li><code>limit</code> and <code>skip</code> are now both usable with Destroy and Run. #367</li> <li>ability to generate generic pipeline interface using <code>init -p</code> or <code>init --piface</code> #368</li> <li>Fixed ability to use custom sample index</li> <li>Added <code>write_custom_template</code>, a built-in pre-submit plugin for writing templates</li> </ul>"},{"location":"looper/changelog/#changed_9","title":"Changed","text":"<ul> <li>looper now returns nonzero if any samples fail submission</li> <li>various other developer changes</li> </ul>"},{"location":"looper/changelog/#deprecated","title":"Deprecated","text":"<ul> <li><code>path</code> variable will be deprecated in favor of <code>var_templates</code> #322</li> </ul>"},{"location":"looper/changelog/#132-2022-02-09","title":"[1.3.2] -- 2022-02-09","text":""},{"location":"looper/changelog/#changed_10","title":"Changed","text":"<ul> <li>Fixed bug with use_2to3 for setuptools compatibility.</li> </ul>"},{"location":"looper/changelog/#131-2021-06-18","title":"[1.3.1] -- 2021-06-18","text":""},{"location":"looper/changelog/#changed_11","title":"Changed","text":"<ul> <li>If remote schemas are not accessible, the job submission doesn't fail anymore</li> <li>Fixed a bug where looper stated \"No failed flag found\" when a failed flag was found</li> </ul>"},{"location":"looper/changelog/#deprecated_1","title":"Deprecated","text":"<ul> <li>Fixed and deprecated <code>looper inspect</code>. Use <code>eido inspect</code> from now on.</li> </ul>"},{"location":"looper/changelog/#130-2020-10-07","title":"[1.3.0] -- 2020-10-07","text":""},{"location":"looper/changelog/#added_7","title":"Added","text":"<ul> <li>New plugin system for pre-submission hooks</li> <li>Included plugin functions: <code>write_sample_yaml</code>, <code>write_sample_yaml_prj</code>, <code>write_sample_yaml_cwl</code> and <code>write_submission_yaml</code></li> <li>New <code>var_templates</code> section for defining variables in the pipeline interface</li> </ul>"},{"location":"looper/changelog/#changed_12","title":"Changed","text":"<ul> <li>Pipeline interface specification was updated to accommodate new <code>var_templates</code> section and pre-submission hooks</li> </ul>"},{"location":"looper/changelog/#deprecated_2","title":"Deprecated","text":"<ul> <li>pipeline interface sections:<ul> <li><code>dynamic_variables_command_template</code>, which can now be more simply accomplished with a pre-submission hook</li> <li><code>path</code>, which is replaced by a more generic <code>var_templates</code> section</li> </ul> </li> </ul>"},{"location":"looper/changelog/#121-2020-08-26","title":"[1.2.1] - 2020-08-26","text":""},{"location":"looper/changelog/#added_8","title":"Added","text":"<ul> <li>Environment variables expansion in custom sample YAML paths; Issue 273</li> <li><code>dynamic_variables_script_path</code> key in the pipeline interface. Path, absolute or relative to the pipeline interface file; Issue 276</li> </ul>"},{"location":"looper/changelog/#changed_13","title":"Changed","text":"<ul> <li>Resolve project pipeline interface path by making it relative to the config not current directory; Issue 268</li> </ul>"},{"location":"looper/changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Unclear error when <code>output_dir</code> was not provided in a config <code>looper</code> section; Issue 286</li> </ul>"},{"location":"looper/changelog/#120-2020-05-26","title":"[1.2.0] - 2020-05-26","text":"<p>This version introduced backwards-incompatible changes.</p>"},{"location":"looper/changelog/#added_9","title":"Added","text":"<ul> <li>Commands:<ul> <li><code>init</code>; initializes <code>.looper.yaml</code> file</li> <li><code>inspect</code>; inspects <code>Project</code> or <code>Sample</code> objects</li> <li><code>table</code>; writes summary stats table</li> <li><code>runp</code>; runs project level pipelines</li> </ul> </li> <li>Input schemas and output schemas</li> <li><code>--settings</code> argument to specify compute resources as a YAML file</li> <li>Option to preset CLI options in a dotfile</li> <li><code>--command-extra</code> and <code>--command-extra-override</code> arguments that append specified string to pipeline commands. These functions supersede the previous <code>pipeline_config</code> and <code>pipeline_args</code> sections, which are now deprecated. The new method is more universal, and can accomplish the same functionality but more simply, using the built-in PEP machinery to selectively apply commands to samples.</li> <li>Option to specify destination of sample YAML in pipeline interface</li> <li><code>--pipeline_interfaces</code> argument that allows pipeline interface specification via CLI</li> </ul>"},{"location":"looper/changelog/#changed_14","title":"Changed","text":"<ul> <li><code>looper summarize</code> to <code>looper report</code></li> <li>Pipeline interface format changed drastically</li> <li>The PyPi name changed from 'loopercli' to 'looper'</li> <li>resources section in pipeline interface replaced with <code>size_dependent_attributes</code> or <code>dynamic_variables_command_template</code>.</li> <li><code>--compute</code> can be used to specify arguments other than resources</li> <li><code>all_input_files</code> and <code>required_input_files</code> keys in pipeline interface moved to the input schema and renamed to <code>files</code> and <code>required_files</code></li> <li>pipeline interface specification</li> </ul>"},{"location":"looper/changelog/#0126-2020-02-21","title":"[0.12.6] -- 2020-02-21","text":""},{"location":"looper/changelog/#added_10","title":"Added","text":"<ul> <li>possibility to execute library module as a script: <code>python -m looper ...</code></li> </ul>"},{"location":"looper/changelog/#changed_15","title":"Changed","text":"<ul> <li>in the summary page account for missing values when plotting; the value is disregarded in such a case and plot is still created</li> <li>show 50 rows in the summary table</li> <li>make links to the summary page relative</li> <li>long entries in the sample stats table are truncated with an option to see original value in a popover</li> </ul>"},{"location":"looper/changelog/#fixed_11","title":"Fixed","text":"<ul> <li>inactive jQuery dependent components in the status page</li> <li>project objects layout in the summary index page</li> <li>inactivation of popovers after Bootstrap Table events</li> <li>non-homogeneous status flags appearance</li> </ul>"},{"location":"looper/changelog/#0125-2019-12-13","title":"[0.12.5] -- 2019-12-13","text":""},{"location":"looper/changelog/#changed_16","title":"Changed","text":"<ul> <li>reduce verbosity of missing options; Issue 174</li> <li>switch to Bootstrap Table in the summary index page table and sample status tables</li> </ul>"},{"location":"looper/changelog/#0124-2019-07-18","title":"[0.12.4] -- 2019-07-18","text":""},{"location":"looper/changelog/#added_11","title":"Added","text":"<ul> <li>Ability to declare <code>required_executables</code> in a <code>PipelineInterface</code>, to trigger a naive \"runnability\" check for a sample submission</li> <li>A possibility to opt out of status page inclusion in the navbar</li> </ul>"},{"location":"looper/changelog/#changed_17","title":"Changed","text":"<ul> <li>The status tables now use DataTables jQuery plugin to make them interactive</li> </ul>"},{"location":"looper/changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Navbar links creation</li> </ul>"},{"location":"looper/changelog/#0123-2019-06-20","title":"[0.12.3] -- 2019-06-20","text":""},{"location":"looper/changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Bug in <code>Sample</code> YAML naming, whereby a base <code>Sample</code> was being suffixed as a subtype would be, leading to a pipeline argument based on <code>yaml_file</code> that did not exist on disk.</li> </ul>"},{"location":"looper/changelog/#0122-2019-06-06","title":"[0.12.2] -- 2019-06-06","text":""},{"location":"looper/changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Fixed various bugs related to populating derived attributes, including using attributes like <code>sample_name</code> as keys.</li> <li>Fixed a bug related to singularity attributes not being passed from a pipeline interface file.</li> <li>Fixed several bugs with incorrect version requirements.</li> </ul>"},{"location":"looper/changelog/#0121-2019-05-20","title":"[0.12.1] -- 2019-05-20","text":""},{"location":"looper/changelog/#added_12","title":"Added","text":"<ul> <li>Made <code>looper.Sample</code> include more specific functionality from <code>peppy</code></li> </ul>"},{"location":"looper/changelog/#changed_18","title":"Changed","text":"<ul> <li>Status table creation is possible outside of <code>looper</code>.</li> <li>In the summary index page the plottable columns list is now scrollable</li> <li>Status page relies on the <code>profile.tsv</code> file rather than <code>*.log</code>; Issue 159</li> </ul>"},{"location":"looper/changelog/#fixed_15","title":"Fixed","text":"<ul> <li>In HTML reporting module, do not ignore objects which are neither HTMLs nor images in the summary, e.g. CSVs</li> <li>Restore parsing and application of pipeline-level computing resource specification from a pipeline interface file; Issue 184</li> <li>Allow <code>ignore_flags</code> to properly modulate submission messaging; Issue 179</li> <li>Do not display time-like summary columns as the plottable ones; Issue 182</li> </ul>"},{"location":"looper/changelog/#0120-2019-05-03","title":"[0.12.0] -- 2019-05-03","text":""},{"location":"looper/changelog/#added_13","title":"Added","text":"<ul> <li>First implementation of pipeline interface 'outputs', so pipeline authors can specify items of interest produced by the pipeline.</li> <li>Functions and attributes on <code>Project</code> to support \"outputs\" (<code>interfaces</code>, <code>get_interfaces</code>, <code>get_outputs</code>)</li> </ul>"},{"location":"looper/changelog/#changed_19","title":"Changed","text":"<ul> <li>Start \"compute\" --&gt; \"compute_packages\" transition</li> <li><code>get_logger</code> moved to <code>peppy</code></li> </ul>"},{"location":"looper/changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Prevent CLI option duplication in pipeline commands generated</li> <li>Make functional CLI spec of particular attribute on which to base selection of a subset of a project's samples (<code>peppy</code> 298)</li> </ul>"},{"location":"looper/changelog/#0111-2019-04-17","title":"[0.11.1] -- 2019-04-17","text":""},{"location":"looper/changelog/#changed_20","title":"Changed","text":"<ul> <li>Improved documentation</li> <li>Improved interaction with <code>peppy</code> and <code>divvy</code> dependencies</li> </ul>"},{"location":"looper/changelog/#011-2019-04-17","title":"[0.11] -- 2019-04-17","text":""},{"location":"looper/changelog/#added_14","title":"Added","text":"<ul> <li>Implemented <code>looper rerun</code> command.</li> <li>Support use of custom <code>resources</code> in pipeline's <code>compute</code> section</li> <li>Listen for itemized compute resource specification on command-line with <code>--resources</code></li> <li>Support pointing to <code>Project</code> config file with folder path rather than full filepath</li> <li>Add <code>selector-attribute</code> parameter for more generic sample selection.</li> </ul>"},{"location":"looper/changelog/#changed_21","title":"Changed","text":"<ul> <li>Switched to a Jinja-style templating system for summary output</li> <li>Made various UI changes to adapt to <code>caravel</code> use.</li> <li>Using <code>attmap</code> for \"attribute-style key-vale store\" implementation</li> <li>Removed Python 3.4 support.</li> <li>UI: change parameter names <code>in/exclude-samples</code> to <code>selector-in/exclude</code>.</li> </ul>"},{"location":"looper/changelog/#0100-2018-12-20","title":"[0.10.0] -- 2018-12-20","text":""},{"location":"looper/changelog/#changed_22","title":"Changed","text":"<ul> <li><code>PipelineInterface</code> now derives from <code>peppy.AttributeDict</code>.</li> <li>On <code>PipelineInterface</code>, iteration over pipelines now is with <code>iterpipes</code>.</li> <li>Rename <code>parse_arguments</code> to <code>build_parser</code>, which returns <code>argparse.ArgumentParser</code> object</li> <li>Integers in HTML reports are made more human-readable by including commas.</li> <li>Column headers in HTML reports are now strictly for sorting; there's a separate list for plottable columns.</li> <li>More informative error messages</li> <li>HTML samples list is fully populated.</li> <li>Existence of an object lacking an anchor image is no longer problematic for <code>summarize</code>.</li> <li>Basic package test in Python 3 now succeeds: <code>python3 setup.py test</code>.</li> </ul>"},{"location":"looper/changelog/#v092-2018-11-12","title":"[v0.9.2] -- 2018-11-12","text":""},{"location":"looper/changelog/#changed_23","title":"Changed","text":"<ul> <li>Fixed bugs with <code>looper summarize</code> when no summarizers were present</li> <li>Added CLI flag to force <code>looper destroy</code> for programmatic access</li> <li>Fixed a bug for samples with duplicate names</li> <li>Added new display features (graphs, table display) for HTML summary output.</li> </ul>"},{"location":"looper/changelog/#091-2018-06-30","title":"[0.9.1] -- 2018-06-30","text":""},{"location":"looper/changelog/#changed_24","title":"Changed","text":"<ul> <li>Fixed several bugs with <code>looper summarize</code> that caused failure on edge cases.</li> </ul>"},{"location":"looper/changelog/#090-2018-06-25","title":"[0.9.0] -- 2018-06-25","text":""},{"location":"looper/changelog/#added_15","title":"Added","text":"<ul> <li>Support for custom summarizers</li> <li>Add <code>allow-duplicate-names</code> command-line options</li> <li>Allow any variables in environment config files or other <code>compute</code> sections to be used in submission templates. This allows looper to be used with containers.</li> <li>Add nice universal project-level HTML reporting</li> </ul>"},{"location":"looper/changelog/#081-2018-04-02","title":"[0.8.1] -- 2018-04-02","text":""},{"location":"looper/changelog/#changed_25","title":"Changed","text":"<ul> <li>Minor documentation and packaging updates for first Pypi release.</li> <li>Fix a bug that incorrectly mapped protocols due to case sensitive issues</li> <li>Fix a bug with <code>report_figure</code> that made it output pandas code</li> </ul>"},{"location":"looper/changelog/#080-2018-01-19","title":"[0.8.0] -- 2018-01-19","text":""},{"location":"looper/changelog/#changed_26","title":"Changed","text":"<ul> <li>Use independent <code>peppy</code> package, replacing <code>models</code> module for core data types.</li> <li>Integrate <code>ProtocolInterface</code> functionality into <code>PipelineInterface</code>.</li> </ul>"},{"location":"looper/changelog/#072-2017-11-16","title":"[0.7.2] -- 2017-11-16","text":""},{"location":"looper/changelog/#changed_27","title":"Changed","text":"<ul> <li>Correctly count successful command submissions when not using <code>--dry-run</code>.</li> </ul>"},{"location":"looper/changelog/#071-2017-11-15","title":"[0.7.1] -- 2017-11-15","text":""},{"location":"looper/changelog/#changed_28","title":"Changed","text":"<ul> <li>No longer falsely display that there's a submission failure.</li> <li>Allow non-string values to be unquoted in the <code>pipeline_args</code> section.</li> </ul>"},{"location":"looper/changelog/#07-2017-11-15","title":"[0.7] -- 2017-11-15","text":""},{"location":"looper/changelog/#added_16","title":"Added","text":"<ul> <li>Add <code>--lump</code> and <code>--lumpn</code> options</li> <li>Catch submission errors from cluster resource managers</li> <li>Implied columns can now be derived</li> <li>Now protocols can be specified on the command-line <code>--include-protocols</code></li> <li>Add rudimentary figure summaries</li> <li>Simplifies command-line help display</li> <li>Allow wildcard protocol_mapping for catch-all pipeline assignment</li> <li>Improve user messages</li> <li>New sample_subtypes section in pipeline_interface</li> </ul>"},{"location":"looper/changelog/#changed_29","title":"Changed","text":"<ul> <li>Sample child classes are now defined explicitly in the pipeline interface. Previously, they were guessed based on presence of a class extending Sample in a pipeline script.</li> <li>Changed 'library' key sample attribute to 'protocol'</li> </ul>"},{"location":"looper/changelog/#06-2017-07-21","title":"[0.6] -- 2017-07-21","text":""},{"location":"looper/changelog/#added_17","title":"Added","text":"<ul> <li>Add support for implied_column section of the project config file</li> <li>Add support for Python 3</li> <li>Merges pipeline interface and protocol mappings. This means we now allow direct pointers to <code>pipeline_interface.yaml</code> files, increasing flexibility, so this relaxes the specified folder structure that was previously used for <code>pipelines_dir</code> (with <code>config</code> subfolder).</li> <li>Allow URLs as paths to sample sheets.</li> <li>Allow tsv format for sample sheets.</li> <li>Checks that the path to a pipeline actually exists before writing the submission script.</li> </ul>"},{"location":"looper/changelog/#changed_30","title":"Changed","text":"<ul> <li>Changed LOOPERENV environment variable to PEPENV, generalizing it to generic models</li> <li>Changed name of <code>pipelines_dir</code> to <code>pipeline_interfaces</code> (but maintained backwards compatibility for now).</li> <li>Changed name of <code>run</code> column to <code>toggle</code>, since <code>run</code> can also refer to a sequencing run.</li> <li>Relaxes many constraints (like resources sections, pipelines_dir columns), making project configuration files useful outside looper. This moves us closer to dividing models from looper, and improves flexibility.</li> <li>Various small bug fixes and dev improvements.</li> <li>Require <code>setuptools</code> for installation, and <code>pandas 0.20.2</code>. If <code>numexpr</code> is installed, version <code>2.6.2</code> is required.</li> <li>Allows tilde in <code>pipeline_interfaces</code></li> </ul>"},{"location":"looper/changelog/#05-2017-03-01","title":"[0.5] -- 2017-03-01","text":""},{"location":"looper/changelog/#added_18","title":"Added","text":"<ul> <li>Add new looper version tracking, with <code>--version</code> and <code>-V</code> options and printing version at runtime</li> <li>Add support for asterisks in file paths</li> <li>Add support for multiple pipeline directories in priority order</li> <li>Revamp of messages make more intuitive output</li> <li>Colorize output</li> <li>Complete rehaul of logging and test infrastructure, using logging and pytest packages</li> </ul>"},{"location":"looper/changelog/#changed_31","title":"Changed","text":"<ul> <li>Removes pipelines_dir requirement for models, making it useful outside looper</li> <li>Small bug fixes related to <code>all_input_files</code> and <code>required_input_files</code> attributes</li> <li>More robust installation and more explicit requirement of Python 2.7</li> </ul>"},{"location":"looper/changelog/#04-2017-01-12","title":"[0.4] -- 2017-01-12","text":""},{"location":"looper/changelog/#added_19","title":"Added","text":"<ul> <li>New command-line interface (CLI) based on sub-commands</li> <li>New subcommand (<code>looper summarize</code>) replacing the <code>summarizePipelineStats.R</code> script</li> <li>New subcommand (<code>looper check</code>) replacing the <code>flagCheck.sh</code> script</li> <li>New command (<code>looper destroy</code>) to remove all output of a project</li> <li>New command (<code>looper clean</code>) to remove intermediate files of a project flagged for deletion</li> <li>Support for portable and pipeline-independent allocation of computing resources with Looperenv.</li> </ul>"},{"location":"looper/changelog/#changed_32","title":"Changed","text":"<ul> <li>Removed requirement to have <code>pipelines</code> repository installed in order to extend base Sample objects</li> <li>Maintenance of sample attributes as provided by user by means of reading them in as strings (to be improved further)</li> <li>Improved serialization of Sample objects</li> </ul>"},{"location":"looper/contributing/","title":"Contributing","text":"<p>Pull requests or issues are welcome.</p> <ul> <li>After adding tests in <code>tests</code> for a new feature or a bug fix, please run the test suite.</li> <li> <p>To do so, the only additional dependencies needed beyond those for the package can be installed with:</p> <p><code>pip install -r requirements/requirements-test.txt</code></p> </li> <li> <p>Once those are installed, the tests can be run with <code>pytest</code> or <code>python setup.py test</code>.</p> </li> </ul>"},{"location":"looper/faq/","title":"FAQ","text":""},{"location":"looper/faq/#what-kind-of-pipelines-can-looper-run","title":"What kind of pipelines can <code>looper</code> run?","text":"<p><code>Looper</code> can run samples through any pipeline that runs on the command line. The flexible pipeline interface file allows <code>looper</code> to execute arbitrary shell commands. A pipeline may consist of scripts in languages like Perl, Python, or bash, or it may be built with a particular framework. Typically, we use Python pipelines built using the <code>pypiper</code> package, which provides some additional power to <code>looper</code>, but that's optional.</p>"},{"location":"looper/faq/#why-isnt-the-looper-executable-available-on-path","title":"Why isn't the <code>looper</code> executable available on <code>PATH</code>?","text":"<p>By default, Python packages are installed to <code>~/.local/bin</code>. You can add that location to your path by appending it (<code>export PATH=$PATH:~/.local/bin</code>).</p>"},{"location":"looper/faq/#how-can-i-run-my-jobs-on-a-cluster","title":"How can I run my jobs on a cluster?","text":"<p>Looper uses the external package divvy for cluster computing, making it flexible enough to use with any cluster resource environment. Please see the tutorial on cluster computing with looper and divvy.</p>"},{"location":"looper/faq/#whats-the-difference-between-looper-and-pypiper","title":"What's the difference between <code>looper</code> and <code>pypiper</code>?","text":"<p><code>pypiper</code> is a more traditional workflow-building framework; it helps you build pipelines to process individual samples. <code>looper</code> is completely pipeline-agnostic, and has nothing to do with individual processing steps; it operates groups of samples (as in a project), submitting the appropriate pipeline(s) to a cluster or server (or running them locally). The two projects are independent and can be used separately, but they are most powerful when combined. They complement one another, together constituting a comprehensive pipeline management system.</p>"},{"location":"looper/faq/#why-isnt-a-sample-being-processed-by-a-pipeline-not-submitting-flag-found-_statusflag","title":"Why isn't a sample being processed by a pipeline (<code>Not submitting, flag found: ['*_&lt;status&gt;.flag']</code>)?","text":"<p>When using the <code>run</code> subcommand, for each sample being processed <code>looper</code> first checks for \"flag\" files in the sample's designated output folder for flag files (which can be <code>_completed.flag</code>, or <code>_running.flag</code>, or <code>_failed.flag</code>).  Typically, we don't want to resubmit a job that's already running or already finished, so by default, <code>looper</code> will not submit a job when it finds a flag file. This is what the message above is indicating.</p> <p>If you do in fact want to re-rerun a sample (maybe you've updated the pipeline, or you want to run restart a failed attempt), you can do so by just passing to <code>looper</code> at startup the <code>--ignore-flags</code> option; this will skip the flag check for all samples. If you only want to re-run or restart a few samples, it's best to just delete the flag files for the samples you want to restart, then use <code>looper run</code> as normal.</p> <p>You may be interested in the usage docs for the <code>looper rerun</code> command, which runs any failed samples.</p>"},{"location":"looper/faq/#how-can-i-resubmit-a-subset-of-jobs-that-failed","title":"How can I resubmit a subset of jobs that failed?","text":"<p>As of version <code>0.11</code>, you can use <code>looper rerun</code> to submit only jobs with a <code>failed</code> flag. By default, <code>looper</code> will not submit a job that has already run. If you want to restart a sample (maybe you've updated the pipeline, or you want to restart a failed attempt), you can either use <code>looper rerun</code> to restart only failed jobs, or you pass <code>--ignore-flags</code>, which will resubmit all samples. If you want more specificity, you can just manually delete the \"flag\" files for the samples you want to restart, then use <code>looper run</code> as normal.</p>"},{"location":"looper/faq/#why-are-computing-resources-defined-in-the-pipeline-interface-file-instead-of-in-the-divvy-computing-configuration-file","title":"Why are computing resources defined in the pipeline interface file instead of in the <code>divvy</code> computing configuration file?","text":"<p>You may notice that the compute config file does not specify resources to request (like memory, CPUs, or time). Yet, these are required in order to submit a job to a cluster. Resources are not handled by the divcfg file because they not relative to a particular computing environment; instead they vary by pipeline and sample. As such, these items should be defined at other stages.</p> <p>Resources defined in the <code>pipeline_interface.yaml</code> file that connects looper to a pipeline. The reason for this is that pipeline developers are the most likely to know what sort of resources their pipeline requires, so they are in the best position to define the resources requested. For more information on how to adjust resources, see the <code>compute</code> section of the pipeline interface page.</p>"},{"location":"looper/faq/#which-configuration-file-has-which-settings","title":"Which configuration file has which settings?","text":"<p>There's information on the initial tutorial page.</p>"},{"location":"looper/install/","title":"Installing","text":"<p>Install using <code>pip</code>:</p> <pre><code>pip install looper\n</code></pre> <p>Update with:</p> <pre><code>pip install --user --upgrade looper\n</code></pre> <p>If the <code>looper</code> executable in not automatically in your <code>$PATH</code>, add the following line to your <code>.bashrc</code> or <code>.profile</code>:</p> <pre><code>export PATH=~/.local/bin:$PATH\n</code></pre> <p>Or, you can install from releases posted on GitHub.</p>"},{"location":"looper/support/","title":"Support","text":"<p>Please use the issue tracker at GitHub to file bug reports or feature requests.</p> <p>Looper supports Python 3, and has been tested in Linux. If you clone this repository and then an attempt at local installation, e.g. with <code>pip install --upgrade ./</code>, fails, this may be due to an issue with <code>setuptools</code> and <code>six</code>. A <code>FileNotFoundError</code> (Python 3) or an <code>IOError</code> (Python2), with a message/traceback about a nonexistent <code>METADATA</code> file means that this is even more likely the cause. To get around this, you can first manually <code>pip install --upgrade six</code> or <code>pip install six==1.11.0</code>, as upgrading from <code>six</code> from 1.10.0 to 1.11.0 resolves this issue, then retry the <code>looper</code> installation.</p>"},{"location":"looper/usage/","title":"Usage reference","text":"<p>Looper doesn't just run pipelines; it can also check and summarize the progress of your jobs, as well as remove all files created by them.</p> <p>Each task is controlled by one of the following commands: <code>run</code>, <code>rerun</code>, <code>runp</code> , <code>table</code>,<code>report</code>, <code>destroy</code>, <code>check</code>, <code>clean</code>, <code>inspect</code>, <code>init</code></p> <ul> <li> <p><code>looper run</code>:  Runs pipelines for each sample, for each pipeline. This will use your <code>compute</code> settings to build and submit scripts to your specified compute environment, or run them sequentially on your local computer.</p> </li> <li> <p><code>looper runp</code>:  Runs pipelines for each pipeline for project.</p> </li> <li> <p><code>looper rerun</code>: Exactly the same as <code>looper run</code>, but only runs jobs with a failed flag.</p> </li> <li> <p><code>looper report</code>: Summarize your project results in a form of browsable HTML pages.</p> </li> <li> <p><code>looper table</code>: This command parses all key-value results reported in the each sample <code>stats.tsv</code> and collates them into a large summary matrix, which it saves in the project output directory. This creates such a matrix for each pipeline type run on the project, and a combined master summary table</p> </li> <li> <p><code>looper check</code>: Checks the run progress of the current project. This will display a summary of job status; which pipelines are currently running on which samples, which have completed, which have failed, etc.</p> </li> <li> <p><code>looper destroy</code>: Deletes all output results for this project.</p> </li> <li> <p><code>looper inspect</code>: Display the Project or Sample information</p> </li> <li> <p><code>looper init</code>: Initialize a looper dotfile (<code>.looper.yaml</code>) in the current directory</p> </li> </ul> <p>Here you can see the command-line usage instructions for the main looper command and for the run subcommand. All commands can be investigated using <code>looper command --help</code>.</p>"},{"location":"looper/usage/#looper-help","title":"<code>looper --help</code>","text":"<pre><code>usage: looper [-h] [-v] [--silent] [--verbosity VERBOSITY] [--logdev] {run,rerun,runp,table,report,destroy,check,clean,init,init_piface,link,inspect} ...\n\nLooper: A job submitter for Portable Encapsulated Projects\n\ncommands:\n  {run,rerun,runp,table,report,destroy,check,clean,init,init_piface,link,inspect}\n    run                 Run or submit sample jobs.\n    rerun               Resubmit sample jobs with failed flags.\n    runp                Run or submit project jobs.\n    table               Write summary stats table for project samples.\n    report              Create browsable HTML report of project results.\n    destroy             Remove output files of the project.\n    check               Check flag status of current runs.\n    clean               Run clean scripts of already processed jobs.\n    init                Initialize looper config file.\n    init_piface         Initialize generic pipeline interface.\n    link                Create directory of symlinks for reported results.\n    inspect             Print information about a project.\n\noptional arguments:\n  --silent              Whether to silence logging (default: False)\n  --verbosity VERBOSITY\n                        Alternate mode of expression for logging level that better accords with intuition about how to convey this. (default: None)\n  --logdev              Whether to log in development mode; possibly among other behavioral changes to logs handling, use a more information-rich message format template. (default: False)\n\nhelp:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n</code></pre>"},{"location":"looper/usage/#looper-run-help","title":"<code>looper run --help</code>","text":"<pre><code>usage: looper run [-h] [-i] [-t TIME_DELAY] [-d] [-x COMMAND_EXTRA] [-y COMMAND_EXTRA_OVERRIDE] [-u LUMP] [-n LUMP_N] [-j LUMP_J] [--divvy DIVVY] [-f] [--compute COMPUTE [COMPUTE ...]] [--package PACKAGE] [--settings SETTINGS]\n                  [--exc-flag EXC_FLAG [EXC_FLAG ...]] [--sel-flag SEL_FLAG [SEL_FLAG ...]] [--sel-attr SEL_ATTR] [--sel-incl SEL_INCL [SEL_INCL ...]] [--sel-excl SEL_EXCL] [-l LIMIT] [-k SKIP] [--pep-config PEP_CONFIG]\n                  [-o OUTPUT_DIR] [-c CONFIG] [-S SAMPLE_PIPELINE_INTERFACES [SAMPLE_PIPELINE_INTERFACES ...]] [-P PROJECT_PIPELINE_INTERFACES [PROJECT_PIPELINE_INTERFACES ...]] [--pipestat PIPESTAT] [--amend AMEND [AMEND ...]]\n                  [--project]\n\noptional arguments:\n  -i, --ignore-flags    Ignore run status flags (default: False)\n  -t TIME_DELAY, --time-delay TIME_DELAY\n                        Time delay in seconds between job submissions (min: 0, max: 30) (default: 0)\n  -d, --dry-run         Don't actually submit jobs (default: False)\n  -x COMMAND_EXTRA, --command-extra COMMAND_EXTRA\n                        String to append to every command (default: )\n  -y COMMAND_EXTRA_OVERRIDE, --command-extra-override COMMAND_EXTRA_OVERRIDE\n                        Same as command-extra, but overrides values in PEP (default: )\n  -u LUMP, --lump LUMP  Total input file size (GB) to batch into one job (default: None)\n  -n LUMP_N, --lump-n LUMP_N\n                        Number of commands to batch into one job (default: None)\n  -j LUMP_J, --lump-j LUMP_J\n                        Lump samples into number of jobs. (default: None)\n  --divvy DIVVY         Path to divvy configuration file. Default=$DIVCFG env variable. Currently: not set (default: None)\n  -f, --skip-file-checks\n                        Do not perform input file checks (default: False)\n  --compute COMPUTE [COMPUTE ...]\n                        List of key-value pairs (k1=v1) (default: [])\n  --package PACKAGE     Name of computing resource package to use (default: None)\n  --settings SETTINGS   Path to a YAML settings file with compute settings (default: )\n  --exc-flag EXC_FLAG [EXC_FLAG ...]\n                        Sample exclusion flag (default: [])\n  --sel-flag SEL_FLAG [SEL_FLAG ...]\n                        Sample selection flag (default: [])\n  --sel-attr SEL_ATTR   Attribute for sample exclusion OR inclusion (default: toggle)\n  --sel-incl SEL_INCL [SEL_INCL ...]\n                        Include only samples with these values (default: [])\n  --sel-excl SEL_EXCL   Exclude samples with these values (default: )\n  -l LIMIT, --limit LIMIT\n                        Limit to n samples (default: None)\n  -k SKIP, --skip SKIP  Skip samples by numerical index (default: None)\n  --pep-config PEP_CONFIG\n                        PEP configuration file (default: None)\n  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        Output directory (default: None)\n  -c CONFIG, --config CONFIG\n                        Looper configuration file (YAML) (default: None)\n  -S SAMPLE_PIPELINE_INTERFACES [SAMPLE_PIPELINE_INTERFACES ...], --sample-pipeline-interfaces SAMPLE_PIPELINE_INTERFACES [SAMPLE_PIPELINE_INTERFACES ...]\n                        Paths to looper sample pipeline interfaces (default: [])\n  -P PROJECT_PIPELINE_INTERFACES [PROJECT_PIPELINE_INTERFACES ...], --project-pipeline-interfaces PROJECT_PIPELINE_INTERFACES [PROJECT_PIPELINE_INTERFACES ...]\n                        Paths to looper project pipeline interfaces (default: [])\n  --pipestat PIPESTAT   Path to pipestat files. (default: None)\n  --amend AMEND [AMEND ...]\n                        List of amendments to activate (default: [])\n  --project             Is this command executed for project-level? (default: False)\n\nhelp:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"looper/using-divvy/","title":"Using divvy","text":"<p>Learning objectives</p> <ul> <li>What is divvy?</li> <li>How can I use it in the CLI?</li> <li>How can I use divvy in the python API?</li> <li>How can I use divvy to submit one-off jobs?</li> </ul> <p>Previously you may have read about using divvy for running job in containers or on clusters.</p> <p>Let's recap what is divvy and why it exists.</p>"},{"location":"looper/using-divvy/#what-is-divvy","title":"What is divvy?","text":"<p>Divvy is a submodule of looper, and it allows you to populate job submission scripts by integrating job-specific settings with separately configured computing environment settings. Divvy makes software portable, so users may easily toggle among any computing resource (laptop, cluster, cloud).</p>"},{"location":"looper/using-divvy/#what-makes-divvy-better","title":"What makes divvy better?","text":"<p>Tools require a particular compute resource setup. For example, one pipeline requires SLURM, another requires AWS, and yet another just runs directly on your laptop. This makes it difficult to transfer to different environments. For tools that can run in multiple environments, each one must be configured separately.</p> <p> Instead, divvy-compatible tools can run on any computing resource. Users configure their computing environment once, and all divvy-compatible tools will use this same configuration.</p> <p>Divvy reads a standard configuration file describing available compute resources and then uses a simple template system to write custom job submission scripts. Computing resources are organized as compute packages, which users select, populate with values, and build scripts for compute jobs.</p>"},{"location":"looper/using-divvy/#quick-start-with-cli","title":"Quick start with CLI","text":"<p>divvy comes installed with looper, so ensure you have the newest version of looper installed: <code>pip install looper</code></p> <p>Let's first use <code>divvy list</code> to show us our available computing packages:</p> <pre><code>divvy list\n</code></pre> CLI output<pre><code>Using default divvy config. You may specify in env var: ['DIVCFG']\nUsing divvy config: /home/drc/GITHUB/looper/master/looper/venv/lib/python3.10/site-packages/looper/default_config/divvy_config.yaml\nAvailable compute packages:\n\nlocal\nslurm\nsingularity\nbulker_local\ndefault\nsingularity_slurm\ndocker\n</code></pre> <p>Note that divvy will default to a <code>divvy_config.yaml</code> if none is provided via the environment variable DIVCFG. This default config is provided by looper.</p> <p>We can take at one of the above templates: <pre><code> cat /home/drc/GITHUB/looper/master/looper/venv/lib/python3.10/site-packages/looper/default_config/divvy_templates/slurm_template.sub\n</code></pre></p> CLI output<pre><code>#!/bin/bash\n#SBATCH --job-name='{JOBNAME}'\n#SBATCH --output='{LOGFILE}'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='{CORES}'\n#SBATCH --time='{TIME}'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{CODE}\n</code></pre> <p>Remember that compute packages use nested templates to wrap the pipeline  <code>{CODE}</code> provided by looper's pipeline interface. More can be read here: advanced computing</p> <p>Divvy allows you to write your own submission script via the CLI. However, you will need a way to tell divvy the compute settings. One way is to use an input settings file.</p> <p>Let's create one now: <pre><code>touch my_job_settings.yaml\n</code></pre></p> <p>Add these settings:</p> my_job_settings.yaml<pre><code>time: 4-0-0\nlogfile: results.log\ncores: 6\npartition: large_mem\nmem: 16G\n</code></pre> <p>Now that you have this setting file you can write a new submission script using <code>divvy write</code>:</p> <pre><code>divvy write --package slurm  --settings my_job_settings.yaml --compute sample=sample1  --outfile submit_script.sub\n</code></pre> <p>This will produce the <code>submit_script.sub</code> file.</p> <pre><code>cat submit_script.sub\n</code></pre> CLI output<pre><code>#!/bin/bash\n#SBATCH --job-name='{JOBNAME}'\n#SBATCH --output='results.log'\n#SBATCH --mem='16G'\n#SBATCH --cpus-per-task='6'\n#SBATCH --time='4-0-0'\n#SBATCH --partition='large_mem'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{CODE}\n</code></pre> <p>We populated several variables, like <code>{LOGFILE}</code> and <code>{TIME}</code>, from the <code>settings.yaml</code> file. However, the <code>{CODE}</code> and <code>{JOBNAME}</code> variables are still unpopulated, so this submission script is incomplete. To remedy this, we'll use <code>divvy</code>'s command-line variable passing: any non-interpreted arguments passed to <code>divvy</code> are assumed to be variables to populate the template. These command-line variables are considered highest priority and so will override any values in the more distant locations. For example:</p> <pre><code>divvy write --package slurm  --settings my_job_settings.yaml  --compute sample=sample1 code=run-this-cmd jobname=12345  --outfile submit_script.sub\n</code></pre> <p>Now if we look at the file: CLI output<pre><code>#!/bin/bash\n#SBATCH --job-name='12345'\n#SBATCH --output='results.log'\n#SBATCH --mem='16G'\n#SBATCH --cpus-per-task='6'\n#SBATCH --time='4-0-0'\n#SBATCH --partition='large_mem'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\nrun-this-cmd\n</code></pre></p> <p>Now we have a complete script, which we can run with <code>sbatch test.sub</code>. Notice also that the <code>time</code> variable uses the one provided on the CLI rather than the one provided in the <code>settings.yaml</code> file, because the CLI has a higher priority.</p> <p>Variables can come from these 3 sources, in order of increasing priority: 1) compute package (defined in the <code>divvy</code> configuration file and selected with the <code>-p</code> or <code>--package</code> argument); 2) <code>settings.yaml</code> file, passed with <code>-s</code> or <code>--settings</code>; 3) any additional variables passed on the command line as key-value pairs to <code>-c</code>.</p> <p>Finally, you can submit these jobs using <code>divvy submit</code>.</p> <pre><code>divvy submit --package slurm  --settings my_job_settings.yaml  --compute sample=sample1 code=run-this-cmd jobname=12345  --outfile submit_script.sub\n</code></pre> <p>Note, <code>slurm</code> requires <code>sbatch</code> to work which will be found HPCs and not my local machine. However, I can use the <code>local</code> template to run jobs locally!</p> <pre><code>divvy submit --package local  --settings my_job_settings.yaml --compute sample=sample1 code=run-this-cmd jobname=12345  --outfile submit_script.sub\n</code></pre>"},{"location":"looper/using-divvy/#using-the-python-api-for-divvy","title":"Using the Python API for divvy","text":"<p>You can also call divvy from a python script. You'll need to provide a path to a divvy config file:</p> divvy_usage.py<pre><code>import looper.divvy as divvy\n\n\nconfig_path = \"/home/drc/GITHUB/looper/master/looper/venv/lib/python3.10/site-packages/looper/default_config/divvy_config.yaml\"\n\n# Now create a ComputingConfiguration object\ndcc = divvy.ComputingConfiguration.from_yaml_file(filepath=config_path)\n</code></pre> <p>Once a ComputingConfiguration object has been created, you can see the default compute package as well as the associated submission script:</p> sub section of divvy_usage.py<pre><code># Print default compute package to terminal\nprint(dcc.compute)\n\n# Access the submission template associated with the compute package.\nwith open(dcc.compute['submission_template']) as f:\n    print(f.read())\n</code></pre> console output<pre><code>submission_template: /home/drc/PythonProjects/testing_looper_docs_tutorial/looper_tutorial/.venv/lib/python3.10/site-packages/looper/default_config/divvy_templates/localhost_template.sub\nsubmission_command: .\n\n#!/bin/bash\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{\n{CODE}\n} | tee {LOGFILE}\n</code></pre> <p>You can also populate the submission script via the API:</p> sub section of divvy_usage.py<pre><code># Populate and create a local submission script\ndcc.write_script(\"test_local.sub\", {\"code\": \"run-this-command\", \"logfile\": \"logfile.txt\"})\n\n# Take a look at the newly populated submission template\nwith open(\"test_local.sub\") as f:\n    print(f.read())\n</code></pre> console output<pre><code>Writing script to /home/drc/PythonProjects/testing_looper_docs_tutorial/looper_tutorial/test_local.sub\n\n#!/bin/bash\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{\nrun-this-command\n} | tee logfile.txt\n</code></pre> <p>You can also activate a computing package via the API:</p> sub section of divvy_usage.py<pre><code># You can also activate a package\nprint(dcc.activate_package(\"slurm\"))\n</code></pre> console output<pre><code>Activating compute package 'slurm'\n</code></pre> <p>Finally, you can also submit jobs:</p> sub section of divvy_usage.py<pre><code>dcc.submit(output_path=\"test_local.sub\")\n</code></pre> <p>Here is the final completed script used for this tutorial:</p> divvy_usage.py<pre><code>import looper.divvy as divvy\n\nconfig_path = \"/home/drc/GITHUB/looper/master/looper/venv/lib/python3.10/site-packages/looper/default_config/divvy_config.yaml\"\ndcc = divvy.ComputingConfiguration.from_yaml_file(filepath=config_path)\n\n# Print default compute package to terminal\nprint(dcc.compute)\n\n# Access the submission template associated with the compute package.\nwith open(dcc.compute['submission_template']) as f:\n    print(f.read())\n\n# Populate and create a local submission script\ndcc.write_script(\"test_local.sub\", {\"code\": \"run-this-command\", \"logfile\": \"logfile.txt\"})\n\n# Take a look at the newly populated submission template\nwith open(\"test_local.sub\") as f:\n    print(f.read())\n\n# You can also activate a package\ndcc.activate_package(\"slurm\")\n\n# You can also submit jobs \ndcc.submit(output_path=\"test_local.sub\")\n</code></pre>"},{"location":"looper/advanced-guide/advanced-computing/","title":"Customizing compute with looper's nested template system","text":""},{"location":"looper/advanced-guide/advanced-computing/#introduction","title":"Introduction","text":"<p>Looper builds job scripts using a nested template system, which consists of an inner and outer template. The inner template, called the command template, generates the individual commands to execute. The outer template, known as the submission template, wraps these commands in environment-handling code. This nested approach separates the computing environment from the pipeline, enhancing portability.</p> <p>This more advanced guide assumes you're already familiar with the \"configuring cluster computing\" chapter of the basic tutorial. We will now explain the nested templates in more detail and then explain how you can customize submission templates to fit just about any computing scenario.</p> <p>Learning objectives</p> <ul> <li>What is looper's nested template system?</li> <li>Why does looper use nested templates?</li> <li>What is the difference between command and submission templates?</li> <li>How can I configure looper to submit jobs in different ways?</li> <li>How do I create custom looper compute packages?</li> </ul>"},{"location":"looper/advanced-guide/advanced-computing/#why-use-nested-templates","title":"Why use nested templates?","text":"<p>Looper\u2019s nested templates are a reflection of its modular design philosophy, aimed at separation of concerns. The system distinguishes between how to run a pipeline and how to configure the environment where it runs. These tasks are done by different people because the pipeline author is not necessarily the same as the pipeline user.</p> <ul> <li>The pipeline author designs how the pipeline runs, but does not know the user's computing environment.</li> <li>The user knows their environment well, but may not be familiar with the pipeline's specifics.</li> </ul> <p>This separation addresses a common frustration with pipelines that hard-code job submission mechanisms, making them difficult to adapt to different environments. By clearly separating the configuration of pipeline execution from the environment setup, looper allows both roles to operate independently.</p>"},{"location":"looper/advanced-guide/advanced-computing/#command-vs-submission-templates","title":"Command vs Submission Templates","text":"<p>The command template, written by the pipeline author, is specified in the pipeline interface file. Meanwhile, the pipeline user defines the outer submission template using submission templates (via divvy, more on that in a moment). This setup offers several benefits:</p> <ol> <li>Commands can be executed in any computing environment by simply switching the submission template.</li> <li>The submission template can handle various environment parameters, such as containers.</li> <li>Each submission template only needs to be defined once per environment, making it reusable across multiple pipelines.</li> <li>The universal submission template can be managed by dedicated submission software.</li> <li>Pipeline authors can focus solely on pipeline design, without worrying about environment configuration. </li> </ol>"},{"location":"looper/advanced-guide/advanced-computing/#template-examples","title":"Template examples","text":"<p>We've seen examples of two templates in the previous tutorials, but to briefly remind you:</p>"},{"location":"looper/advanced-guide/advanced-computing/#the-command-template","title":"The command template","text":"<p>The command template is specified by the pipeline in the pipeline interface. A basic command template might look like this:</p> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>The command template provides the command and should be written by the pipeline author.</p>"},{"location":"looper/advanced-guide/advanced-computing/#the-submission-template","title":"The submission template","text":"<p>As a user, the submission template is more relevant -- it tells the computer how, where, and when to execute the command. To submit commands to a cluster, we simply need to add some more information around the command above, specifying things like memory use, job name, etc. It may be tempting to add these details directly to the command template. This would work; however, this would restrict the pipeline to only running in that way, since the submission code would now be tightly coupled to the command code. Instead, looper retains flexibility by introducing a second template layer, the submission template. A submission template can be as simple or complex as required. </p> <p>For, example, a basic submission template to run in a local computing environment might look like this:</p> simple_submission_template.sub<pre><code>#! /usr/bin/bash\n\n{CODE}\n</code></pre> <p>The <code>{CODE}</code> variable is populated by the populated result of the command template -- that's what makes the templates nested. This example does nothing but pass the command through. A more complicated template could submit a job to a SLURM cluster:</p> slurm_template.sub<pre><code>#!/bin/bash\n#SBATCH --job-name='{JOBNAME}'\n#SBATCH --output='{LOGFILE}'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='{CORES}'\n#SBATCH --time='{TIME}'\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\nsrun {CODE}\n</code></pre> <p>Now that you understand the difference between the command template and the submission template, let's move on to how you can configure looper for different computing environments. Remember, we're taking the perspective of the pipeline user here; we assume the pipeline author has provided the command correctly in the pipeline interface, so we won't delve any deeper into command templates here. If you want to learn more about developing pipelines and advanced features of the pipeline interface, consult the developer tutorials on how to write a pipeline interface.</p>"},{"location":"looper/advanced-guide/advanced-computing/#custom-compute-packages-using-divvy","title":"Custom compute packages using divvy","text":"<p>Looper will come with several pre-built packages via a module called divvy. However, these may not fit your computing environment exactly, and you'll probably want to create your own compute package.</p> <p>The available compute packages are defined in the divvy configuration file (<code>DIVCFG</code> for short), <code>yaml</code> file. Note that looper will use a default divvy config file if the user does not specify one via the <code>DIVCFG</code> environment variable (e.g. <code>export DIVCFG=path/to/compute_config.yaml</code>). Each compute package is defined by at lease two things: a path to a template, the command used to submit the template. Here is an example divvy configuration file:</p> divvy_config.yaml<pre><code>compute_packages:\n  default:\n    submission_template: divvy_templates/local_template.sub\n    submission_command: sh\n  local:\n    submission_template: divvy_templates/local_template.sub\n    submission_command: sh\n  develop_package:\n    submission_template: divvy_templates/slurm_template.sub\n    submission_command: sbatch\n    partition: develop\n  big:\n    submission_template: divvy_templates/slurm_template.sub\n    submission_command: sbatch\n    partition: bigmem\n</code></pre> <p>Each entry in <code>compute_packages</code> defines a package. If you don't specify a package to activate, <code>divvy</code> uses the package named <code>default</code>. You can make your default whatever you like.  You can add any custom attributes you want, but each compute package must specify at least the <code>submission_command</code> and <code>submission_template</code>.</p>"},{"location":"looper/advanced-guide/advanced-computing/#the-submission_command-attribute","title":"The <code>submission_command</code> attribute","text":"<p>The <code>submission_command</code> attribute is the string divvy will use to submit a job. For example, in our compute package named <code>develop_package</code>, we've set <code>submission_command</code> to <code>sbatch</code>. We are telling divvy that, when this package is activated, divvy should submit the job by running: <code>sbatch &lt;submission_script.txt&gt;</code>.</p>"},{"location":"looper/advanced-guide/advanced-computing/#the-submission_template-attribute","title":"The <code>submission_template</code> attribute","text":"<p>The <code>submission_template</code> attribute is a path to a template file. The template file provides a skeleton that <code>divvy</code> will populate with job-specific attributes. These paths can be relative or absolute; relative paths are considered relative to the DIVCFG file. We saw examples of these templates earlier.</p>"},{"location":"looper/advanced-guide/advanced-computing/#creating-a-custom-compute-package","title":"Creating a custom compute package","text":"<p>To create a custom compute package and add it to divvy is easy:</p> <ol> <li>Create your own template, a text file with <code>{VARIABLE}</code> syntax for any job-specific variables. You can find examples in the submit_templates folder.</li> <li>Add a new compute package as an entry under <code>compute_packages</code> in your divvy config file.</li> <li>Point to your custom template in the <code>submission_template</code> attribute of your new compute package.</li> <li>Add the appropriate <code>submission_command</code> for this package.</li> </ol>"},{"location":"looper/advanced-guide/advanced-computing/#using-divvy-to-submit-jobs-from-the-command-line","title":"Using divvy to submit jobs from the command-line","text":"<p>Once you have the above configured, you're ready to submit jobs! You can actually use <code>divvy</code> without using looper, to submit one-off jobs to a cluster. Divvy will take variables from a file or the command line, merge these with environment settings to create a specific job script. </p> <p>In fact, there are several sources where information comes from. Divvy can incorporate variables provided in any of these sources:</p> <ol> <li>the command line, where the user provides any on-the-fly variables for a particular run.</li> <li>the PEP, which provides information on the project and samples.</li> <li>the pipeline interface, which provides information on the pipeline to run.</li> <li>the divvy config file, which provides information on the computing environment.</li> </ol> <p>Let's walk through how this works.</p> <p>You use <code>divvy write</code>. Let's see what happens when we write a new script after selecting a compute package, but providing no variables:</p> <pre><code>divvy write --package slurm\n</code></pre> <p>This will write out the template, populated with whatever variables you provided (which in this case was nothing). By default, it will write to stdout:</p> Divvy write output<pre><code>Using divvy config: /home/nsheff/Dropbox/env/divvy_config/zither.yaml\nActivating compute package 'slurm'\n#!/bin/bash\n#SBATCH --job-name='{JOBNAME}'\n#SBATCH --output='{LOGFILE}'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='{CORES}'\n#SBATCH --time='{TIME}'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{CODE}\n</code></pre> <p>This gives us a way to view the SLURM template. But, what we really want to do is populate those variables to make an actual script we can run or submit. You can provide variables in several ways. Divvy uses the same computing configuration arguments you can use with looper. In fact, all looper does is take these arguments and pass them along to divvy, under the hood. So, let's try populating a variable with the <code>--compute</code> argument:</p> <pre><code>divvy write --package slurm \\\n    --compute jobname=my_sample_job cores=1\n</code></pre> <p>You will see the <code>{JOBNAME}</code> and <code>{CORES}</code> variables are populated, but not the others:</p> Divvy write output<pre><code>Using divvy config: /home/nsheff/Dropbox/env/divvy_config/zither.yaml\nActivating compute package 'slurm'\n#!/bin/bash\n#SBATCH --job-name='my_sample_job'\n#SBATCH --output='{LOGFILE}'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='1'\n#SBATCH --time='{TIME}'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{CODE}\n</code></pre> <p>Sometimes it's more convenient to provide settings in through a file, instead of on the command line. Create a file called <code>myjob.yaml</code> and pass this file using <code>--settings</code>:</p> myjob.yaml<pre><code>jobname: my_custom_job\nlogfile: logs/my_job.log\npartition: standard\ncores: 1\nmem: 2GB\ntime: \"1:00:00\"\ncode: \"echo \\\"Success! The job ran\\\"\"\n</code></pre> <pre><code>divvy write --package slurm \\\n    --settings myjob.yaml\n</code></pre> <p>And you will see that the output acknowledges the settings file, and is now populating all the provided variables:</p> Divvy write output<pre><code>Using divvy config: /home/nsheff/Dropbox/env/divvy_config/zither.yaml\nActivating compute package 'slurm'\nLoading settings file: myjob.yaml\n#!/bin/bash\n#SBATCH --job-name='my_custom_job'\n#SBATCH --output='logs/my_job.log'\n#SBATCH --mem='2GB'\n#SBATCH --cpus-per-task='1'\n#SBATCH --time='1:00:00'\n#SBATCH --partition='standard'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\necho \"Success! The job ran\"\n</code></pre> <p>Now, we don't actually want to log the populated template to <code>stdout</code>, we want to create a submission script. You can use output redirection with <code>&gt;</code> (which works because the information text is actually being printed to <code>stderr</code>), or you can use the <code>--outfile</code> argument. These will do the same thing, allowing you to write a submission script from the command line:</p> <pre><code>divvy write --package slurm \\\n    --settings myjob.yaml \\\n    &gt; submit_script.txt\n</code></pre> <pre><code>divvy write --package slurm \\\n    --settings myjob.yaml \\\n    --outfile submit_script.txt\n</code></pre> <p>Once your script is created, you could inspect it and then submit it with <code>sbatch submit_script.txt</code>.</p> <p>If you want to save yourself this final step, you can just use <code>divvy submit</code> to write the script and then execute it automatically (using whatever command is specified for the compute package <code>submission_command</code>):</p> <pre><code>divvy submit --package slurm \\\n    --settings myjob.yaml \\\n    --outfile submit_script.txt\n</code></pre> <p>Note: Using divvy from within Python</p> <p>You can also use divvy via Python API, allowing you to create and submit jobs from within Python. You can use this make your own Python tools divvy-compatible. Here's an example of using divvy to write scripts from within python. For more information, please see the divvy documentation.</p> <pre><code>import divvy\ndcc = divvy.ComputingConfiguration()\ndcc.activate_package(\"slurm\")\n\n# write out a submission script\ndcc.write_script(\"test_script.sub\", \n    {\"code\": \"bowtie2 input.bam output.bam\"})\n</code></pre>"},{"location":"looper/advanced-guide/advanced-computing/#divvy-config-variable-adapters-and-built-in-looper-variables","title":"Divvy config variable adapters and built-in looper variables","text":"<p>We've covered how to construct submission templates and then populate them with variables using divvy. There's one last thing we need to do to connect looper. We have to provide a mapping between the template variables and their source. Looper also provides a lot of built-in variables that we can use with our submission templates. In the divvy examples above, we just matched them by specifying exactly the same variable name in our <code>--settings</code> or <code>--compute</code> arguments. </p> <p>The default divvy templates use variables like <code>{CODE}</code>, <code>{JOBNAME}</code>, and <code>{LOGFILE}</code>. But what if you want to use variables with different names, or add other variables? How can you connect looper-provided information into these template? To do this, we need more more step. These variables are linked to looper namespaces via divvy adapters.  The adapters is a section of the divvy configuration file that provides a set of variable mappings that <code>divvy</code> uses to populate the submission templates. This makes the connection with <code>divvy</code> and client software more flexible and more elegant, since the source of the data does not need to follow any particular naming scheme, any mapping can be used and adapted to work with any <code>divvy</code> templates.</p> <p>Here's an example adapters section, added into the divvy config example:</p> divvy_config.yaml<pre><code>adapters:\n  CODE: looper.command\n  LOGFILE: looper.log_file\n  JOBNAME: user_settings.program.job_name\n  CORES: processors_number\ncompute_packages:\n  default:\n    submission_template: templates/local_template.sub\n    submission_command: sh\n  local:\n    submission_template: templates/local_template.sub\n    submission_command: sh\n  develop_package:\n    submission_template: templates/slurm_template.sub\n    submission_command: sbatch\n    partition: develop\n  big:\n    submission_template: templates/slurm_template.sub\n    submission_command: sbatch\n    partition: bigmem\n</code></pre> <p>As you can see in the example <code>adapters</code> section above, each adapter is a key-value pair that maps a <code>divvy</code> template variable to a target value.</p> <p>Here are the default divvy adapters: <pre><code>adapters:\n  CODE: looper.command\n  JOBNAME: looper.job_name\n  CORES: compute.cores\n  LOGFILE: looper.log_file\n  TIME: compute.time\n  MEM: compute.mem\n  DOCKER_ARGS: compute.docker_args\n  DOCKER_IMAGE: compute.docker_image\n  SINGULARITY_IMAGE: compute.singularity_image\n  SINGULARITY_ARGS: compute.singularity_args\n</code></pre></p> <p>The divvy adapters is a section in the divvy configuration file that links the divvy template variable (left side) to any other arbitrary variable names (right side). The target values can use namespaces (nested mapping). In fact, notice how the default adapters are using namespaces, like <code>looper.&lt;variable&gt;</code> or <code>compute.&lt;variable&gt;</code>. These are variables that are provided by looper, automatically. Looper provides a lot of really useful variables that you can use in your templates. And, it also allows you to actually add your own variables through the submission hook system. Let's go through what built-in variables you have available:</p>"},{"location":"looper/advanced-guide/advanced-computing/#looper-variable-namespaces","title":"Looper variable namespaces","text":"<p>To keep things organized, looper groups variables into namespaces. These namespaces are used first to populate the command template, which produces a built command. This command is then treated as a variable in itself, which is pooled with the other variables to populate the submission template. Looper provides 6 variable namespaces for populating the templates:</p>"},{"location":"looper/advanced-guide/advanced-computing/#1-project","title":"1. project","text":"<p>The <code>project</code> namespace contains all PEP config attributes. For example, if you have a config file like this:</p> <pre><code>pep_version: 2.0.0\nmy_variable: 123\n</code></pre> <p>Then <code>project.my_variable</code> would have value <code>123</code>. You can use the project namespace to refer to any information in the project. You can use <code>project.looper</code> to refer to any attributes in the <code>looper</code> section of the PEP.</p>"},{"location":"looper/advanced-guide/advanced-computing/#2-sample-or-samples","title":"2. sample or samples","text":"<p>For sample-level pipelines, the <code>sample</code> namespace contains all PEP post-processing sample attributes for the given sample. For project-level pipelines, looper constructs a single job for an entire project, so there is no <code>sample</code> namespace; instead, there is a <code>samples</code> (plural) namespace, which is a list of all the samples in the project. This can be useful if you need to iterate through all the samples in your command template.</p>"},{"location":"looper/advanced-guide/advanced-computing/#3-pipeline","title":"3. pipeline","text":"<p>Everything under <code>pipeline</code> in the pipeline interface for this pipeline. This simply provides a convenient way to annotate pipeline-level variables for use in templates.</p>"},{"location":"looper/advanced-guide/advanced-computing/#4-looper","title":"4. looper","text":"<p>The <code>looper</code> namespace consists of automatic variables created by looper:</p> <p>paths:</p> <ul> <li><code>looper.output_dir</code> -- parent output directory provided in <code>project.looper.output_dir</code> in the project configuration file</li> <li><code>looper.results_subdir</code> -- the path to the results directory. It is a sub directory of <code>output_dir</code> called <code>project.looper.results_subdir</code> or \"results_pipeline\" by default</li> <li><code>sample_output_folder</code> -- a sample-specific or project-specific output folder (<code>looper.results_subdir</code>/<code>sample.sample_name</code>)</li> <li><code>looper.piface_dir</code> -- directory the pipeline interface has been read from</li> <li><code>looper.pep_config</code> -- path to the project configuration file used for this looper run</li> <li><code>looper.log_file</code> -- an automatically created log file path, to be stored in the looper submission subdirectory</li> </ul> <p>non-paths:</p> <ul> <li><code>looper.total_input_size</code> -- the sum of file sizes for all files marked as input files in the input schema</li> <li><code>looper.command</code> -- the result of populating the command template</li> <li><code>looper.job_name</code> -- job name made by concatenating the pipeline identifier and unique sample name</li> </ul> <p>The <code>looper.command</code> value is what enables the two-layer template system, whereby the output of the command template is used as input to the submission template.</p>"},{"location":"looper/advanced-guide/advanced-computing/#5-compute","title":"5. compute","text":"<p>The <code>compute</code> namespace consists of a group of variables relevant for computing resources. The <code>compute</code> namespace has a unique behavior: it aggregates variables from several sources in a priority order, overriding values with more specific ones as priority increases. The list of variable sources in priority order is:</p> <ol> <li>Looper CLI (<code>--compute</code> or <code>--settings</code> for on-the-fly settings) e.g. <code>looper run --config .your_config.yaml --package slurm --compute PARTITION=standard time='01-00:00:00' cores='32' mem='32000'</code></li> <li>PEP config, <code>project.looper.compute</code> section</li> <li>Pipeline interface, <code>compute</code> section</li> <li>Activated divvy compute package (<code>--package</code> CLI argument)</li> </ol> <p>So, the compute namespace is first populated with any variables from the selected divvy compute package. It then updates this with settings given in the <code>compute</code> section of the pipeline interface. It then updates from the PEP <code>project.looper.compute</code>, and then finally anything passed to <code>--compute</code> on the looper CLI. This provides a way to modulate looper behavior at the level of a computing environment, a pipeline, a project, or a run, in that order.</p>"},{"location":"looper/advanced-guide/advanced-computing/#6-pipestat","title":"6. pipestat","text":"<p>The <code>pipestat</code> namespace consists of a group of variables that reflect the pipestat configuration for a submission.</p> <ol> <li><code>pipestat.file</code> - The pipestat results file.</li> <li><code>pipestat.record_identifier</code> - record_identifier</li> <li><code>pipestat.config_file</code>- config file path</li> <li><code>pipestat.output_schema</code> - output schema path</li> <li><code>pipestat.pephub_path</code> - path to PEP on PEPhub as destination for reporting results</li> </ol>"},{"location":"looper/advanced-guide/advanced-computing/#best-practices-on-storing-compute-variables","title":"Best practices on storing compute variables","text":"<p>Since compute variables can be stored in several places, it can be confusing to know where you should put things. Here are some guidelines:</p>"},{"location":"looper/advanced-guide/advanced-computing/#partition-or-queue-name","title":"Partition or queue name","text":"<p>Because the partition or queue name is relative to your environment, we don't usually specify this in the <code>resources</code> section, but rather, in the <code>pepenv</code> config.</p>"},{"location":"looper/advanced-guide/advanced-computing/#divcfg-config-file","title":"DIVCFG config file","text":"<p>Variables that describes settings of a compute environment should go in the <code>DIVCFG</code> file. Any attributes in the activated compute package will be available to populate template variables. For example, the <code>partition</code> attribute is specified in many of our default <code>DIVCFG</code> files; that attribute is used to populate a template <code>{PARTITION}</code> variable. This is what enables pipelines to work in any compute environment, since we have no control over what your partitions are named. You can also use this to change SLURM queues on-the-fly.</p>"},{"location":"looper/advanced-guide/advanced-computing/#pipeline-interface","title":"Pipeline interface","text":"<p>Variables that are specific to a pipeline can be defined in the <code>pipeline interface</code> file,  <code>compute</code> section.As an example of a variable pulled from the <code>compute</code> section, we defined in our <code>pipeline_interface.yaml</code> a variable pointing to the singularity or docker image that can be used to run the pipeline, like this:</p> <pre><code>compute:\n  singularity_image: /absolute/path/to/images/image\n</code></pre> <p>Now, this variable will be available for use in a template as <code>{SINGULARITY_IMAGE}</code>. This makes sense to put in the pipeline interface because it is specific to this pipeline. This path should probably be absolute, because a relative path will be interpreted as relative to the working directory where your job is executed (not relative to the pipeline interface). This section is also useful for adjusting the amount of resources we need to request from a resource manager like SLURM. For example: <code>{MEM}</code>, <code>{CORES}</code>, and <code>{TIME}</code> are all defined frequently in this section, and they vary for different input file sizes.</p>"},{"location":"looper/advanced-guide/advanced-computing/#project-config","title":"Project config","text":"<p>Finally, project-level variables can also be populated from the <code>compute</code> section of a project config file. This would enable you to make project-specific compute changes (such as billing a particular project to a particular SLURM resource account).</p>"},{"location":"looper/advanced-guide/advanced-computing/#resources","title":"Resources","text":"<p>You may notice that the compute config file does not specify resources to request (like memory, CPUs, or time). Yet, these are required in order to submit a job to a cluster. Resources are not handled by the divcfg file because they not relative to a particular computing environment; instead they vary by pipeline and sample. As such, these items should be provided elsewhere. The user can provide them at run time via the CLI:</p> <pre><code>looper run --package slurm --compute PARTITION=standard time='01-00:00:00' cores='32' mem='32000'\n</code></pre> <p>Summary</p> <ul> <li>Looper jobs are created by first populating a command template, which is then provided as a variable to populate a submission template.</li> <li>Looper's nested templates provide a powerful separation of concerns, so pipeline developers don't have to worry about configurint computing environments (like cluster resources), and pipeline users don't have to worry about the specifics of the pipeline interface.</li> <li>You can easily create your own compute packages to customize job submission for any computing environment.</li> <li>Looper provides a lot of built-in variables you can use in your job submission templates, which are organized into looper variable namespaces.</li> <li>You can use <code>divvy</code> independently of looper to create and submit independent job scripts. Looper uses divvy to create job scripts for each sample in your sample table.</li> </ul>"},{"location":"looper/advanced-guide/advanced-metadata/","title":"Advanced metadata features","text":"<p>We already covered how you can specify sample metadata using either a simple csv file or a PEP. But in that tutorial we covered only the basic features of PEPs. PEPs are actually a lot more powerful, and many of those features are useful for looper projects. Here, we'll show you a few of the more advanced features of PEPs and explain how they can be useful with looper.  We still won't cover everything here, though. If you want to see all the features of PEP, you should consult the detailed PEP documentation.</p> <p>Learning objectives</p> <ul> <li>What else can PEPs do that can make my life easier?</li> </ul> <p>Note</p> <p>These concepts aren't strictly about <code>looper</code>, they are about PEP. They just show you some examples of how your <code>looper</code> project could take advantage of PEP features.</p>"},{"location":"looper/advanced-guide/advanced-metadata/#implied-attributes","title":"Implied attributes","text":"<p>At some point, you may have a situation where you need a single sample attribute (or column) to populate several different pipeline arguments with different values. In other words, the value of a given attribute may imply values for other attributes. It would be nice if you didn't have to enumerate all of these secondary, implied attributes, and could instead just infer them from the value of the original attribute?</p> <p>For example, if my <code>organism</code> attribute is <code>human</code>, this implies a few other secondary attributes (which may be project-specific): For one project, I want to set <code>genome</code> to <code>hg38</code> and <code>macs_genome_size</code> to <code>hs</code>. Of course, I could just define columns called <code>genome</code> and <code>macs_genome_size</code>, but these would be constant across samples, so it feels inefficient and unwieldy. Plus, changing the aligned genome would require changing the sample annotation sheet (every sample, in fact). You can certainly do this with <code>looper</code>, but a better way is to handle these things at the project level.</p> <p>As a more elegant alternative, PEP provides the <code>imply</code> sample modifier. Instead of hard-coding <code>genome</code> and <code>macs_genome_size</code> in the sample annotation sheet, you can simply specify that the attribute <code>organism</code> implies additional attribute-value pairs, which vary by sample based on the value of the <code>organism</code> attribute. This lets you specify assemblies, genome size, and other similar variables all in your project config file.</p> <p>To do this, just add an <code>imply</code> sample modifier. Example:</p> <pre><code>sample_modifiers:\n  imply:\n    - if:\n        organism: \"human\"\n      then:\n        genome: \"hg38\"\n        macs_genome_size: \"hs\"\n    - if:\n        organism: \"mouse\"\n      then:\n        genome: \"mm10\"\n        macs_genome_size: \"mm\"\n</code></pre> <p>In this example, any samples with organism set to <code>\"human\"</code> will automatically also have attributes for <code>genome</code> (<code>\"hg38\"</code>) and for <code>macs_genome_size</code> (<code>\"hs\"</code>). Any samples with <code>organism</code> set to <code>\"mouse\"</code> will have the corresponding values. A sample with <code>organism</code> set to <code>\"frog\"</code> would lack attributes for <code>genome</code> and <code>macs_genome_size</code>, since those columns are not implied by <code>\"frog\"</code>.</p> <p>This system essentially lets you set global, species-level attributes at the project level instead of duplicating that information for every sample that belongs to a species. Even better, it's generic, so you can do this for any partition of samples (just replace <code>organism</code> with whatever you like).  This makes your project more portable and does a better job conceptually with separating sample attributes from project attributes. After all, a reference assembly is not a property of a sample, but is part of the broader project context.</p>"},{"location":"looper/advanced-guide/advanced-metadata/#the-amend-project-modifier-for-subprojects","title":"The 'amend' project modifier for subprojects","text":"<p>PEP provides not only sample modifiers, but project modifiers. You can use this to encode slightly different versions of a project, without duplicating the settings. For example, say we have a project that we align to a particular reference genome, say \"hg38\". We want to try running that on a different reference genome, say \"hg19\". Rather than duplicate the whole project or sample table and change everything, we can actually do this using the <code>amend</code> project modifier. Consider this PEP:</p> <pre><code>sample_modifiers:\n  append:\n    genome: \"hg38\"\n\nproject_modifiers:\n  amend:\n    hg19_alignment:\n      sample_modifiers:\n        append:\n          genome: \"hg19\"\n</code></pre> <p>This is using the <code>append</code> modifier to set the <code>genome</code> attribute to <code>hg38</code> for all samples. We can then use <code>{sample.genome}</code> in the pipeline interface to pass <code>hg38</code> as a pipeline parameter. But we also have an <code>amend</code> section, which defines an amendment called <code>hg19_alignment</code>. If we activate this project with <code>--amend hg19_alignment</code>, then everything under that amendment will be attached to the PEP. In this example, it will add a new append modifier, which sets the <code>genome</code> attribute to <code>hg19</code>. Thus, reparameterizing this pipeline is as simple as choosing the amendment with the command line, <code>--amend hg19_alignment</code>.</p>"},{"location":"looper/advanced-guide/advanced-metadata/#how-to-handle-multiple-input-files","title":"How to handle multiple input files","text":"<p>Sometimes a sample has multiple input files that belong to the same attribute. For example, a common use case is a single library that was spread across multiple sequencing lanes, yielding multiple input files that need to be merged, and then run through the pipeline as one. Dealing with multiple input files is described in detail in the PEP documentation, but covered briefly here. PEP has two ways to merge these:</p> <ol> <li>Use shell expansion characters (like <code>*</code> or <code>[]</code>) in your file path definitions (good for simple merges)</li> <li>Specify a sample subannotation tables which maps input files to samples for samples with more than one input file (infinitely customizable for more complicated merges).</li> </ol> <p>To accommodate complex merger use cases, this is infinitely customizable.</p> <p>Warning</p> <p>Do not use both of these options for the same sample at the same time; that will lead to multiple mergers.</p>"},{"location":"looper/advanced-guide/advanced-metadata/#using-wild-cards-in-derived-sources","title":"Using wild cards in derived sources","text":"<p>To do the first option, simply change the data source specification to use wild card characters:</p> pep_config.yaml<pre><code>pep_version: 2.1.0\nsample_table: sample_table.csv\nsample_modifiers:\n  append:\n    file_path: source1\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: \"data/{sample_name}_*.txt\"\n</code></pre>"},{"location":"looper/advanced-guide/advanced-metadata/#using-a-subsample-table","title":"Using a subsample table","text":"<p>For the second option, provide a subsample table in your pep config file:</p> pep_config.yaml<pre><code>pep_version: 2.1.0\nsample_table: sample_table.csv\nsubsample_table: subsample_table.csv\n</code></pre> Example subsample_table.csv<pre><code>sample_name,file_path\ncanada,data/canada_1.txt\ncanada,data/canada_2.txt\nswitzerland,data/switzerland_1.txt\nswitzerland,data/switzerland_2.txt\nmexico,data/mexico_1.txt\nmexico,data/mexico_2.txt\n</code></pre> <p>Make sure the <code>sample_name</code> column of this table matches, and then include any columns needed to point to the data. Looper will automatically include all of these files as input passed to the pipelines.</p> <p>Important</p> <p>Mergers are not the way to handle different functional/conceptual kinds of input files (e.g., <code>read1</code> and <code>read2</code> for a sample sequenced with a paired-end protocol). Such cases should be handled as separate derived columns in the main sample annotation sheet if they're different arguments to the pipeline.</p>"},{"location":"looper/advanced-guide/advanced-metadata/#multi-value-sample-attributes-behavior-in-the-pipeline-interface-command-templates","title":"Multi-value sample attributes behavior in the pipeline interface command templates","text":"<p>Both subsample tables and shell expansion characters lead to sample attributes with multiple values, stored in a list of strings as opposed to a standard scenario, where a single value is stored as a string (<code>single_attr</code>).</p> <p>For example: <pre><code>Sample\nsample_name: canada\nfile_path: ['data/canada_1.txt', 'data/canada_2.txt']\nsingle_attr: random_test_val\n</code></pre></p>"},{"location":"looper/advanced-guide/advanced-metadata/#access-individual-elements-in-lists","title":"Access individual elements in lists","text":"<p>Pipeline interface author can leverage that fact and access the individual elements, e.g iterate over them and append to a string using the Jinja2 syntax:</p> <pre><code>pipeline_name: test_iter\npipeline_type: sample\ncommand_template: &gt;\n  --input-iter {%- for x in sample.file_path -%} --test-individual {x} {% endfor %} # iterate over multiple values\n  --input-single {sample.single_attr} # use the single value as is\n</code></pre> <p>This results in a submission script that includes the following command: <pre><code>--input-iter  --test-individual data/canada_1.txt  --test-individual data/canada_2.txt\n--input-single  random_test_val\n</code></pre></p>"},{"location":"looper/advanced-guide/advanced-metadata/#concatenate-elements-in-lists","title":"Concatenate elements in lists","text":"<p>The most common use case is just concatenating the multiple values and separate them with space -- providing multiple input values to a single argument on the command line. Therefore, all the multi-value sample attributes that have not been processed with Jinja2 logic are automatically concatenated. For instance, the following command template in a pipeline interface will result in the submission script presented below:</p> <p>Pipeline interface: <pre><code>pipeline_name: test_concat\npipeline_type: sample\ncommand_template: &gt;\n  --input-concat {sample.file_path} # concatenate all the values\n</code></pre></p> <p>Command in the submission script: <pre><code>--input-concat  data/canada_1.txt data/canada_2.txt\n</code></pre></p>"},{"location":"looper/advanced-guide/advanced-metadata/#others","title":"Others","text":"<p>Some other things you might find interesting:</p> <ul> <li>imports allow PEPs to import other PEPs, so you can re-use information across projects.</li> </ul> <p>Summary</p> <ul> <li>You can use the <code>imply</code> sample modifier to eliminate redundant columns in your sample table.</li> <li>You can use multiple input files for your pipeline using wildcards or subsample tables.</li> <li>PEP provides many powerful features that work well with looper.</li> </ul>"},{"location":"looper/advanced-guide/advanced-run-options/","title":"Advanced run options","text":""},{"location":"looper/advanced-guide/advanced-run-options/#introduction","title":"Introduction","text":"<p>The basic tutorials showed you how to use <code>looper run</code> with the defaults, and introduced a few of the arguments, like <code>--dry-run</code> and <code>--limit</code>. The computing tutorial covered some arguments related to computing resources, like <code>--package</code> and <code>--compute</code>. There are many other arguments to <code>looper run</code> that can help you control how looper creates and submits jobs. Let's introduce some of the more advanced capabilities of <code>looper run</code>.</p> <p>Learning objectives</p> <ul> <li>What are some of the more advanced options for <code>looper run</code>?</li> <li>How do I submit a job for the entire project, instead of a separate job for each sample?</li> <li>How can I submit different pipelines for different samples?</li> <li>How can I adjust pipeline arguments on-the-fly?</li> <li>What if I want to submit multiple samples in a single job?</li> <li>Can I exclude certain samples from a run?</li> </ul>"},{"location":"looper/advanced-guide/advanced-run-options/#grouping-many-jobs-into-one","title":"Grouping many jobs into one","text":"<p>By default, <code>looper</code> will translate each row in your <code>sample_table</code> into a single job. But perhaps you are running a project with tens of thousands of rows, and each job only takes mere minutes to run; in this case, you'd rather just submit a single job to process many samples. <code>Looper</code> makes this easy with the <code>--lump</code> and <code>--lump-n</code> command line arguments.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#lumping-jobs-by-job-count-lump-n","title":"Lumping jobs by job count: <code>--lump-n</code>","text":"<p>It's quite simple: if you want to run 100 samples in a single job submission script, just tell looper <code>--lump-n 100</code>.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#lumping-jobs-by-input-file-size-lump","title":"Lumping jobs by input file size: <code>--lump</code>","text":"<p>But what if your samples are quite different in terms of input file size? For example, your project may include many small samples, which you'd like to lump together with 10 jobs to 1, but you also have a few control samples that are very large and should have their own dedicated job. If you just use <code>--lump-n</code> with 10 samples per job, you could end up lumping your control samples together, which would be terrible. To alleviate this problem, <code>looper</code> provides the <code>--lump</code> argument, which uses input file size to group samples together. By default, you specify an argument in number of gigabytes. Looper will go through your samples and accumulate them until the total input file size reaches your limit, at which point it finalizes and submits the job. This will keep larger files in independent runs and smaller files grouped together.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#lumping-jobs-by-job-count-lump-j","title":"Lumping jobs by job count: <code>--lump-j</code>","text":"<p>If you want to split your samples across a specific number of jobs, use <code>--lump-j</code>. For example, <code>--lump-j 10</code> will distribute all your samples evenly across 10 jobs.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#running-project-level-pipelines","title":"Running project-level pipelines","text":""},{"location":"looper/advanced-guide/advanced-run-options/#what-are-project-level-pipelines","title":"What are project-level pipelines?","text":"<p>The tutorials assume you're running a pipeline where you're trying to run one job per sample, i.e. the samples are independent, and you want to do the same thing to each of them. This is the most common use case for looper. </p> <p>But sometimes, you're interested in running a job on an entire project. This could be a pipeline that integrates data across all the samples, or one that will summarize the results of your independent sample pipeline runs. In this case, you really only need to submit a single job. Looper's main benefit is handling the boilerplate needed to construct the submission scripts and submit a separate job for each sample. But looper also provides some help for the task of submitting a single job for the whole project.</p> <p>So, there are really two types of pipelines looper can submit. The typical ones, which need one job per sample, we call sample-level pipelines. For many use cases, that's all you need to worry about. The broader level, which need one job for the whole project, we call project-level pipelines. </p>"},{"location":"looper/advanced-guide/advanced-run-options/#split-apply-combine","title":"Split-apply-combine","text":"<p>One of the most common uses of a project-level pipeline is to summarize or aggregate the results of the sample-level pipeline. This approach essentially employs looper as an implementation of the MapReduce programming model, which applies a split-apply-combine strategy. We split the project into samples and apply the first tier of processing (the sample pipeline). We then combine the results in the second tier of processing (the project pipeline). Looper doesn't require you to use this two-stage system, but it does make it easy to do so. Many pipelines operate only at the sample level and leave the downstream cross-sample analysis to the user.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#how-to-run-a-project-level-pipeline","title":"How to run a project-level pipeline","text":"<p>The usual <code>looper run</code> command runs sample-level pipelines. This will create a separate job for each sample. Pipeline interfaces defining a sample pipeline do so under the  <code>sample_interface</code> attribute.</p> <p>Project pipelines are run with <code>looper runp</code> (where the p stands for project). The interface specifies that it is a project pipeline by using the <code>project_interface</code> attribute. Running a project pipeline operates in almost exactly the same way as the sample pipeline, with 2 key differences:</p> <ol> <li>First, instead of a separate command for every sample, <code>looper runp</code> creates a single command  for the project (per pipeline).</li> <li>Second, the command template cannot access the <code>sample</code> namespace representing a particular sample, since it's not running on a particular sample; instead, it will have access to a <code>samples</code> (plural) namespace, which contains all the attributes from all the samples.</li> </ol> <p>In a typical workflow, a user will first run the samples individually using <code>looper run</code>, and then, if the pipeline provides one, will run the project component using <code>looper runp</code> to summarize to aggregate the results into a project-level output.</p> <p>Example of a pipeline interface containing a sample-level interface AND a project-level interface: <pre><code>pipeline_name: example_pipeline\noutput_schema: pipestat_output_schema.yaml\nsample_interface:\n  command_template: &gt;\n    count_lines.sh {sample.file} {sample.sample_name}\nproject_interface:\n  command_template: &gt;\n    count_lines_project.sh \"data/*.txt\"\n</code></pre></p>"},{"location":"looper/advanced-guide/advanced-run-options/#running-multiple-pipelines","title":"Running multiple pipelines","text":"<p>To run more than one pipeline, specify multiple pipeline interfaces in the looper config file:</p> <pre><code>pep_config: pephub::databio/looper:default\noutput_dir: \"$HOME/hello_looper-master/output\"\npipeline_interfaces:\n  - \"$HOME/hello_looper-master/pipeline/pipeline_interface\"\n  - \"$HOME/hello_looper-master/project/pipeline\"\n</code></pre> <p>You can also link to the pipeline interface with a sample attribute. If you want the same pipeline to run on all samples, it's as easy as using an <code>append</code> modifier like this:</p> <pre><code>sample_modifiers:\n  append:\n    pipeline_interfaces: \"test.yaml\"\n</code></pre> <p>But if you want to submit different samples to different pipelines, depending on a sample attribute, like <code>protocol</code>, you can use an implied attribute:</p> <pre><code>sample_modifiers:\n  imply:\n    - if:\n        protocol: [PRO-seq, pro-seq, GRO-seq, gro-seq] # OR\n      then:\n        pipeline_interfaces: [\"peppro.yaml\"]\n</code></pre> <p>This approach uses only functionality of PEPs to handle the connection to pipelines as sample attributes, which provides full control and power using the familiar sample modifiers. It completely eliminates the need for re-inventing this complexity within looper, which eliminated the protocol mapping section to simplify the looper pipeline interface files. You can read more about the rationale of this change in issue 244.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#modulating-pipeline-by-sample","title":"Modulating pipeline by sample","text":"<p>If you have a project that contains samples of different types, you may want to submit a different pipeline for each sample. Usually, we think of looper as running the same pipeline for each sample. The best way to solve this problem is to split your sample table up into different tables, and then run a different pipeline on each. But if you really want to, you can actually modulate the pipeline by sample attributes. You can use an <code>imply</code> modifier in your PEP to select which pipelines you want to run on which samples, like this:</p> <pre><code>sample_modifiers:\n  imply:\n    - if:\n        protocol: \"RRBS\"\n      then:\n        pipeline_interfaces: \"/path/to/pipeline_interface.yaml\"\n    - if:\n        protocol: \"ATAC\"\n      then:\n        pipeline_interfaces: \"/path/to/pipeline_interface2.yaml\"\n</code></pre> <p>A more complicated example taken from PEPATAC of a <code>project_config.yaml</code> file:</p> <pre><code>pep_version: 2.0.0\nsample_table: tutorial.csv\n\nsample_modifiers:\n  derive:\n    attributes: [read1, read2]\n    sources:\n      # Obtain tutorial data from http://big.databio.org/pepatac/ then set\n      # path to your local saved files\n      R1: \"${TUTORIAL}/tools/pepatac/examples/data/{sample_name}_r1.fastq.gz\"\n      R2: \"${TUTORIAL}/tools/pepatac/examples/data/{sample_name}_r2.fastq.gz\"\n  imply:\n    - if: \n        organism: [\"human\", \"Homo sapiens\", \"Human\", \"Homo_sapiens\"]\n      then: \n        genome: hg38\n        prealignment_names: [\"rCRSd\"]\n        deduplicator: samblaster # Default. [options: picard]\n        trimmer: skewer          # Default. [options: pyadapt, trimmomatic]\n        peak_type: fixed         # Default. [options: variable]\n        extend: \"250\"            # Default. For fixed-width peaks, extend this distance up- and down-stream.\n        frip_ref_peaks: None     # Default. Use an external reference set of peaks instead of the peaks called from this run\n</code></pre>"},{"location":"looper/advanced-guide/advanced-run-options/#passing-extra-command-line-arguments","title":"Passing extra command-line arguments","text":"<p>Occasionally, a particular project needs to run a particular flavor of a pipeline. How can you  adjust pipeline arguments for just this project? You can use looper command extras to solve this problem. Command extras let you pass any string on to the pipeline, which will be appended to the command.</p> <p>There are 2 ways to use command extras: for sample pipelines, or for project pipelines:</p>"},{"location":"looper/advanced-guide/advanced-run-options/#1-sample-pipeline-command-extras","title":"1. Sample pipeline command extras","text":""},{"location":"looper/advanced-guide/advanced-run-options/#adding-sample-command-extras-via-sample-attributes","title":"Adding sample command extras via sample attributes","text":"<p>Looper uses a reserved sample attribute called <code>command_extras</code>, which you can set using general PEP sample modifiers however you wish. For example, if your extras are the same for all samples you could use an <code>append</code> modifier:</p> <pre><code>sample_modifiers:\n  append:\n    command_extra: \"--flavor-flag\"\n</code></pre> <p>This will add <code>--flavor-flag</code> the end of the command looper constructs. If you need to modulate the extras depending on another attribute value, you could use an imply modifier:</p> <pre><code>sample_modifiers:\n  imply:\n    - if:\n        protocol: \"rrbs\"\n      then:\n        command_extra: \"-C flavor.yaml --epilog\"\n</code></pre>"},{"location":"looper/advanced-guide/advanced-run-options/#adding-sample-command-extras-via-the-command-line","title":"Adding sample command extras via the command line","text":"<p>You can also pass extra arguments using <code>--command-extra</code> like this:</p> <pre><code>looper run project_config.yaml --command-extra=\"--flavor-flag\"\n</code></pre>"},{"location":"looper/advanced-guide/advanced-run-options/#2-project-pipeline-command-extras","title":"2. Project pipeline command extras","text":"<p>For project pipelines, you can specify command extras in the <code>looper</code> section of the PEP config:</p> <pre><code>looper:\n  output_dir: \"/path/to/output_dir\"\n  cli:\n    runp:\n      command-extra: \"--flavor\"\n</code></pre> <p>or as an argument to the <code>looper runp</code> command:</p> <pre><code>looper runp project_config.yaml --command-extra=\"--flavor-flag\"\n</code></pre>"},{"location":"looper/advanced-guide/advanced-run-options/#overriding-pep-based-command-extras","title":"Overriding PEP-based command extras","text":"<p>By default, the CLI extras are appended to the command_extra specified in your PEP. If you instead want to override the command extras listed in the PEP, you can instead use <code>--command-extra-override</code>.</p> <p>So, for example, make your looper call like this:</p> <pre><code>looper run --command-extra-override=\"-R\"\n</code></pre> <p>That will remove any defined command extras and append <code>-R</code> to the end of any commands created by looper.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#add-cli-arguments-to-looper-config","title":"Add CLI arguments to looper config","text":"<p>You can provide a <code>cli</code> keyword to specify any command line (CLI) options from within the looper config file. The subsections within this section direct the arguments to the respective <code>looper</code> subcommands. For example, to specify a sample submission limit for a <code>looper run</code> command, use:</p> <pre><code>cli:\n    run:\n      limit: 2\n</code></pre> <p>Keys in the <code>cli.&lt;subcommand&gt;</code> section must match the long argument parser option strings, so <code>command-extra</code>, <code>limit</code>, <code>dry-run</code> and so on. For more CLI options refer to the subcommands usage.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#selecting-or-excluding-samples","title":"Selecting or excluding samples","text":"<p>Looper provides several ways to select (filter) samples, so you only submit certain ones.</p>"},{"location":"looper/advanced-guide/advanced-run-options/#sample-selection-by-inclusion","title":"Sample selection by inclusion","text":"<p>To submit only certain samples, specify the sample attribute with <code>--sel-attr</code> and the values the attribute can take <code>--sel-incl</code>.  For example, to choose only samples where the <code>species</code> attribute is <code>human</code>, <code>mouse</code>, or <code>fly</code>:</p> <pre><code>looper run \\\n  --sel-attr species \\\n  --sel-incl human mouse fly\n</code></pre> <p>Similarly, to submit only one sample, with <code>sample_name</code> as <code>sample</code>, you could use:</p> <pre><code>looper run \\\n  --sel-attr sample_name \\\n  --sel-incl sample1\n</code></pre>"},{"location":"looper/advanced-guide/advanced-run-options/#sample-selection-by-exclusion","title":"Sample selection by exclusion","text":"<p>If it's more convenient to exclude samples by filter, you can use the analogous arguments <code>--sel-attr</code> with <code>--sel-excl</code>. This will exclude any samples matching the specified values. For example, to run all samples except those where <code>species</code> is <code>rat</code>:</p> <pre><code>looper run \\\n  --sel-attr species \\\n  --sel-excl rat\n</code></pre>"},{"location":"looper/advanced-guide/advanced-run-options/#toggling-sample-jobs-through-the-sample-table","title":"Toggling sample jobs through the sample table","text":"<p>You can also set the <code>toggle</code> value of attributes (either in your sample table or via a sample modifier). If the value of this column is not 1, <code>looper</code> will not submit the pipeline for that sample. This enables you to submit a subset of samples.</p> <p>Summary</p> <ul> <li>You can use <code>--lump</code>, <code>--lump-n</code>, or <code>--lump-j</code> to group jobs into the same script.</li> <li>Looper <code>run</code> has sample selection and exclusion arguments.</li> <li>You can attach multiple pipeline interfaces to a looper project, resulting in multiple pipeline submissions.</li> </ul>"},{"location":"looper/code/python-api/","title":"API","text":""},{"location":"looper/code/python-api/#package-looper-documentation","title":"Package <code>looper</code> Documentation","text":"<p>Project configuration, particularly for logging.</p> <p>Project-scope constants may reside here, but more importantly, some setup here will provide a logging infrastructure for all of the project's modules. Individual modules and classes may provide separate configuration on a more local level, but this will at least provide a foundation.</p>"},{"location":"looper/code/python-api/#class-project","title":"Class <code>Project</code>","text":"<p>Looper-specific Project.</p>"},{"location":"looper/code/python-api/#parameters","title":"Parameters:","text":"<ul> <li><code>cfg</code> (<code>str</code>):  path to configuration file with data fromwhich Project is to be built</li> <li><code>amendments</code> (<code>Iterable[str]</code>):  name indicating amendment to use, optional</li> <li><code>divcfg_path</code> (<code>str</code>):  path to an environment configuration YAML filespecifying compute settings.</li> <li><code>permissive</code> (<code>bool</code>):  Whether a error should be thrown ifa sample input file(s) do not exist or cannot be open.</li> <li><code>compute_env_file</code> (<code>str</code>):  Environment configuration YAML file specifyingcompute settings.</li> </ul> <pre><code>def __init__(self, cfg=None, amendments=None, divcfg_path=None, **kwargs)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <pre><code>def amendments(self)\n</code></pre> <p>Return currently active list of amendments or None if none was activated</p>"},{"location":"looper/code/python-api/#returns","title":"Returns:","text":"<ul> <li><code>Iterable[str]</code>:  a list of currently active amendment names</li> </ul> <pre><code>def cli_pifaces(self)\n</code></pre> <p>Collection of pipeline interface sources specified in object constructor</p>"},{"location":"looper/code/python-api/#returns_1","title":"Returns:","text":"<ul> <li><code>list[str]</code>:  collection of pipeline interface sources</li> </ul> <pre><code>def config(self)\n</code></pre> <p>Get the config mapping</p>"},{"location":"looper/code/python-api/#returns_2","title":"Returns:","text":"<ul> <li><code>Mapping</code>:  config. May be formatted to comply with the mostrecent version specifications</li> </ul> <pre><code>def config_file(self)\n</code></pre> <p>Get the config file path</p>"},{"location":"looper/code/python-api/#returns_3","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the config file</li> </ul> <pre><code>def description(self)\n</code></pre> <pre><code>def from_dict(cls, pep_dictionary: dict)\n</code></pre> <p>Init a peppy project instance from a dictionary representation of an already processed PEP.</p>"},{"location":"looper/code/python-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>pep_dictionary</code> (<code>Dict[Any]</code>):  dict,_samples: list | dict, _subsamples: list[list | dict]}</li> </ul> <pre><code>def from_pandas(cls, samples_df: pandas.core.frame.DataFrame, sub_samples_df: List[pandas.core.frame.DataFrame]=None, config: dict=None)\n</code></pre> <p>Init a peppy project instance from a pandas Dataframe</p>"},{"location":"looper/code/python-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>samples_df</code> (``):  in-memory pandas DataFrame object of samples</li> <li><code>sub_samples_df</code> (``):  in-memory list of pandas DataFrame objects of sub-samples</li> <li><code>config</code> (``):  dict of yaml file</li> </ul> <pre><code>def from_pep_config(cls, cfg: str=None, amendments: Union[str, Iterable[str]]=None, sample_table_index: Union[str, Iterable[str]]=None, subsample_table_index: Union[str, Iterable[str]]=None, defer_samples_creation: bool=False)\n</code></pre> <p>Init a peppy project instance from a yaml file</p>"},{"location":"looper/code/python-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>cfg</code> (<code>str</code>):  Project config file (YAML) or sample table (CSV/TSV)with one row per sample to constitute project</li> <li><code>sample_table_index</code> (<code>str | Iterable[str]</code>):  name of the columns to setthe sample_table index to</li> <li><code>subsample_table_index</code> (<code>str | Iterable[str]</code>):  name of the columns to setthe subsample_table index to</li> <li><code>amendments</code> (<code>str | Iterable[str]</code>):  names of the amendments to activate</li> <li><code>amendments</code> (<code>Iterable[str]</code>):  amendments to use within configuration file</li> <li><code>defer_samples_creation</code> (<code>bool</code>):  whether the sample creation should be skipped</li> </ul> <pre><code>def from_sample_yaml(cls, yaml_file: str)\n</code></pre> <p>Init a peppy project instance from a yaml file</p>"},{"location":"looper/code/python-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>yaml_file</code> (<code>str</code>):  path to yaml file</li> </ul> <pre><code>def get_sample_piface(self, sample_name)\n</code></pre> <p>Get a list of pipeline interfaces associated with the specified sample.</p> <p>Note that only valid pipeline interfaces will show up in the result (ones that exist on disk/remotely and validate successfully against the schema)</p>"},{"location":"looper/code/python-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>sample_name</code> (<code>str</code>):  name of the sample to retrieve list ofpipeline interfaces for</li> </ul>"},{"location":"looper/code/python-api/#returns_4","title":"Returns:","text":"<ul> <li><code>list[looper.PipelineInterface]</code>:  collection of validpipeline interfaces associated with selected sample</li> </ul> <pre><code>def get_schemas(pifaces, schema_key='input_schema')\n</code></pre> <p>Get the list of unique schema paths for a list of pipeline interfaces</p>"},{"location":"looper/code/python-api/#parameters_6","title":"Parameters:","text":"<ul> <li><code>pifaces</code> (<code>str | Iterable[str]</code>):  pipeline interfaces to searchschemas for</li> <li><code>schema_key</code> (<code>str</code>):  where to look for schemas in the piface</li> </ul>"},{"location":"looper/code/python-api/#returns_5","title":"Returns:","text":"<ul> <li><code>Iterable[str]</code>:  unique list of schema file paths</li> </ul> <pre><code>def is_sample_table_large(self)\n</code></pre> <pre><code>def list_amendments(self)\n</code></pre> <p>Return a list of available amendments or None if not declared</p>"},{"location":"looper/code/python-api/#returns_6","title":"Returns:","text":"<ul> <li><code>Iterable[str]</code>:  a list of available amendment names</li> </ul> <pre><code>def make_project_dirs(self)\n</code></pre> <p>Create project directory structure if it doesn't exist.</p> <pre><code>def name(self)\n</code></pre> <pre><code>def output_dir(self)\n</code></pre> <p>Output directory for the project, specified in object constructor</p>"},{"location":"looper/code/python-api/#returns_7","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the output directory</li> </ul> <pre><code>def pep_version(self)\n</code></pre> <p>The declared PEP version string</p> <p>It is validated to make sure it is a valid PEP version string</p>"},{"location":"looper/code/python-api/#returns_8","title":"Returns:","text":"<ul> <li><code>str</code>:  PEP version string</li> </ul>"},{"location":"looper/code/python-api/#raises","title":"Raises:","text":"<ul> <li><code>InvalidConfigFileException</code>:  in case of invalid PEP version</li> </ul> <pre><code>def piface_key(self)\n</code></pre> <p>Name of the pipeline interface attribute for this project</p>"},{"location":"looper/code/python-api/#returns_9","title":"Returns:","text":"<ul> <li><code>str</code>:  name of the pipeline interface attribute</li> </ul> <pre><code>def populate_pipeline_outputs(self)\n</code></pre> <p>Populate project and sample output attributes based on output schemas that pipeline interfaces point to.</p> <pre><code>def results_folder(self)\n</code></pre> <p>Path to the results folder for the project</p>"},{"location":"looper/code/python-api/#returns_10","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the results folder in the output folder</li> </ul> <pre><code>def sample_name_colname(self)\n</code></pre> <p>Deprecated, please use <code>Project.sample_table_index</code> instead</p> <p>Name of the effective sample name containing column in the sample table. It is \"sample_name\" by default, but when it's missing it could be replaced by the selected sample table index, defined on the object instantiation stage.</p>"},{"location":"looper/code/python-api/#returns_11","title":"Returns:","text":"<ul> <li><code>str</code>:  name of the column that consist of sample identifiers</li> </ul> <pre><code>def sample_table(self)\n</code></pre> <p>Get sample table. If any sample edits were performed, it will be re-generated</p>"},{"location":"looper/code/python-api/#returns_12","title":"Returns:","text":"<ul> <li><code>pandas.DataFrame</code>:  a data frame with current samples attributes</li> </ul> <pre><code>def sample_table_index(self)\n</code></pre> <p>The effective sample table index.</p> <p>It is <code>sample_name</code> by default, but could be overwritten by the selected sample table index, defined on the object instantiation stage or in the project configuration file via <code>sample_table_index</code> field. That's the sample table index selection priority order: 1. Constructor specified 2. Config specified 3. Default: <code>sample_table</code></p>"},{"location":"looper/code/python-api/#returns_13","title":"Returns:","text":"<ul> <li><code>str</code>:  name of the column that consist of sample identifiers</li> </ul> <pre><code>def samples(self)\n</code></pre> <p>Generic/base Sample instance for each of this Project's samples.</p>"},{"location":"looper/code/python-api/#returns_14","title":"Returns:","text":"<ul> <li><code>Iterable[Sample]</code>:  Sample instance for eachof this Project's samples</li> </ul> <pre><code>def selected_compute_package(self)\n</code></pre> <p>Compute package name specified in object constructor</p>"},{"location":"looper/code/python-api/#returns_15","title":"Returns:","text":"<ul> <li><code>str</code>:  compute package name</li> </ul> <pre><code>def set_sample_piface(self, sample_piface: Union[List[str], str]) -&gt; NoReturn\n</code></pre> <p>Add sample pipeline interfaces variable to object</p>"},{"location":"looper/code/python-api/#parameters_7","title":"Parameters:","text":"<ul> <li><code>sample_piface</code> (<code>list | str</code>):  sample pipeline interface</li> </ul> <pre><code>def submission_folder(self)\n</code></pre> <p>Path to the submission folder for the project</p>"},{"location":"looper/code/python-api/#returns_16","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the submission in the output folder</li> </ul> <pre><code>def subsample_table(self)\n</code></pre> <p>Get subsample table</p>"},{"location":"looper/code/python-api/#returns_17","title":"Returns:","text":"<ul> <li><code>pandas.DataFrame</code>:  a data frame with subsample attributes</li> </ul> <pre><code>def subsample_table_index(self)\n</code></pre> <p>The effective subsample table indexes.</p> <p>It is <code>[subasample_name, sample_name]</code> by default, but could be overwritten by the selected subsample table indexes, defined on the object instantiation stage or in the project configuration file via <code>subsample_table_index</code> field. That's the subsample table indexes selection priority order: 1. Constructor specified 2. Config specified 3. Default: <code>[subasample_name, sample_name]</code></p>"},{"location":"looper/code/python-api/#returns_18","title":"Returns:","text":"<ul> <li><code>List[str]</code>:  names of the columns that consist of sample and subsample identifiers</li> </ul>"},{"location":"looper/code/python-api/#class-pipelineinterface","title":"Class <code>PipelineInterface</code>","text":"<p>This class parses, holds, and returns information for a yaml file that specifies how to interact with each individual pipeline. This includes both resources to request for cluster job submission, as well as arguments to be passed from the sample annotation metadata to the pipeline</p>"},{"location":"looper/code/python-api/#parameters_8","title":"Parameters:","text":"<ul> <li><code>config</code> (<code>str | Mapping</code>):  path to file from which to parseconfiguration data, or pre-parsed configuration data.</li> <li><code>pipeline_type</code> (<code>str</code>):  type of the pipeline,must be either 'sample' or 'project'.</li> </ul> <pre><code>def __init__(self, config, pipeline_type=None)\n</code></pre> <p>Object constructor</p>"},{"location":"looper/code/python-api/#parameters_9","title":"Parameters:","text":"<ul> <li><code>entries</code> (<code>Iterable[(str, object)] | Mapping[str, object]</code>):  YAML collectionof key-value pairs.</li> <li><code>filepath</code> (<code>str</code>):  Path to the YAML config file.</li> <li><code>yamldata</code> (<code>str</code>):  YAML-formatted string</li> <li><code>locked</code> (<code>bool</code>):  Whether to initialize as locked (providing write capability)</li> <li><code>wait_max</code> (<code>int</code>):  how long to wait for creating an object when the filethat data will be read from is locked</li> <li><code>strict_ro_locks</code> (<code>bool</code>):  By default, we allow RO filesystems that can't be locked.Turn on strict_ro_locks to error if locks cannot be enforced on readonly filesystems.</li> <li><code>skip_read_lock</code> (<code>bool</code>):  whether the file should not be locked for readingwhen object is created in read only mode</li> <li><code>schema_source</code> (<code>str</code>):  path or a URL to a jsonschema in YAML format to usefor optional config validation. If this argument is provided the object is always validated at least once, at the object creation stage.</li> <li><code>validate_on_write</code> (<code>bool</code>):  a boolean indicating whether the object should bevalidated every time the <code>write</code> method is executed, which is a way of preventing invalid config writing</li> <li><code>create_file</code> (<code>str</code>):  Create an empty file at filepath upon data load.</li> </ul> <pre><code>def choose_resource_package(self, namespaces, file_size)\n</code></pre> <p>Select resource bundle for given input file size to given pipeline.</p>"},{"location":"looper/code/python-api/#parameters_10","title":"Parameters:","text":"<ul> <li><code>file_size</code> (<code>float</code>):  Size of input data (in gigabytes).</li> <li><code>namespaces</code> (<code>Mapping[Mapping[str]]</code>):  namespaced variables to passas a context for fluid attributes command rendering</li> </ul>"},{"location":"looper/code/python-api/#returns_19","title":"Returns:","text":"<ul> <li><code>MutableMapping</code>:  resource bundle appropriate for given pipeline,for given input file size</li> </ul>"},{"location":"looper/code/python-api/#raises_1","title":"Raises:","text":"<ul> <li><code>ValueError</code>:  if indicated file size is negative, or if thefile size value specified for any resource package is negative</li> <li><code>InvalidResourceSpecificationException</code>:  if no defaultresource package specification is provided</li> </ul> <pre><code>def copy(self)\n</code></pre> <p>Copy self to a new object.</p> <pre><code>def exp(self)\n</code></pre> <p>Returns a copy of the object's data elements with env vars and user vars expanded. Use it like: object.exp[\"item\"]</p> <pre><code>def get_pipeline_schemas(self, schema_key='input_schema')\n</code></pre> <p>Get path to the pipeline schema.</p>"},{"location":"looper/code/python-api/#parameters_11","title":"Parameters:","text":"<ul> <li><code>schema_key</code> (<code>str</code>):  where to look for schemas in the pipeline iface</li> </ul>"},{"location":"looper/code/python-api/#returns_20","title":"Returns:","text":"<ul> <li><code>str</code>:  absolute path to the pipeline schema file</li> </ul> <pre><code>def pipeline_name(self)\n</code></pre> <pre><code>def rebase(self, *args, **kwargs)\n</code></pre> <pre><code>def render_var_templates(self, namespaces)\n</code></pre> <p>Render path templates under 'var_templates' in this pipeline interface.</p>"},{"location":"looper/code/python-api/#parameters_12","title":"Parameters:","text":"<ul> <li><code>namespaces</code> (<code>dict</code>):  namespaces to use for rendering</li> </ul> <pre><code>def reset(self, *args, **kwargs)\n</code></pre> <pre><code>def settings(self)\n</code></pre> <pre><code>def write(self, *args, **kwargs)\n</code></pre>"},{"location":"looper/code/python-api/#class-submissionconductor","title":"Class <code>SubmissionConductor</code>","text":"<p>Collects and then submits pipeline jobs.</p> <p>This class holds a 'pool' of commands to submit as a single cluster job. Eager to submit a job, each instance's collection of commands expands until it reaches the 'pool' has been filled, and it's therefore time to submit the job. The pool fills as soon as a fill criteria has been reached, which can be either total input file size or the number of individual commands.</p> <pre><code>def __init__(self, pipeline_interface, prj, delay=0, extra_args=None, extra_args_override=None, ignore_flags=False, compute_variables=None, max_cmds=None, max_size=None, max_jobs=None, automatic=True, collate=False)\n</code></pre> <p>Create a job submission manager.</p> <p>The most critical inputs are the pipeline interface and the pipeline key, which together determine which provide critical pipeline information like resource allocation packages and which pipeline will be overseen by this instance, respectively.</p>"},{"location":"looper/code/python-api/#parameters_13","title":"Parameters:","text":"<ul> <li><code>pipeline_interface</code> (<code>PipelineInterface</code>):  Collection of importantdata for one or more pipelines, like resource allocation packages and option/argument specifications</li> <li><code>prj</code> (``):  Project with which each sample being considered isassociated (what generated each sample)</li> <li><code>delay</code> (<code>float</code>):  Time (in seconds) to wait before submitting a jobonce it's ready</li> <li><code>extra_args</code> (<code>str</code>):  string to pass to each job generated,for example additional pipeline arguments</li> <li><code>extra_args_override</code> (<code>str</code>):  string to pass to each job generated,for example additional pipeline arguments. This deactivates the 'extra' functionality that appends strings defined in Sample.command_extra and Project.looper.command_extra to the command template.</li> <li><code>ignore_flags</code> (<code>bool</code>):  Whether to ignore flag files present inthe sample folder for each sample considered for submission</li> <li><code>compute_variables</code> (<code>dict[str]</code>):  A dict with variables that will be madeavailable to the compute package. For example, this should include the name of the cluster partition to which job or jobs will be submitted</li> <li><code>max_cmds</code> (<code>int | NoneType</code>):  Upper bound on number of commands toinclude in a single job script.</li> <li><code>max_size</code> (<code>int | float | NoneType</code>):  Upper bound on total filesize of inputs used by the commands lumped into single job script.</li> <li><code>max_jobs</code> (<code>int | float | NoneType</code>):  Upper bound on total number of jobs togroup samples for submission.</li> <li><code>automatic</code> (<code>bool</code>):  Whether the submission should be automatic oncethe pool reaches capacity.</li> <li><code>collate</code> (<code>bool</code>):  Whether a collate job is to be submitted (runs onthe project level, rather that on the sample level)</li> </ul> <pre><code>def add_sample(self, sample, rerun=False)\n</code></pre> <p>Add a sample for submission to this conductor.</p>"},{"location":"looper/code/python-api/#parameters_14","title":"Parameters:","text":"<ul> <li><code>sample</code> (<code>peppy.Sample</code>):  sample to be included with this conductor'scurrently growing collection of command submissions</li> <li><code>rerun</code> (<code>bool</code>):  whether the given sample is being rerun rather thanrun for the first time</li> </ul>"},{"location":"looper/code/python-api/#returns_21","title":"Returns:","text":"<ul> <li><code>bool</code>:  Indication of whether the given sample was added tothe current 'pool.'</li> </ul>"},{"location":"looper/code/python-api/#raises_2","title":"Raises:","text":"<ul> <li><code>TypeError</code>:  If sample subtype is provided but does not extendthe base Sample class, raise a TypeError.</li> </ul> <pre><code>def failed_samples(self)\n</code></pre> <pre><code>def is_project_submittable(self, force=False)\n</code></pre> <p>Check whether the current project has been already submitted</p>"},{"location":"looper/code/python-api/#parameters_15","title":"Parameters:","text":"<ul> <li><code>frorce</code> (<code>bool</code>):  whether to force the project submission (ignore status/flags)</li> </ul> <pre><code>def num_cmd_submissions(self)\n</code></pre> <p>Return the number of commands that this conductor has submitted.</p>"},{"location":"looper/code/python-api/#returns_22","title":"Returns:","text":"<ul> <li><code>int</code>:  Number of commands submitted so far.</li> </ul> <pre><code>def num_job_submissions(self)\n</code></pre> <p>Return the number of jobs that this conductor has submitted.</p>"},{"location":"looper/code/python-api/#returns_23","title":"Returns:","text":"<ul> <li><code>int</code>:  Number of jobs submitted so far.</li> </ul> <pre><code>def submit(self, force=False)\n</code></pre> <p>Submit one or more commands as a job.</p> <p>This call will submit the commands corresponding to the current pool of samples if and only if the argument to 'force' evaluates to a true value, or the pool of samples is full.</p>"},{"location":"looper/code/python-api/#parameters_16","title":"Parameters:","text":"<ul> <li><code>force</code> (<code>bool</code>):  Whether submission should be done/simulated evenif this conductor's pool isn't full.</li> </ul>"},{"location":"looper/code/python-api/#returns_24","title":"Returns:","text":"<ul> <li><code>bool</code>:  Whether a job was submitted (or would've been ifnot for dry run)</li> </ul> <pre><code>def write_script(self, pool, size)\n</code></pre> <p>Create the script for job submission.</p>"},{"location":"looper/code/python-api/#parameters_17","title":"Parameters:","text":"<ul> <li><code>pool</code> (<code>Iterable[peppy.Sample]</code>):  collection of sample instances</li> <li><code>size</code> (<code>float</code>):  cumulative size of the given pool</li> </ul>"},{"location":"looper/code/python-api/#returns_25","title":"Returns:","text":"<ul> <li><code>str</code>:  Path to the job submission script created.</li> </ul>"},{"location":"looper/code/python-api/#class-computingconfiguration","title":"Class <code>ComputingConfiguration</code>","text":"<p>Represents computing configuration objects.</p> <p>The ComputingConfiguration class provides a computing configuration object that is an in memory representation of a <code>divvy</code> computing configuration file. This object has various functions to allow a user to activate, modify, and retrieve computing configuration files, and use these values to populate job submission script templates.</p>"},{"location":"looper/code/python-api/#parameters_18","title":"Parameters:","text":"<ul> <li><code>entries</code> (<code>str | Iterable[(str, object)] | Mapping[str, object]</code>):  configCollection of key-value pairs.</li> <li><code>filepath</code> (<code>str</code>):  YAML file specifying computing package data. (the<code>DIVCFG</code> file)</li> </ul> <pre><code>def __init__(self, entries=None, wait_max=None, strict_ro_locks=False, schema_source=None, validate_on_write=False)\n</code></pre> <p>Object constructor</p>"},{"location":"looper/code/python-api/#parameters_19","title":"Parameters:","text":"<ul> <li><code>entries</code> (<code>Iterable[(str, object)] | Mapping[str, object]</code>):  YAML collectionof key-value pairs.</li> <li><code>yamldata</code> (<code>str</code>):  YAML-formatted string</li> <li><code>wait_max</code> (<code>int</code>):  how long to wait for creating an object when the filethat data will be read from is locked</li> <li><code>strict_ro_locks</code> (<code>bool</code>):  By default, we allow RO filesystems that can't be locked.Turn on strict_ro_locks to error if locks cannot be enforced on readonly filesystems.</li> <li><code>skip_read_lock</code> (<code>bool</code>):  whether the file should not be locked for readingwhen object is created in read only mode</li> <li><code>schema_source</code> (<code>str</code>):  path or a URL to a jsonschema in YAML format to usefor optional config validation. If this argument is provided the object is always validated at least once, at the object creation stage.</li> <li><code>validate_on_write</code> (<code>bool</code>):  a boolean indicating whether the object should bevalidated every time the <code>write</code> method is executed, which is a way of preventing invalid config writing</li> </ul> <pre><code>def activate_package(self, package_name)\n</code></pre> <p>Activates a compute package.</p> <p>This copies the computing attributes from the configuration file into the <code>compute</code> attribute, where the class stores current compute settings.</p>"},{"location":"looper/code/python-api/#parameters_20","title":"Parameters:","text":"<ul> <li><code>package_name</code> (<code>str</code>):  name for non-resource compute bundle,the name of a subsection in an environment configuration file</li> </ul>"},{"location":"looper/code/python-api/#returns_26","title":"Returns:","text":"<ul> <li><code>bool</code>:  success flag for attempt to establish compute settings</li> </ul> <pre><code>def clean_start(self, package_name)\n</code></pre> <p>Clear current active settings and then activate the given package.</p>"},{"location":"looper/code/python-api/#parameters_21","title":"Parameters:","text":"<ul> <li><code>package_name</code> (<code>str</code>):  name of the resource package to activate</li> </ul>"},{"location":"looper/code/python-api/#returns_27","title":"Returns:","text":"<ul> <li><code>bool</code>:  success flag</li> </ul> <pre><code>def compute_env_var(self)\n</code></pre> <p>Environment variable through which to access compute settings.</p>"},{"location":"looper/code/python-api/#returns_28","title":"Returns:","text":"<ul> <li><code>list[str]</code>:  names of candidate environment variables, for whichvalue may be path to compute settings file; first found is used.</li> </ul> <pre><code>def default_config_file(self)\n</code></pre> <p>Path to default compute environment settings file.</p>"},{"location":"looper/code/python-api/#returns_29","title":"Returns:","text":"<ul> <li><code>str</code>:  Path to default compute settings file</li> </ul> <pre><code>def exp(self)\n</code></pre> <p>Returns a copy of the object's data elements with env vars and user vars expanded. Use it like: object.exp[\"item\"]</p> <pre><code>def from_obj(cls, entries: object, **kwargs)\n</code></pre> <p>Initialize from a Python object (dict, list, or primitive).</p>"},{"location":"looper/code/python-api/#parameters_22","title":"Parameters:","text":"<ul> <li><code>entries</code> (<code>obj</code>):  object to initialize from.</li> <li><code>kwargs</code> (``):  Keyword arguments to pass to the constructor.</li> </ul> <pre><code>def from_yaml_data(cls, yamldata, **kwargs)\n</code></pre> <p>Initialize from a YAML string.</p>"},{"location":"looper/code/python-api/#parameters_23","title":"Parameters:","text":"<ul> <li><code>yamldata</code> (<code>str</code>):  YAML-formatted string.</li> <li><code>kwargs</code> (``):  Keyword arguments to pass to the constructor.</li> </ul> <pre><code>def from_yaml_file(cls, filepath: str, create_file: bool=False, **kwargs)\n</code></pre> <p>Initialize from a YAML file.</p>"},{"location":"looper/code/python-api/#parameters_24","title":"Parameters:","text":"<ul> <li><code>filepath</code> (<code>str</code>):  Path to the YAML config file.</li> <li><code>create_file</code> (<code>str</code>):  Create a file at filepath if it doesn't exist.</li> <li><code>kwargs</code> (``):  Keyword arguments to pass to the constructor.</li> </ul> <pre><code>def get_active_package(self) -&gt; yacman.yacman_future.FutureYAMLConfigManager\n</code></pre> <p>Returns settings for the currently active compute package</p>"},{"location":"looper/code/python-api/#returns_30","title":"Returns:","text":"<ul> <li><code>YAMLConfigManager</code>:  data defining the active compute package</li> </ul> <pre><code>def get_adapters(self) -&gt; yacman.yacman_future.FutureYAMLConfigManager\n</code></pre> <p>Get current adapters, if defined.</p> <p>Adapters are sourced from the 'adapters' section in the root of the divvy configuration file and updated with an active compute package-specific set of adapters, if any defined in 'adapters' section under currently active compute package.</p>"},{"location":"looper/code/python-api/#returns_31","title":"Returns:","text":"<ul> <li><code>YAMLConfigManager</code>:  current adapters mapping</li> </ul> <pre><code>def list_compute_packages(self)\n</code></pre> <p>Returns a list of available compute packages.</p>"},{"location":"looper/code/python-api/#returns_32","title":"Returns:","text":"<ul> <li><code>set[str]</code>:  names of available compute packages</li> </ul> <pre><code>def rebase(self, *args, **kwargs)\n</code></pre> <pre><code>def reset(self, *args, **kwargs)\n</code></pre> <pre><code>def reset_active_settings(self)\n</code></pre> <p>Clear out current compute settings.</p>"},{"location":"looper/code/python-api/#returns_33","title":"Returns:","text":"<ul> <li><code>bool</code>:  success flag</li> </ul> <pre><code>def settings(self)\n</code></pre> <pre><code>def submit(self, output_path, extra_vars=None)\n</code></pre> <pre><code>def template(self)\n</code></pre> <p>Get the currently active submission template.</p>"},{"location":"looper/code/python-api/#returns_34","title":"Returns:","text":"<ul> <li><code>str</code>:  submission script content template for current state</li> </ul> <pre><code>def templates_folder(self)\n</code></pre> <p>Path to folder with default submission templates.</p>"},{"location":"looper/code/python-api/#returns_35","title":"Returns:","text":"<ul> <li><code>str</code>:  path to folder with default submission templates</li> </ul> <pre><code>def update_packages(self, config_file)\n</code></pre> <p>Parse data from divvy configuration file.</p> <p>Given a divvy configuration file, this function will update (not overwrite) existing compute packages with existing values. It does not affect any currently active settings.</p>"},{"location":"looper/code/python-api/#parameters_25","title":"Parameters:","text":"<ul> <li><code>config_file</code> (<code>str</code>):  path to file with new divvy configuration data</li> </ul> <pre><code>def write(self, filename=None)\n</code></pre> <pre><code>def write_script(self, output_path, extra_vars=None)\n</code></pre> <p>Given currently active settings, populate the active template to write a submission script. Additionally use the current adapters to adjust the select of the provided variables</p>"},{"location":"looper/code/python-api/#parameters_26","title":"Parameters:","text":"<ul> <li><code>output_path</code> (<code>str</code>):  Path to file to write as submission script</li> <li><code>extra_vars</code> (<code>Iterable[Mapping]</code>):  A list of Dict objects withkey-value pairs with which to populate template fields. These will override any values in the currently active compute package.</li> </ul>"},{"location":"looper/code/python-api/#returns_36","title":"Returns:","text":"<ul> <li><code>str</code>:  Path to the submission script file</li> </ul> <pre><code>def select_divvy_config(filepath)\n</code></pre> <p>Selects the divvy config file path to load.</p> <p>This uses a priority ordering to first choose a config file path if it's given, but if not, then look in a priority list of environment variables and choose the first available file path to return. If none of these options succeed, the default config path will be returned.</p>"},{"location":"looper/code/python-api/#parameters_27","title":"Parameters:","text":"<ul> <li><code>filepath</code> (<code>str | NoneType</code>):  direct file path specification</li> </ul>"},{"location":"looper/code/python-api/#returns_37","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the config file to read</li> </ul> <p>Version Information: <code>looper</code> v2.0.0a1, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"looper/developer-tutorial/developer-pipestat/","title":"Setting up pipestat","text":""},{"location":"looper/developer-tutorial/developer-pipestat/#introduction","title":"Introduction","text":"<p>In our previous tutorials, we deployed the <code>count_lines.sh</code> pipeline. The result of that pipeline was the number of provinces in several countries, which was simply printed into the log file. In a real-life pipeline, we usually don't just want to dig results out of log files. What if we want to do something with the pipeline result, like put it into an aggregated results file, or a database, or into PEPhub? In this tutorial, we'll demonstrate how to do that. We'll do this with pipestat, another component in PEPkit.</p> <p>Like the other PEPkit components, pipestat is a standalone tool. You can read the complete details about pipestat as a standalone tool in the pipestat documentation. You can use pipestat without using looper, and vice versa, but using pipestat alongside looper unlocks a set of helpful tools such as html reports via <code>looper report</code>. Pipestat will help us record and retrieve pipeline results in a much cleaner way.  In this tutorial, we will wire up our simple pipeline with pipestat, and demonstrate these powerful reporting tools.</p> <p>Learning objectives</p> <ul> <li>How do I configure a looper pipeline interface to interact with pipestat?</li> <li>How should I use pipestat in my pipeline to make use of looper integration?</li> </ul>"},{"location":"looper/developer-tutorial/developer-pipestat/#set-up-a-working-directory","title":"Set up a working directory","text":"<p>In this tutorial, we'll start with the files from the previous tutorial, and adjust them to use pipestat. If you'd like the completed pipestat example, you can download the pipestat_example from the hello_looper repo. Otherwise, you can follow along to create all the necessary files. To begin, create a copy of the previous tutorial:</p> <pre><code>cd ..\ncp -r pep_derived_attrs pipestat_example\nrm -rf pipestat_example/results  # remove results folder\ncd pipestat_example\n</code></pre>"},{"location":"looper/developer-tutorial/developer-pipestat/#create-a-pipestat-output-schema","title":"Create a pipestat output schema","text":"<p>First, we need a pipestat output schema.  An output schema tells pipestat what results a pipeline can report. We'll start with a simple output schema for our <code>count_lines.sh</code> pipeline, which reports a single result. Create a file named <code>pipeline/pipestat_output_schema.yaml</code> and paste this content into it:</p> pipeline/pipestat_output_schema.yaml<pre><code>title: Pipestat output schema for counting lines\ndescription: A pipeline that uses pipestat to report sample level results.\ntype: object\nproperties:\n  pipeline_name: count_lines\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        number_of_lines:\n          type: integer\n          description: \"Number of lines in the input file.\"\n</code></pre> <p>This file specifies what results are reported by a pipeline, and what type they are. It's actually a JSON Schema, which allows us to use this file to validate the results, which we'll cover later. What matters now is that this schema says our <code>count_lines</code> pipeline produces one result, called <code>number_of_lines</code>, which is of type <code>integer</code>. This is recorded under the <code>samples</code> array as a property of each sample, because your pipeline will report the <code>number_of_lines</code> for each sample.</p>"},{"location":"looper/developer-tutorial/developer-pipestat/#adapt-the-pipeline-to-report-results-with-pipestat","title":"Adapt the pipeline to report results with pipestat","text":"<p>Next, we'll update our <code>count_lines.sh</code> pipeline to make it use pipestat, instead of just logging the results to screen. For a Python pipeline, pipestat can be called directly from within Python, but <code>count_lines.sh</code> is a shell script, so we'll use the <code>pipestat</code> CLI interface. All we have to do is call pipestat to report the results of the pipeline, which we do by adding one line to the pipeline script:</p> pipeline/count_lines.sh<pre><code>#!/bin/bash\nlinecount=`wc -l $1 | sed -E 's/^[[:space:]]+//' | cut -f1 -d' '`\npipestat report -r $2 -i 'number_of_lines' -v $linecount -c $3\necho \"Number of lines: $linecount\"\n</code></pre> <p>We added one new line, which runs <code>pipestat</code> and provides it with this information:</p> <ul> <li><code>-r</code> provides the record identifier (from <code>$2</code>, the second script argument, which will be the <code>sample_name</code> in a moment)</li> <li><code>-i</code> provides the ID (or key) of the result to report, as defined in the output schema (<code>number_of_lines</code>)</li> <li><code>-v</code> provides the actual value we are reporting (the number of lines, <code>$linecount</code>)</li> <li><code>-c</code> provides a path to a pipestat configuration file, which configures how the result is stored (from <code>$3</code>, the third script argument, which we'll set up next)</li> </ul>"},{"location":"looper/developer-tutorial/developer-pipestat/#connect-pipestat-to-looper","title":"Connect pipestat to looper","text":"<p>Next, we need to update our pipeline interface so looper passes all the necessary information to the pipeline. Since the sample name (<code>-r $2</code>) and the pipestat config file (<code>-c $3</code>) weren't previously passed to the pipeline, we need to adjust the pipeline interface, to make sure the command template specifies all the inputs our pipeline needs:</p> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path} {sample.sample_name} {pipestat.config_file}\n</code></pre> <p>Now, looper will pass the sample_name and the pipestat config file as additional arguments to <code>count_lines.sh</code>. The <code>{sample.sample_name}</code> will just take the appropriate value from the sample table, just like we did previously with <code>{sample.file_path}</code> The <code>{pipestat.config_file}</code> is automatically provided by looper. Looper generates this config file based on the looper configuration and the pipeline interface. To read more about pipestat config files, see here: pipestat configuration.</p> <p>Next, we need to tell looper we're dealing with a pipestat-compatible pipeline. Specify this by adding the <code>output_schema</code> in the pipeline interface to the pipestat output schema we created earlier:</p> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\noutput_schema: pipestat_output_schema.yaml\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path} {sample.sample_name} {pipestat.config_file}\n</code></pre> <p>Finally, we need to configure where the pipestat results will be stored. Pipestat offers several ways to store results, including a simple file for a basic pipeline, or a relational database, or even PEPhub. We'll start with the simplest option and configure pipestat to use a results file. Configure pipestat through the looper config file like this:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  results_file_path: results.yaml\n</code></pre> <p>This instructs looper to configure pipestat to store the results in a <code>.yaml</code> file. Looper will now configure the pipeline to report results into a <code>results.yaml</code> file.</p> <p>Execute the run with: <pre><code>looper run\n</code></pre></p> <p>You should now be able to navigate to the <code>results.yaml</code> file and see the reported results within. Now that you have your first pipestat pipeline configured with looper, there's many other, more powerful things you can add to make this even more useful. For example, now that looper knows the structure of results your pipeline reports, it can automatically generate beautiful, project-wide results summary HTML pages for you. It also provides some ability to monitor jobs and make sure they are all succeeding. But before we get into the details of these advanced features, we'll take a small detour now to show you how to wire up pipestat to a Python pipeline, instead of a shell script.</p> <p>Key Point</p> <p>One of the remarkable things about this setup is that the result reports are decoupled from the pipeline. All the pipeline has to do is call <code>pipestat</code>  and provide the sample identifier, the key of the result, and the result value. There's no configuration of pipestat at the pipeline level; instead, configuration of the results storage is passed through from the user's looper config file. This allows a user to change how the results are reported for any pipeline. We've basically shifted the task of where to store pipeline results from the pipeline author (who usually handles that) to the pipeline user (who actually cares about that).</p>"},{"location":"looper/developer-tutorial/developer-pipestat/#a-python-based-pipeline","title":"A  Python-based pipeline","text":"<p>If your pipeline is written in Python, using pipestat is even easier. Looper can run a <code>.py</code> just as easily as a <code>.sh</code> file and, using pipestat's python API, can report results from within the <code>.py</code> file.</p> <p>First, we will need to change our shell pipeline to a Python-based pipeline. In the same folder as your <code>count_lines.sh</code>, create a new file <code>count_lines.py</code> with this code:</p> <pre><code>import pipestat\nimport sys\nimport os.path\n\n# Very simple pipeline that counts lines and reports the final count via pipestat\n\n# Obtain arguments invoked during looper submission via command templates\ntext_file = sys.argv[1] \nsample_name = sys.argv[2]   # pipestat needs a unique sample identifier. Looper uses sample_name but pipestat uses record_identifier\nconfig_file_path = sys.argv[3]  # this is the config file path\n\n# Create pipestat manager\npsm = pipestat.PipestatManager(\n    record_identifier=sample_name,\n    config_file=config_file_path,\n)\n\n# Read text file and count lines\ntext_file = os.path.abspath(text_file)\nwith open(text_file, \"r\") as f:\n    result = {\"number_of_lines\": len(f.readlines())}\n\n# Report Results using Pipestat\npsm.report(record_identifier=sample_name, values=result)\n</code></pre> <p>Make sure <code>count_lines.py</code> is executable:</p> <pre><code>chmod 755 pipeline/count_lines.py\n</code></pre> <p>You can now run the example with:</p> <pre><code>looper run\n</code></pre>"},{"location":"looper/developer-tutorial/developer-pipestat/#generating-result-reports","title":"Generating result reports","text":""},{"location":"looper/developer-tutorial/developer-pipestat/#html-reports","title":"HTML reports","text":"<p>Now comes the fun part: the reason we want to go to the work of specifying the results in the pipestat output schema, and then reporting them using pipestat, is that this will structure them in a way that we can easily read and aggregate them. Looper provides an easy <code>report</code> command that creates an html report of all reported results. You've already configured everything above. To get the report, run the command:</p> <pre><code>looper report\n</code></pre> <p>This command will call <code>pipestat summarize</code> on the results located in your results location. In this case, the <code>results.yaml</code> file.</p> <p>Here is an example html report for the above tutorial examples: count lines report</p> <p>A more advanced example of an html report using <code>looper report</code> can be found here: PEPATAC Gold Summary</p>"},{"location":"looper/developer-tutorial/developer-pipestat/#create-tables-and-stats-summaries","title":"Create tables and stats summaries","text":"<p>Having a nice HTML-browsable record of results is great for human browsing, but you may also want the aggregated results in a machine-readable form for downstream analysis. Looper can also create summaries in a computable format as  <code>.tsv</code> and <code>.yaml</code> files.</p> <p>Run: <pre><code>looper table\n</code></pre></p> <p>This will produce a <code>.tsv</code> file for aggregated primitive results (integers, strings, etc), as well as a <code>.yaml</code> file for any aggregated object results: <pre><code>Looper version: 2.0.0\nCommand: table\nUsing looper config (.looper.yaml).\nCreating objects summary\n'count_lines' pipeline stats summary (n=4): results/count_lines_stats_summary.tsv\n'count_lines' pipeline objects summary (n=0): results/count_lines_objs_summary.yaml\n</code></pre></p>"},{"location":"looper/developer-tutorial/developer-pipestat/#reporting-results-back-to-pephub","title":"Reporting results back to PEPhub","text":"<p>In the previous tutorial, you configured looper to read sample metadata from PEPhub. Now, by adding in pipestat integration, we can also report pipeline results back to PEPhub. In this example, we'll report the results back to the demo PEP we used earlier, <code>databio/pipestat_demo:default</code>. But you won't be able to report the results back to the demo repository because you don't have permissions. So if you want to follow along, you'll first need to create your own PEP on PEPHub to hold these results. Then, you can run this section yourself by replacing <code>databio/pipestat_demo:default</code> with the registry path to a PEP you control.</p> <p>To configure pipestat to report results to PEPhub instead of to a file, we just change our looper config to point to a <code>pephub_path</code>:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  pephub_path: \"databio/pipestat_demo:default\"\n  flag_file_dir: results/flags\n</code></pre> <p>No other changes are necessary. You will have to authenticate with PEPhub using <code>phc login</code>, and then looper will pass along the information in the generated pipestat config file. Pipestat will read the <code>pephub_path</code> from the config file and report results directly to PEPhub using its API!</p>"},{"location":"looper/developer-tutorial/developer-pipestat/#setting-and-checking-status","title":"Setting and checking status","text":"<p>Besides reporting results, another feature of pipestat is that it allows users to set pipeline status. If your pipeline uses pipestat to set status flags, then looper can be used to check the status of pipeline runs. Let's modify the pipeline to set status:</p> <pre><code>import pipestat\nimport sys\nimport os.path\n\n# Very simple pipeline that counts lines and reports the final count via pipestat\n\n# Obtain arguments invoked during looper submission via command templates\ntext_file = sys.argv[1] \nsample_name = sys.argv[2]   # pipestat needs a unique sample identifier. Looper uses sample_name but pipestat uses record_identifier\nconfig_file_path = sys.argv[3]  # this is the config file path\n\n# Create pipestat manager\npsm = pipestat.PipestatManager(\n    record_identifier=sample_name,\n    config_file=config_file_path,\n)\n\n# Set status for this sample to 'running'\npsm.set_status(record_identifier=sample_name, status_identifier=\"running\")\n\n#  Read text file and count lines\ntext_file = os.path.abspath(text_file)\nwith open(text_file, \"r\") as f:\n    result = {\"number_of_lines\": len(f.readlines())}\n\n# Report Results using Pipestat\npsm.report(record_identifier=sample_name, values=result)\n\n# Set status for this sample to 'completed'\npsm.set_status(record_identifier=sample_name, status_identifier=\"completed\")\n</code></pre> <p>This function creates a flag file in our flag file directory that indicates the status of the current sample. When the pipeline begins, pipestat sets the status to 'running'. When the pipeline completes, pipestat sets the status to 'completed'. Looper will be able to read these flags to tell us how many jobs are running and completed.</p> <p>One last thing: we should modify the looper config file to set pipestat's \"flag file directory\". This <code>flag_file_dir</code> will be passed along in the configuration file generated by looper:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  results_file_path: results.yaml\n  flag_file_dir: results/flags\n</code></pre> <p>Now, the pipeline is configured to set pipestat status flags. Run looper again: <code>looper run</code>. Then, to check the status of all samples, use:</p> <pre><code>looper check\n</code></pre> <p>For this example, the 'running' flag doesn't really help because the pipeline runs so fast that it immediately finishes. But in a pipeline that will take minutes or hours to complete, it can be useful to know how many and which jobs are running. That's why <code>looper check</code> can be helpful for these long-running pipelines.</p> <p>Do I have to use pipestat?</p> <p>No. You can use looper just as we did in the first two tutorials to run any command. Often, you'll want to use looper to run an existing pipeline that you didn't create. In that case, you won't have the option of using pipestat, since you're unlikely to go to the effort of adapting someone else's pipeline to use it. For non-pipestat-compatible pipelines, you can still use looper to run pipelines, but you won't be able to use <code>looper report</code> or <code>looper check</code> to manage their output.</p> <p>What benefits does pipestat give me? If you are developing your own pipeline, then you might want to consider using pipestat in your pipeline. This will allow users to use <code>looper check</code> to check on the status of pipelines. It will also enable <code>looper report</code> and <code>looper table</code> to create summarized outputs of pipeline results.</p> <p>Summary</p> <ul> <li>Pipestat is a standalone tool that can be used with or without looper.</li> <li>Pipestat standardizes reporting of pipeline results. It provides a standard specification for how pipeline outputs should be stored; and an implementation to easily write results to that format from within Python or from the command line.</li> <li>A pipeline user can configure a pipestat-compatible pipeline to record results in a file, in a database, or in PEPhub.</li> <li>Looper synergizes with pipestat to add powerful features such as checking job status and generating html reports.</li> </ul>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/","title":"Pipeline interface specification","text":"Pipeline interface specification <p>Table of contents:</p> <ul> <li>Introduction</li> <li>Overview of pipeline interface components</li> <li>Example pipeline interface</li> <li>Details of pipeline interface components<ul> <li>pipeline_name</li> <li>sample_interface and project_interface</li> <li>command_template</li> <li>input_schema</li> <li>output_schema</li> <li>compute<ul> <li>size_dependent_variables</li> <li>var_templates</li> <li>pre_submit</li> </ul> </li> </ul> </li> <li>Validating a pipeline interface</li> </ul>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#introduction","title":"Introduction","text":"<p>To run an arbitrary pipeline, we require a formal specification for how the pipeline is run. We define this using a pipeline interface file. It maps attributes of a PEP project or sample to the pipeline CLI arguments. Thus, it defines the interface between the project metadata (the PEP) and the pipeline itself.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#overview-of-pipeline-interface-components","title":"Overview of pipeline interface components","text":"<p>A pipeline interface may contain the following keys:</p> <ul> <li><code>pipeline_name</code> (REQUIRED) - A string identifying the pipeline.</li> <li><code>sample_interface</code> OR <code>project_interface</code> (REQUIRED) - which will hold the  <code>command_template</code> (REQUIRED) variable with a Jinja2 template used to construct a pipeline command to run.</li> <li><code>input_schema</code> (RECOMMENDED) - A PEP Schema formally defining required inputs for the pipeline</li> <li><code>output_schema</code> (RECOMMENDED) - A schema describing the outputs of the pipeline, for pipestat-compatible pipelines.</li> <li><code>compute</code> (RECOMMENDED) - Settings for computing resources</li> <li><code>var_templates</code> (OPTIONAL) - A mapping of Jinja2 templates and corresponding names, typically used to parameterize plugins</li> <li><code>pre_submit</code> (OPTIONAL) - A mapping that defines the pre-submission tasks to be executed.</li> </ul>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#example-pipeline-interface","title":"Example pipeline interface","text":"pipeline_interface.yaml<pre><code>pipeline_name: RRBS\nvar_templates:\n  pipeline: \"{looper.piface_dir}/pipeline1.py\"\n  sample_info: \"{looper.piface_dir}/{sample.name}/info.txt\"\ninput_schema: path/to/rrbs_schema.yaml\nsample_interface:\n  command_template: &gt;\n    {pipeline.var_templates.pipeline} --input {sample.data_path} --info {pipeline.sample_info.path}\n</code></pre>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#details-of-pipeline-interface-components","title":"Details of pipeline interface components","text":""},{"location":"looper/developer-tutorial/pipeline-interface-specification/#pipeline_name","title":"pipeline_name","text":"<p>The <code>pipeline_name</code> is arbitrary. It should be unique for each pipeline.  Looper uses it for a few things:</p> <ol> <li> <p>to construct the <code>job_name</code> variable (accessible via <code>{ looper.job_name }</code>). See looper variable namespaces in advanced computing for more details.</p> </li> <li> <p>to check for flags. For pipelines that produce flags, looper will be aware of them and not re-submit running jobs.</p> </li> <li> <p>to communicate with the user. In log files, or on the screen, this name will be the string used to identify the pipeline to the user.</p> </li> </ol>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#sample_interface-and-project_interface","title":"sample_interface and project_interface","text":"<p>Looper can run 2 kinds of pipeline: sample pipelines run once per sample; project pipelines run once per project. The pipeline interface should define either a <code>sample_interface</code>, a <code>project_interface</code>, or both. You must nest the <code>command_template</code> under a <code>sample_interface</code> or <code>project_interface</code> key to let looper know at which level to run the pipeline.</p> <p>When a user invokes <code>looper run</code>, looper reads data under <code>sample_interface</code> and will create one job template per sample. In contrast, when a user invokes <code>looper runp</code>, looper reads data under <code>project_interface</code> and  creates one job per project (one job total). </p> <p>The only other difference is that the sample-level command template has access to the <code>{sample.&lt;attribute&gt;}</code> namespace, whereas the project-level command template has access to the <code>{samples}</code> array.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#command_template","title":"command_template","text":"<p>The command template is the most critical part of the pipeline interface. It is a Jinja2 template for the command to run for each sample. Within the <code>command_template</code>, you have access to variables from several sources. These variables are divided into namespaces depending on the variable source. You can access the values of these variables in the command template using the single-brace jinja2 template language syntax: <code>{namespace.variable}</code>. For example, looper automatically creates a variable called <code>job_name</code>, which you may want to pass as an argument to your pipeline. You can access this variable with <code>{looper.job_name}</code>. The available namespaces are described in detail in the advanced computing guide.</p> <p>Because it's based on Jinja2, command templates are extremely flexible. For example, optional arguments can be accommodated using Jinja2 syntax, like this:</p> pipeline_interface.yaml<pre><code>command_template: &gt;\n  {pipeline.path}\n  --sample-name {sample.sample_name}\n  --genome {sample.genome}\n  --input {sample.read1}\n  --single-or-paired {sample.read_type}\n  {% if sample.read2 is defined %} --input2 {sample.read2} {% endif %}\n  {% if sample.peak_caller is defined %} --peak-caller {sample.peak_caller} {% endif %}\n  {% if sample.FRIP_ref is defined %} --frip-ref-peaks {sample.FRIP_ref} {% endif %}\n</code></pre> <p>The arguments wrapped in these Jinja2 conditionals will only be added if the specified attribute exists for the sample.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#input_schema","title":"input_schema","text":"<p>The input schema formally specifies the input processed by this pipeline. The input schema serves 2 related purposes:</p> <ol> <li> <p>Validation. Looper uses the input schema to ensure that the project fulfills all pipeline requirements before submitting any jobs. Looper uses the PEP validation tool, eido, to validate input data by ensuring that input samples have the attributes and input files required by the pipeline. Looper will only submit a sample pipeline if the sample validates against the pipeline's input schema.</p> </li> <li> <p>Description. The input schema is also useful to describe the inputs, including both required and optional inputs, thereby providing a standard way to describe a pipeline's inputs. In the schema, the pipeline author can describe exactly what the inputs mean, making it easier for users to learn how to structure a project for the pipeline.</p> </li> </ol> <p>Details for how to write a schema in writing a schema. The input schema format is an extended PEP JSON-schema validation framework, which adds several capabilities, including</p> <ul> <li><code>required</code> (optional): A list of sample attributes (columns in the sample table) that must be defined</li> <li><code>tangible</code> (optional): A list of sample attributes that point to input files that must exist.</li> <li><code>sizing</code> (optional): A list of sample attributes that point to input files that are not necessarily required, but if they exist, should be counted in the total size calculation for requesting resources.</li> </ul> <p>If no <code>input_schema</code> is included in the pipeline interface, looper will not be able to validate the samples and will simply submit each job without validation.</p> <p>Here is an example input schema:</p> Example input_schema.yaml<pre><code>description: A PEP for ATAC-seq samples for the PEPATAC pipeline.\nimports:\n  - http://schema.databio.org/pep/2.0.0.yaml\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        sample_name: \n          type: string\n          description: \"Name of the sample\"\n        organism: \n          type: string\n          description: \"Organism\"\n        protocol: \n          type: string\n          description: \"Must be an ATAC-seq or DNAse-seq sample\"\n        genome:\n          type: string\n          description: \"Refgenie genome registry identifier\"\n        read_type:\n          type: string\n          description: \"Is this single or paired-end data?\"\n          enum: [\"SINGLE\", \"PAIRED\"]\n        read1:\n          anyOf:\n            - type: string\n              description: \"Fastq file for read 1\"\n            - type: array\n              items:\n                type: string\n        read2:\n          anyOf:\n            - type: string\n              description: \"Fastq file for read 2 (for paired-end experiments)\"\n            - type: array\n              items:\n                type: string\n      tangible:\n        - read1\n      sizing:\n        - read1\n        - read2\n      required:\n        - sample_name\n        - protocol\n        - read1\n        - genome\nrequired:\n  - samples\n</code></pre>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#output_schema","title":"output_schema","text":"<p>The output schema formally specifies the output produced by this pipeline. It is used by downstream tools to that need to be aware of the products of the pipeline for further visualization or analysis. Beginning with Looper 1.6.0 and Pipestat 0.6.0, the output schema is a JSON-schema: pipestat schema specification.</p> <p>Here is an example output schema:</p> <p>Example output_schema.yaml<pre><code>title: An example output schema\ndescription: A pipeline that uses pipestat to report sample and project level results.\ntype: object\nproperties:\n  pipeline_name: \"default_pipeline_name\"\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        number_of_things:\n          type: integer\n          description: \"Number of things\"\n        percentage_of_things:\n          type: number\n          description: \"Percentage of things\"\n        name_of_something:\n          type: string\n          description: \"Name of something\"\n        switch_value:\n          type: boolean\n          description: \"Is the switch on or off\"\n        md5sum:\n          type: string\n          description: \"MD5SUM of an object\"\n          highlight: true\n        collection_of_images:\n          description: \"This store collection of values or objects\"\n          type: array\n          items:\n            properties:\n                prop1:\n                  description: \"This is an example file\"\n                  $ref: \"#/$defs/file\"\n        output_file_in_object:\n          type: object\n          properties:\n            prop1:\n              description: \"This is an example file\"\n              $ref: \"#/$defs/file\"\n            prop2:\n              description: \"This is an example image\"\n              $ref: \"#/$defs/image\"\n          description: \"Object output\"\n        output_file_in_object_nested:\n          type: object\n          description: First Level\n          properties:\n            prop1:\n              type: object\n              description: Second Level\n              properties:\n                prop2:\n                  type: integer\n                  description: Third Level\n        output_file:\n          $ref: \"#/$defs/file\"\n          description: \"This a path to the output file\"\n        output_image:\n          $ref: \"#/$defs/image\"\n          description: \"This a path to the output image\"\n$defs:\n  image:\n    type: object\n    object_type: image\n    properties:\n      path:\n        type: string\n      thumbnail_path:\n        type: string\n      title:\n        type: string\n    required:\n      - path\n      - thumbnail_path\n      - title\n  file:\n    type: object\n    object_type: file\n    properties:\n      path:\n        type: string\n      title:\n        type: string\n    required:\n      - path\n      - title\n</code></pre> Looper uses the output schema in its <code>report</code> function, which produces a browsable HTML report summarizing the pipeline results. The output schema provides the relative locations to sample-level and project-level outputs produced by the pipeline, which looper can then integrate into the output results. If the output schema is not included, the <code>looper report</code> will be unable to locate and integrate the files produced by the pipeline and will therefore be limited to simple statistics.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#compute","title":"compute","text":"<p>The compute section of the pipeline interface provides a way to set compute settings at the pipeline level. These variables can then be accessed in the command template. They can also be overridden by values in the PEP config, or on the command line. </p> <p>There is one reserved attribute under <code>compute</code> with specialized behavior -- <code>size_dependent_variables</code> which we'll now describe in detail.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#size_dependent_variables","title":"size_dependent_variables","text":"<p>The <code>size_dependent_variables</code>  section lets you specify variables with values that are modulated based on the total input file size for the run. This is typically used to add variables for memory, CPU, and clock time to request, if they depend on the input file size. Specify variables by providing a relative path to a <code>.tsv</code> file that defines the variables as columns, with input sizes as rows.</p> <p>The pipeline interface simply points to a <code>tsv</code> file:</p> <pre><code>pipeline_type: sample\nvar_templates:\n  pipeline: {looper.piface_dir}/pepatac.py\ncommand_template: &gt;\n  {pipeline.var_templates.pipeline} ...\ncompute:\n  size_dependent_variables: resources-sample.tsv\n</code></pre> <p>The <code>resources-sample.tsv</code> file consists of a file with at least 1 column called <code>max_file_size</code>. Add any other columns you wish, each one will represent a new attribute added to the <code>compute</code> namespace and available for use in your command template. Here's an example:</p> <pre><code>max_file_size cores mem time\n0.001 1 8000  00-04:00:00\n0.05  2 12000 00-08:00:00\n0.5 4 16000 00-12:00:00\n1 8 16000 00-24:00:00\n10  16  32000 02-00:00:00\nNaN 32  32000 04-00:00:00\n</code></pre> <p>This example will add 3 variables: <code>cores</code>, <code>mem</code>, and <code>time</code>, which can be accessed via <code>{compute.cores}</code>, <code>{compute.mem}</code>, and <code>{compute.time}</code>. Each row defines a \"packages\" of variable values. Think of it like a group of steps of increasing size. For a given job, looper calculates the total size of the input files (which are defined in the <code>input_schema</code>). Using this value, looper then selects the best-fit row by iterating over the rows until the calculated input file size does not exceed the <code>max_file_size</code> value in the row. This selects the largest resource package whose <code>max_file_size</code> attribute does not exceed the size of the input file. Max file sizes are specified in GB, so <code>5</code> means 5 GB.</p> <p>This final line in the resources <code>tsv</code> must include <code>NaN</code> in the <code>max_file_size</code> column, which serves as a catch-all for files larger than the largest specified file size. Add as many resource sets as you want.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#var_templates","title":"var_templates","text":"<p>This section can consist of variable templates that are rendered and can be reused. The namespaces available to the templates are listed in advanced computing section.</p> <p>The use of <code>var_templates</code> is for parameterizing pre-submission functions with parameters that require template rendering. This is an advanced use case.</p> <p>Beginning with Looper 1.9.0, var_templates can also be nested:</p> <pre><code>var_templates:\n  refgenie_plugin:\n    config_path: \"...\"\n  custom_template_plugin:\n     config_path: \"...\"\n</code></pre>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#pre_submit","title":"pre_submit","text":"<p>This section can consist of two subsections: <code>python_functions</code> and/or <code>command_templates</code>, which specify the pre-submission tasks to be run before the main pipeline command is submitted. Please refer to the pre-submission hooks system section for a detailed explanation of this feature and syntax.</p>"},{"location":"looper/developer-tutorial/pipeline-interface-specification/#validating-a-pipeline-interface","title":"Validating a pipeline interface","text":"<p>A pipeline interface can be validated using JSON Schema against schema.databio.org/pipelines/pipeline_interface.yaml. Looper automatically validates pipeline interfaces at submission initialization stage.</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/","title":"Pre-submission hooks","text":""},{"location":"looper/developer-tutorial/pre-submission-hooks/#purpose","title":"Purpose","text":"<p>Sometimes we need to run a set-up task before submitting the main pipeline. For example, we may need to generate a particular representation of the sample metadata to be consumed by a pipeline run. Some pre-submission tasks may depend on information outside of the sample, such as compute settings. For this purpose, looper provides pre-submission hooks, which allow users to run arbitrary shell commands or Python functions before submitting the actual pipeline. These hooks have access to all of the job submission settings looper uses to populate the primary command template. They can be used in two ways: 1) to simply run required tasks, producing required output before the pipeline is run; and 2) to modify the job submission settings, which can then be used in the actual submission template.</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#how-to-specify-pre-submission-tasks-in-the-pipeline-interface","title":"How to specify pre-submission tasks in the pipeline interface","text":"<p>The pre-submission tasks to be executed are listed in the pipeline interface file under the top-level <code>pre_submit</code> key. The <code>pre_submit</code> section is divided into two subsections corresponding to two types of hooks: <code>python_functions</code> and <code>command_templates</code>. The <code>python_functions</code> key specifies a list of strings corresponding to Python functions to run. The <code>command_templates</code> key is more generic, specifying shell command templates to be executed in a subprocess. Here is an example:</p> pipeline_interface.yaml<pre><code>...\npre_submit:\n  python_functions:\n    - \"package_name.function_name\"\n    - \"package_name1.function_name\"\n  command_templates:\n    - \"tool.sh --param {sample.attribute}\"\n    - \"tool1.sh --param {sample.attribute1}\"\n...\n</code></pre> <p>Because the looper variables are the input to each task, and are also potentially modified by each task, the order of execution is critical. Execution order follows two rules: First, <code>python_functions</code> are always executed before <code>command_templates</code>; and second, the user-specified order in the pipeline interface is preserved within each subsection.</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#built-in-pre-submission-functions","title":"Built-in pre-submission functions","text":"<p>Looper ships with several included plugins that you can use as pre-submission functions without installing additional software. These plugins produce various representations of the sample metadata, which can be useful for different types of pipelines. The included plugins are described below:</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#included-plugin-looperwrite_sample_yaml","title":"Included plugin: <code>looper.write_sample_yaml</code>","text":"<p>Saves all sample metadata as a YAML file. The output file path can be customized using <code>var_templates.sample_yaml_path</code>. If this parameter is not provided, the file will be saved as <code>{looper.output_dir}/submission/{sample.sample_name}_sample.yaml</code>.</p> <p>Parameters:</p> <ul> <li><code>pipeline.var_templates.sample_yaml_path</code> (optional): absolute path to file where YAML is to be stored.</li> </ul> <p>Usage:</p> <pre><code>var_templates:\n  main: \"{looper.piface_dir}/pipelines/pipeline1.py\"\n  sample_yaml_path: \"{looper.output_dir}/custom_sample_yamls/{sample.sample_name}.yaml\"\npre_submit:\n  python_functions:\n    - looper.write_sample_yaml\ncommand_template: &gt;\n  {pipeline.var_templates.main} ...\n</code></pre>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#included-plugin-looperwrite_sample_yaml_cwl","title":"Included plugin: <code>looper.write_sample_yaml_cwl</code>","text":"<p>This plugin writes a sample yaml file compatible as a job input file for a CWL pipeline. This plugin allows looper to be used as a scatterer to run an independent CWL workflow for each sample in your PEP sample table. You can parametrize the plugin with a custom output file name using <code>sample_yaml_cwl_path</code>. If the parameter is not provided, the file will be saved in <code>{looper.output_dir}/submission/{sample.sample_name}_sample_cwl.yaml</code>.</p> <p>Parameters:</p> <ul> <li><code>pipeline.var_templates.sample_yaml_path</code> (optional): absolute path to file where YAML is to be stored.</li> </ul> <p>Usage:</p> <pre><code>var_templates:\n  main: \"{looper.piface_dir}/pipelines/pipeline1.py\"\n  sample_yaml_cwl_path: \"{looper.output_dir}/custom_sample_yamls/custom_{sample.name}.yaml\"\npre_submit:\n  python_functions:\n    - looper.write_sample_yaml_cwl\ncommand_template: &gt;\n  {pipeline.var_templates.main} ...\n</code></pre>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#included-plugin-looperwrite_sample_yaml_prj","title":"Included plugin: <code>looper.write_sample_yaml_prj</code>","text":"<p>Saves the sample to YAML file with project reference.  This plugin can be parametrized with a custom YAML directory (see \"parameters\" below). If the parameter is not provided, the file will be saved in <code>{looper.output_dir}/submission/{sample.sample_name}_sample_prj.yaml</code>.</p> <p>Parameters:</p> <ul> <li><code>pipeline.var_templates.sample_yaml_prj_path</code> (optional): absolute path to file where YAML is to be stored.</li> </ul> <p>Usage:</p> <pre><code>var_templates:\n  main: \"{looper.piface_dir}/pipelines/pipeline1.py\"\n  sample_yaml_prj_path: \"{looper.output_dir}/custom_sample_yamls\"\npre_submit:\n  python_functions:\n    - looper.write_sample_yaml_prj\ncommand_template: &gt;\n  {pipeline.var_templates.main} ...\n</code></pre>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#included-plugin-looperwrite_submission_yaml","title":"Included plugin: <code>looper.write_submission_yaml</code>","text":"<p>Saves all five namespaces of pre-submission to YAML file.  This plugin can be parametrized with a custom YAML directory (see \"parameters\" below). If the parameter is not provided, the file will be saved in <code>{looper.output_dir}/submission/{sample.sample_name}_submission.yaml</code>.</p> <p>Parameters:</p> <ul> <li><code>pipeline.var_templates.submission_yaml_path</code> (optional): a complete and absolute path to the directory where submission YAML representation is to be stored.</li> </ul> <p>Example usage:</p> <pre><code>var_templates:\n  main: \"{looper.piface_dir}/pipelines/pipeline1.py\"\n  submission_yaml_path: \"{looper.output_dir}/custom_path\"\npre_submit:\n  python_functions:\n    - looper.write_submission_yaml\ncommand_template: &gt;\n  {pipeline.var_templates.main} ...\n</code></pre>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#included-plugin-looperwrite_custom_template","title":"Included plugin: <code>looper.write_custom_template</code>","text":"<p>Populates an independent jinja template with values from all the available looper namespaces.</p> <p>Parameters:</p> <ul> <li><code>pipeline.var_templates.custom_template</code> (required): a jinja template to be populated for each job.</li> <li><code>pipeline.var_templates.custom_template_output</code> (optional): path to which the populated template file will be saved. If not provided, the populated fill will be saved in `{looper.output_dir}/submission/{sample.sample_name}_config.yaml</li> </ul> <p>Example usage:</p> <pre><code>var_templates:\n  custom_template: custom_template.jinja\n  custom_template_output: \"{looper.output_dir}/submission/{sample.sample_name}_custom_config.yaml\"\npre_submit:\n  python_functions:\n    - looper.write_custom_template\ncommand_template: &gt;\n  {pipeline.var_templates.main} ...\n</code></pre>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#writing-your-own-pre-submission-hooks","title":"Writing your own pre-submission hooks","text":"<p>Pre-submission tasks can be written as a Python function or a shell commands. We will explain each type below:</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#python-functions","title":"Python functions","text":"<p>Python plugin functions have access all of the metadata variables looper has access to to construct the primary command template. The Python function must obey the following rules:</p> <ol> <li> <p>The Python function must take as input a <code>namespaces</code> object, which is a Python <code>dict</code> of looper variable namespaces.</p> </li> <li> <p>The function should return any updated namespace variables; or can potentially return an empty <code>dict</code> (<code>{}</code>) if no changes are intended, which may the case if the function is only used for its side effect.</p> </li> </ol>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#custom-function-input-parameters-using-var_templates","title":"Custom function input parameters: using <code>var_templates</code>","text":"<p>How can you parameterize your plugin function? Since the function will have access to all the looper variable namespaces, this means that plugin authors may require users to specify any attributes within any namespace to parametrize them. For example, a plugin that increases the compute wall time by an arbitrary amount of time may require <code>extra_time</code> attribute in the <code>pipeline</code> namespace. Users would specify this parameter like this:</p> <pre><code>pipeline_name: my_pipeline\nextra_time: 3\n</code></pre> <p>This variable would be accessible in your python function as <code>namespaces[\"pipeline\"][\"extra_time\"]</code>. This works, but we recommend keeping things clean by putting all required pipeline parameters into the <code>pipeline.var_templates</code> section. This not only keeps things tidy in a particular section, but also adds additional functionality of making these templates that can themselves refer to namespace variables, which can be very convenient. For example, a better approach would be:</p> <pre><code>pipeline_name: my_pipeline\nvar_templates:\n  extra_time: 3\n  plugin_path: \"{looper.piface_dir}/plugin_results\"\n</code></pre> <p>In this example you'd use <code>namespaces[\"pipeline\"][\"var_templates\"][\"extra_time\"]</code> to access the user-provided parameter. Notice we included another example, <code>plugin_path</code>, which can refer to the <code>{looper.piface_dir}</code> variable. Because this variable is included under <code>var_templates</code>, it will be populated with any namespace variables.</p> <p>The plugins need to handle incomplete parametrization, either by providing defaults or by raising exceptions.</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#function-output-updating-submission-metadata-via-return-value","title":"Function output: updating submission metadata via return value","text":"<p>One of the features of the pre-submission hooks is that they can be used to update the looper variable namespaces so that you can use modified variables in your primary command template. This is effectively a way for a plugin function to provide output that can be used by looper. The way this works is that after every successful pre-submission hook execution, the input namespaces are updated with the return value of the hook execution. Existing values are overwritten with the returned ones, whereas omitted values are not changed. Therefore, you must simply write your function to return any updated variables in the same format as in the input function. That is, your return value should be a Python <code>dict</code> of looper variable namespaces</p> <p>For example, given this input (which represents the looper variable namespaces):</p> <p>Input: <pre><code>sample:\n    name: test\n    size: 30\n    genome: hg38\nlooper:\n    log_file: /home/michal/my_log.txt\n    job_name: test_pepatac\ncompute:\n    submission_template: /home/michal/divvy_templates/localhost_template.sub\n    submission_command: sh\n...\n</code></pre></p> <p>Say your function returned this data: <pre><code>sample:\n    size: 1000\nlooper:\n    log_file: /home/michal/Desktop/new_log.txt\n</code></pre></p> <p>Then looper would have this object available for populating the primary command template (input + returned data): <pre><code>sample:\n    name: test\n    size: 1000\n    genome: hg38\nlooper:\n    log_file: /home/michal/Desktop/new_log.txt\n    job_name: test_pepatac\ncompute:\n    submission_template: /home/michal/divvy_templates/localhost_template.sub\n    submission_command: sh\n...\n</code></pre></p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#shell-command-plugins","title":"Shell command plugins","text":"<p>In case you need more flexibility than a Python function, you can also execute arbitrary commands as a pre-submission task. You define exactly what command you want to run, like this:</p> <pre><code>var_templates:\n  compute_script: \"{looper.piface_dir}/hooks/script.py\"\npre_submit:\n  command_templates:\n    - \"{pipeline.var_templates.compute_script} --genome {sample.genome} --log-file {looper.output_dir}/log.txt\"\n</code></pre> <p>This <code>command_templates</code> section specifies a list with one or more entries. Each entry specifies a command. The commands are themselves templates, just like the primary <code>command_template</code>, so you have access to the looper variable namespaces to put together the appropriate command. In fact, really, the other difference between these <code>pre_submit.command_templates</code> and the primary <code>command_template</code> is that the final one has access to the changes introduce in the variables by the <code>pre_submit</code> commands. The inputs to the script are completely user-defined -- you choose what information and how you want to pass it to your script.</p> <p>Output: The output of your command should be a JSON-formatted string (<code>str</code>), that is processed with json.loads and subprocess.check_output as follows: <code>json.loads(subprocess.check_output(str))</code>. This JSON object will be used to update the looper variable namespaces.</p>"},{"location":"looper/developer-tutorial/pre-submission-hooks/#example-dynamic-compute-parameters","title":"Example: Dynamic compute parameters","text":"<p>In the <code>compute</code> section of the pipeline interface, looper allows you to specify a <code>size_dependent_variables</code> section, which  lets you specify variables with values that are modulated based on the total input file size for the run. This is typically used to add variables for memory, CPU, and clock time to request, if they depend on the input file size. This a good example  of modulating computing variables based on file size, but it is not flexible enough to allow modulated compute variables on the basis of other sample attributes. For a more flexible version, you can use a pre-submission hook.</p> <p>The <code>pre_submit.command_templates</code> specifies a list of Jinja2 templates to construct a system command run in a subprocess. This command template has available all of the namespaces in the primary command template. The command should return a JSON object, which is then used to populate the namespaces. This allows you to specify computing variables that depend on any attributes of a project, sample, or pipeline, which can be used for ultimate flexibility in computing.</p> <p>Usage:</p> <pre><code>var_templates:\n  pipeline_path: \"{looper.piface_dir}/pipelines/pepatac.py\"\n  compute_script: \"{looper.piface_dir}/hooks/script.py\"\npre_submit:\n  command_templates:\n    - \"{pipeline.var_templates.compute_script} --genome {sample.genome} --log-file {looper.output_dir}/log.txt\"\ncommand_template: &gt;\n  {pipeline.var_templates.pipeline_path} ...\n</code></pre> <p>Script example:</p> <pre><code>#!/usr/bin/env python3\n\nimport json\nfrom argparse import ArgumentParser\n\nparser = ArgumentParser(description=\"Test script\")\n\nparser.add_argument(\"-s\", \"--sample-size\", help=\"Sample size\", required=False)\nparser.add_argument(\"-g\", \"--genome\", type=str, help=\"Genome\", required=True)\nparser.add_argument(\"-m\", \"--log-file\", type=str, help=\"Log file path\", required=True)\nparser.add_argument(\"-c\", \"--custom-cores\", type=str, help=\"Force number of cores to use\", required=False)\nargs = parser.parse_args()\n\ny = json.dumps({\n    \"cores\": args.custom_cores or \"4\",\n    \"mem\": \"10000\" if args.genome == \"hg38\" else \"20000\",\n    \"time\": \"00-11:00:00\",\n    \"logfile\": args.log_file\n})\n\nprint(y)\n</code></pre>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/","title":"Writing a pipeline interface","text":""},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#introduction","title":"Introduction","text":"<p>In the User Tutorial we walked you through creating a Looper workspace to run a pipeline on a dataset. That tutorial assumed you already want to run an existing looper-compatible pipeline. For pipelines that are already looper-compatible, you just point your looper configuration file at pipeline's interface file, as described in the User Tutorial.</p> <p>This Developer Tutorial goes into more detail on how to create a looper-compatible pipeline. If you're interested in building a looper-compatible pipeline, or taking an existing pipeline and making it work with looper, the critical point of contact is the pipeline interface. A pipeline interface describes how to run a pipeline.</p> <p>This tutorial will show you how to write a pipeline interface. Once you've been through this, you can consult the formal pipeline interface format specification for further details and reference.</p> <p>Learning objectives</p> <ul> <li>What is a looper pipeline interface?</li> <li>How do I write a looper pipeline interface for my custom pipeline?</li> <li>Do I have to write a pipeline interface to have looper just run some command that isn't really a pipeline?</li> </ul>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#an-example-pipeline-interface","title":"An example pipeline interface","text":"<p>Each Looper project requires one or more pipeline interfaces that points to sample and/or project pipelines. The looper config file points to the pipeline interface, and the pipeline interface tells looper how to run the pipeline.</p> <p></p> <p>Let's revisit the simple pipeline interface the <code>count_lines</code> pipeline example:</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>There are 2 required keys. First, the <code>pipeline_name</code> can be anything, as long as it is unique to the pipeline. It will be used for communication and keeping track of which results belong to which pipeline. Think of it as a unique identifier label you assign to the pipeline.</p> <p>Second, the <code>sample_interface</code> tells looper (and pipestat) that this is a sample-level pipeline. Under <code>sample_interface</code> is the <code>command_template</code>, which holds our command string with variables that will be populated. In this example, looper will run <code>pipeline/count_lines.sh</code>. Looper will pass a single argument to the script, <code>{sample.file_path}</code>, which is a variable that will be populated with the <code>file_path</code> attribute on the sample. This <code>file_path</code> field which corresponds to a column header in the sample table. As the pipeline author, your command template can use any attributes with <code>{sample.&lt;attribute&gt;}</code> syntax. You can make the command template much more complicated and refer to any sample or project attributes, as well as a bunch of other variables made available by looper.</p> <p>That's all you need for a basic pipeline interface! But there's also a lot more you can do, such as providing a schema to specify inputs or outputs, making input-size-dependent compute settings, and more. Let's walk through some of these more advanced options.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#initializing-a-generic-pipeline-interface","title":"Initializing a generic pipeline interface","text":"<p>So you don't have to remember the general syntax, you can run a command that will create a generic pipeline interface, a generic <code>output_schema.yaml</code> (for pipestat-compatible pipelines) and a generic pipeline to get you started:</p> <pre><code>looper init_piface\n</code></pre>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#making-pipeline-interface-portable-with-looperpiface_dir","title":"Making pipeline interface portable with <code>{looper.piface_dir}</code>","text":"<p>One of the problems with the above pipeline interface is that it is hard-coding a relative path to the pipeline, <code>pipeline/count_lines.sh</code>:</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>Looper will literally run <code>pipeline/count_lines.sh</code>, which will only work if the working directory contains the <code>pipeline/</code> subfolder holding <code>count_lines.sh</code>. This may work, since we typically call <code>looper run</code> from our looper workspace folder, where the <code>.looper.yaml</code> config file lives. </p> <p>For example, it would work if you invoke <code>looper run</code> from this working directory:</p> <pre><code>.\n\u251c\u2500\u2500 pipeline\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 count_lines.sh\n\u251c\u2500\u2500 pipeline_interface.yaml\n\u2514\u2500\u2500 .looper.yaml\n</code></pre> <p>But what if we want to run the jobs from a different directory? It would be better if the jobs could run correctly from any working directory.</p> <p>One way we could do that is to hard-code the absolute path to the script. Then, the resulting commands could run from anywhere on that system.</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    /absolute/path/to/pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>But then this pipeline interface this would not be portable. It could not be shared with another user on a different system.</p> <p>Another solution is that we could just put the pipeline command into our shell <code>PATH</code>, so it can be run globally, like this:</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    count_lines {sample.file_path}\n</code></pre> <p>This way, it would run correctly from any folder, and would work on any system. But then users of our pipeline would have to adjust their <code>PATH</code> to make the pipeline globally runnable. This is fine for an installed pipeline intended to be in the <code>PATH</code>, but it is not ideal for basic scripts or other non-global pipelines.</p> <p>Luckily, looper offers a better solution.  It is convenient to specify a path relative to the pipeline interface file, which looper can populate as needed. For this purpose, looper provides the <code>{looper.piface_dir}</code> variable. The pipeline interface author can make any paths portable by simply prepending <code>{looper.piface_dir}</code> to the script in a <code>command_template</code>.</p> <p>Our improved pipeline interface looks like this:</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    {looper.piface_dir}/pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>Looper will automatically populate <code>{looper.piface_dir}</code> on the system with the absolute path, giving us the best of both worlds. The pipeline script can be distributed alongside the pipeline interface, and this will automatically work from any working directory, on any system.</p> <p>Therefore, we recommend pipeline authors always use <code>{looper.piface_dir}</code> to make sure their pipeline interfaces are portable. This is also true for pre-submit command templates.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#validating-sample-input-attributes","title":"Validating sample input attributes","text":"<p>Right now, looper will create and submit jobs for every row in the table. What if we want to prevent jobs from being submitted if the row lacks all the information required by the pipeline? This could help us fail early and save submitting jobs that won't succeed. For example, what if we want to make sure a particular required attribute is set before we submit a job?</p> <p>To demonstrate, let's make a simple modification to the pipeline. We'll adjust it to report the area type.</p> pipeline/count_lines.sh<pre><code>#!/bin/bash\nlinecount=`wc -l $1 | sed -E 's/^[[:space:]]+//' | cut -f1 -d' '`\nexport area_type=$2\necho \"Number of ${area_type}s: $linecount\"\n</code></pre> <p>Then, we'll adjust the pipeline interface to pass the <code>area_type</code> as the second argument to the pipeline.</p> pipeline_interface.yaml<pre><code># stuff before\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path} {sample.area_type}\n</code></pre> <p>Now, let's add a new sample to the sample table that lacks the <code>area_type</code>.</p> metadata/sample_table.csv<pre><code>sample_name,area_type\nmexico,state\nswitzerland,canton\ncanada,province\nusa,\n</code></pre> <p>Right now, if you invoke <code>looper run</code>, you will create a job for all 4 samples. But what we want to do is tell looper that the last sample is incomplete, and should give an error instead of creating and running the job. We will do that through the <code>input_schema</code>.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#create-an-input-schema","title":"Create an input schema","text":"<p>The input schema formally specifies the input processed by this pipeline. It serves several purposes, like sample validation, specifying which attributes should be used to determine sample size, and specifying which attributes are files. We'll get into those details later. First, let's make an input schema. Paste this text into <code>pipeline/input_schema.yaml</code></p> input_schema.yaml<pre><code>description: An input schema for count_lines pipeline pipeline.\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        sample_name: \n          type: string\n          description: \"Name of the sample\"\n          minLength: 1 # set a minimum length required for this required attribute\n        file_path: \n          type: string\n          description: \"Path to the input file to count\"\n          minLength: 1 # set a minimum length required for this required attribute\n        area_type:\n          type: string\n          description: \"Name of the components of the country\"\n          minLength: 1 # set a minimum length required for this required attribute\n      required:\n        - sample_name\n        - file_path\n        - area_type\nrequired:\n  - samples\n</code></pre> <p>This file specifies what inputs the pipeline uses, and what type they are. It is a JSON Schema, which allows us to use this file to validate the inputs, which we'll cover later. For now, it defines that our input samples have 3 properties: <code>sample_name</code>, <code>file_path</code>, and <code>area_type</code>.  We also specify a minimum length required for these inputs with <code>minLength: 1</code>.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#adapt-the-pipeline-interface-to-use-the-input-schema","title":"Adapt the pipeline interface to use the input schema","text":"<p>Next, we need to tell looper to use this as the input schema for the pipeline. Do this by adding the <code>input_schema</code> in the pipeline interface.</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\ninput_schema: input_schema.yaml\nsample_interface:\n  command_template: &gt;\n    {looper.piface_dir}/pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>Now, the input schema is created and linked to the pipeline. If you invoke <code>looper run</code>, looper will read the schema, and first validate the sample before running the job. In this example, the first 3 samples will validate and run as before, but the final sample will not pass the validation stage.</p> <p>To solve the problem, we need to specify the <code>area_type</code> for country <code>usa</code>. Let's add that back in.</p> metadata/sample_table.csv<pre><code>sample_name,area_type\nmexico,state\nswitzerland,canton\ncanada,province\nusa,state\n</code></pre> <p>Now, all 4 samples will validate and <code>looper run</code> will run all jobs.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#validating-that-input-files-exist","title":"Validating that input files exist","text":"<p>Another thing we may want to validate is that a file actually exists. But how does looper know which attributes point to files that must exist? It is not enough to state that the <code>file_path</code> attribute is <code>required</code>; this simply means the sample specifies something for that attribute. It also must actually point to a file that exists. Looper does this through the <code>tangible</code> key.</p> input_schema.yaml<pre><code>description: An input schema for count_lines pipeline pipeline.\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        sample_name: \n          type: string\n          description: \"Name of the sample\"\n        file_path: \n          type: string\n          description: \"Path to the input file to count\"\n      tangible:\n        - file_path\n      sizing:\n        - file_path\n      required:\n        - sample_name\n        - file_path\nrequired:\n  - samples\n</code></pre> <p>By specifying <code>file_path</code> under <code>required</code>, we are telling looper to make sure that attribute is defined on the sample. Then, by adding <code>file_path</code> under <code>tangible</code>, we are telling looper that the file it points to must actually exist.</p> <p>To test, the <code>usa</code> sample should be missing its data file, so it should not run. Try it by running <code>looper run</code>.</p> <p>You will see:</p> <pre><code>...\n## [3 of 4] sample: canada; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_canada.sub\nJob script (n=1; 0.05Gb): /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_canada.sub\nCompute node: zither\nStart time: 2024-09-19 16:27:32\nNumber of lines: 4091500\n## [4 of 4] sample: usa; pipeline: count_lines\n1 input files missing, job input size was not calculated accurately\n&gt; Not submitted: Missing files: data/usa.txt\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_usa.sub\n\nLooper finished\n</code></pre> <p>Looper will not submit this job because it recognizes that there is no file in <code>data/usa.txt</code>. Let's make one. This command will make new data file for usa with 100 million lines in it and a file size of around 1 Gb.</p> <pre><code>shuf -r -n 100000000 data/mexico.txt &gt; data/usa.txt\n</code></pre> <p>Now if you run this it will work.  But this also leads to another thought. The <code>usa.txt</code> text file is way larger than the other files. What if we need to modulate the job parameters by the size of the input file?</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#parameterizing-job-templates-by-sample-through-size-dependent-variables","title":"Parameterizing job templates by sample through size-dependent variables","text":"<p>In the <code>count_lines</code> tutorials, we showed how to parameterize a job submission template via the <code>compute</code> section in the pipeline interface file, like this:</p> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    {looper.piface_dir}/pipeline/count_lines.sh {sample.file_path}\ncompute:\n  partition: standard\n  time: '01-00:00:00'\n  cores: '32'\n  mem: '32000'\n</code></pre> <p>This allows a job submission template to use computing variables like <code>{CORES}</code> and <code>{MEM}</code>, so you can control cluster resources. This provides the same compute parameters for every sample. What if we have a huge file and we want to use different parameters? It's common to need to increase memory usage for a larger sample. Looper provides a simple but powerful way to configure pipelines depending on the input size of the samples.</p> <p>Replace the <code>compute</code> variables with a single variable, <code>size_dependent_variables</code>, which points to a <code>.tsv</code> file:</p> pipeline_interface.yaml<pre><code>compute:\n  partition: standard\n  size_dependent_variables: resources-sample.tsv\n</code></pre> <p>(Here, we leave the <code>partition</code> variable there, since it won't change by sample input size).</p> <p>Then, create <code>resources-sample.tsv</code> with these contents:</p> resources-sample.tsv<pre><code>max_file_size   cores   mem time\n0.05    1   1000    00-01:00:00\n0.5 2   2000    00-03:00:00\nNaN 4   4000    00-05:00:00\n</code></pre> <p>In this example, the <code>partition</code> will remain constant for all samples, but the <code>cores</code>, <code>mem</code>, and <code>time</code> variables will be modulated by sample file size. For each sample, looper will select the first row for which the sample's input file size does not exceed the row's <code>max_file_size</code> value in gigabytes. In this example: </p> <p>Files up to 0.05 Gb (50 Mb) will use 1 core, 1000 mb of RAM, and 1 hour of clock time. Files up to 0.5 Gb (500 Mb) will use 2 cores, 2000 mb of RAM, and 3 hours of clock time. Anything larger will use 4 cores, 4000 mb of RAM, and 5 hours of clock time.</p> <p>You can think of it like a coin sorter machine, going through a priority list of parameter sets with increasing file size allowed, and the sample falls into first bin in which it fits. This allows a pipeline author to specify different computing requirements depending on the size of the input sample. </p> <p>To see this in action, we will need to use a compute template that uses these parameters. The <code>slurm</code> template should do the trick. We'll use <code>-d</code> to specify a dry run, so the jobs are created, but not run.</p> <pre><code>looper run -p slurm -d\n</code></pre> <p>You should see output like this:</p> <pre><code>Activating compute package 'slurm'\n## [1 of 4] sample: mexico; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_mexico.sub\nJob script (n=1; 0.00Gb): /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_mexico.sub\nDry run, not submitted\n## [2 of 4] sample: switzerland; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_switzerland.sub\nJob script (n=1; 0.00Gb): /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_switzerland.sub\nDry run, not submitted\n## [3 of 4] sample: canada; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_canada.sub\nJob script (n=1; 0.00Gb): /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_canada.sub\nDry run, not submitted\n## [4 of 4] sample: usa; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_usa.sub\nJob script (n=1; 0.00Gb): /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_usa.sub\nDry run, not submitted\n</code></pre> <p>But wait! The usa sample file size is listed as 0Gb. Why is it not recognizing the true size of the input file? There is one more thing we need to do.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#configuring-how-looper-determines-sample-size","title":"Configuring how looper determines sample size","text":"<p>You can use the input_schema <code>sizing</code> keyword to tell looper which attributes determine file size. In most cases, <code>sizing</code> should be the same as <code>tangible,</code> because any file that is listed and must exist on disk would probably be the files that you want to use to determine the input file size. But there is a possibility that you might have some required input files that you don't want to use to modulate how you think of the size of the sample. For this reason, we have the ability to define them separately. So, all we have to do is add <code>file_path</code> to the <code>sizing</code> section in the input schema, like this.</p> input_schema.yaml<pre><code>description: An input schema for count_lines pipeline pipeline.\nproperties:\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        sample_name: \n          type: string\n          description: \"Name of the sample\"\n        file_path: \n          type: string\n          description: \"Path to the input file to count\"\n        area_type:\n          type: string\n          description: \"Name of the components of the country\"\n      required:\n        - sample_name\n        - file_path\n        - area_type\n      sizing:\n        - file_path\nrequired:\n  - samples\n</code></pre> <p>Under <code>sizing</code>, it lists <code>file_path</code>, which tells looper that this variable should be used for determining the size of the sample.</p> <p>Now, if we try again, we will see that it works</p> <pre><code>looper run -p slurm -d --skip 3\n</code></pre> <pre><code>## [1 of 4] sample: usa; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_usa.sub\nJob script (n=1; 0.99Gb): /home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_usa.sub\nDry run, not submitted\n</code></pre> <p>Looper is computing this size based on the size of file listed in <code>file_path</code>. Now, look at the job script that was created, which contains the parameters for large files:</p> <pre><code>cat results/submission/count_lines_usa.sub\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name='count_lines_usa'\n#SBATCH --output='/home/nsheff/code/hello_looper/input_schema_example/results/submission/count_lines_usa.log'\n#SBATCH --mem='4000'\n#SBATCH --cpus-per-task='4'\n#SBATCH --time='00-05:00:00'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\npipeline/count_lines.sh data/usa.txt  \n</code></pre> <p>We recommend pipeline authors configure their pipeline interfaces with appropriate <code>size_dependent_variables</code> file, which should be distributed alongside the pipeline interface.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#adding-pre-submission-commands-to-your-pipeline-run","title":"Adding pre-submission commands to your pipeline run","text":"<p>Sometimes we need to run a set-up task before submitting the main pipeline. Looper handles this through a pre-submission hook system.</p> <p>Looper provides several built-in hooks that do things like write out the sample information in different ways, which can be useful for different types of pipeline. If your pipeline would benefit from one of these representations, all you need to do is add a reference to the plugin into the pipeline interface, like this:</p> pipeline_interface.yaml<pre><code>pre_submit:\n  python_functions:\n    - looper.write_sample_yaml\n</code></pre> <p>This will instruct looper to engage this plugin. You can also parameterize the plugin by adding other variables into the pipeline interface. Consult the plugin documentation for more details about how to parameterize the  plugins.</p> <p>You can also write your own custom plugins, which can be as simple as a shell script, or they could be Python functions. Here's an example of a Python script that will be executed through the command-line:</p> pipeline_interface.yaml<pre><code>pre_submit:\n  command_templates:\n    - \"{looper.piface_dir}/hooks/script.py --genome {sample.genome} --log-file {looper.output_dir}/log.txt\"\n</code></pre> <p>Looper will run this command before running each job, allowing you to create little set-up scripts to prepare things before a full job is run.</p>"},{"location":"looper/developer-tutorial/writing-a-pipeline-interface/#project-level-pipeline-interfaces","title":"Project-level pipeline interfaces","text":"<p>Remember, looper distinguishes sample-level from project-level pipelines. This is explained in detail in Advanced run options and in How to run a project-level pipeline. Basically, sample-level pipelines run once per sample, whereas project-level pipelines run once per project. If this interface were describing a project-level pipeline, we would change out <code>sample_interface</code> to <code>project_interface</code>.</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nproject_interface:\n  command_template: '{looper.piface_dir}/count_lines.sh\n</code></pre> <p>You can also write an interface with both a <code>sample_interface</code> and a <code>project_interface</code>. This would make sense to do for a pipeline that had two parts, one that you run independently for each sample, and a second one that aggregates all those sample results at the project level.</p> <p>The differences between using <code>sample_interface</code> vs <code>project_interface</code> are pretty simple, actually. When a user invokes <code>looper run</code>, looper reads data under <code>sample_interface</code> and will create one job template per sample. In contrast, when a user invokes <code>looper runp</code>, looper reads data under <code>project_interface</code> and  creates one job per project (one job total). </p> <p>The only other difference is that the sample-level command template has access to the <code>{sample.&lt;attribute&gt;}</code> namespace, whereas the project-level command template has access to the <code>{samples}</code> array.</p> <p>Summary</p> <ul> <li>You can initialize and modify a generic pipeline interface using <code>looper init_piface</code>.</li> <li>You should make your pipeline interfaces portable by using <code>{looper.piface_dir}</code>.</li> <li>Pipeline-specific and sample-specific compute variables can be added to the pipeline interface to set items such as memory and number of cpus during the pipeline run.</li> <li>You can specify pre-submission hooks for job setup in the pipeline interface.</li> </ul>"},{"location":"looper/how-to/configure-pipestat-backends/","title":"How to configure pipestat back-ends","text":""},{"location":"looper/how-to/configure-pipestat-backends/#introduction","title":"Introduction","text":"<p>In the basic pipestat tutorial, we showed how to create a simple pipeline that could use pipestat to report results. One of the powerful features of pipestat is that you can configure it to store results in different locations. This document will cover more details about how to configure pipestat in looper:</p> <p>Learning objectives</p> <ul> <li>How do I configure pipestat to use different backends with looper?</li> </ul>"},{"location":"looper/how-to/configure-pipestat-backends/#how-looper-configures-pipestat","title":"How looper configures pipestat","text":"<p>As looper processes samples and creates a job submission script, if <code>.looper.yaml</code> config file contains <code>pipestat</code> section, it creates a pipestat configuration file, which merges information found in 1. pipeline interface and 2. looper_config file.</p> <p>Looper combines the necessary configuration data and writes a pipestat configuration for each pipeline interface. The configuration file is then used by the pipeline interface (and pipeline) to create a PipestatManager object. This instantiated PipestatManager object can then be used to report results.</p> <p>To configure looper to use pipestat, you must</p>"},{"location":"looper/how-to/configure-pipestat-backends/#1-add-pipestat-section-to-the-looper-config-file","title":"1. Add <code>pipestat</code> section to the looper config file.","text":"<p>The looper config file must contain a <code>pipestat</code> section. This section must contain information on where to store results, which is either a file path for a results file, database credentials if using a PostgreSQL database backend, a SQLite path if using a SQLite database backend,or PEPhub registry path. Here are 4 examples showing how to configure pipestat to use different back-ends:</p>"},{"location":"looper/how-to/configure-pipestat-backends/#file-back-end","title":"File back-end","text":".looper.yaml<pre><code>pep_config: project_config_pipestat.yaml\noutput_dir: results\npipeline_interfaces:\n  - ./pipeline_interface1_sample_pipestat.yaml\npipestat:\n  flag_file_dir: output/results_pipeline\n  results_file_path: tmp_pipestat_results.yaml\n</code></pre>"},{"location":"looper/how-to/configure-pipestat-backends/#postgres-sql-database-back-end","title":"Postgres SQL database back-end","text":".looper.yaml<pre><code>pep_config: project_config_pipestat.yaml \noutput_dir: results\npipeline_interfaces:\n  - ./pipeline_interface1_sample_pipestat.yaml\npipestat:\n  flag_file_dir: output/results_pipeline\n  database:\n    dialect: postgresql\n    driver: psycopg2\n    name: pipestat-test\n    user: postgres\n    password: pipestat-password\n    host: 127.0.0.1\n    port: 5432\n</code></pre>"},{"location":"looper/how-to/configure-pipestat-backends/#sqlite-database-back-end","title":"SQLite database back-end","text":".looper.yaml<pre><code>pep_config: project_config_pipestat.yaml \noutput_dir: results\npipeline_interfaces:\n  - ./pipeline_interface.yaml\npipestat:\n  flag_file_dir: output/results_pipeline\n  database:\n    sqlite_url: \"sqlite:///yourdatabase.sqlite3\"\n</code></pre>"},{"location":"looper/how-to/configure-pipestat-backends/#pephub-back-end","title":"PEPhub back-end","text":".looper.yaml<pre><code>pep_config: project_config_pipestat.yaml # pephub registry path or local path\noutput_dir: results\npipeline_interfaces:\n  - ./pipeline_interface1_sample_pipestat.yaml\npipestat:\n  flag_file_dir: output/results_pipeline\n  pephub_path: \"databio/pipestat_demo:default\"\n</code></pre>"},{"location":"looper/how-to/configure-pipestat-backends/#2-the-pipeline-interface-must-include-information-required-by-pipestat-such-as-pipeline_name-and-an-output-schema-path","title":"2. The pipeline interface must include information required by pipestat such as pipeline_name and an output schema path:","text":"pipeline_interface.yaml<pre><code>pipeline_name: example_pipestat_pipeline\noutput_schema: pipeline_pipestat/pipestat_output_schema.yaml\nsample_interface:\n  command_template: &gt;\n    python {looper.piface_dir}/count_lines.py {sample.file} {sample.sample_name} {pipestat.results_file}\n</code></pre> <p>Using the above information, Looper will construct the pipestat configuration file automatically. It will have the name <code>pipestat_config_{pipeline_name}.yaml</code> and be placed in the results output directory. It will contain the aggregation of pipestat relevant items found in the looper config file and the pipeline interface.</p> example of pipestat_config_count_lines.yaml<pre><code>results_file_path: /home/drc/GITHUB/hello_looper/hello_looper/pipestat_example/./results/count_lines/results.yaml\nflag_file_dir: /home/drc/GITHUB/hello_looper/hello_looper/pipestat_example/./results/flags\noutput_dir: /home/drc/GITHUB/hello_looper/hello_looper/pipestat_example/./results\nschema_path: /home/drc/GITHUB/hello_looper/hello_looper/pipestat_example/pipeline/pipestat_output_schema.yaml\npipeline_name: count_lines\n</code></pre> <p>Summary</p> <ul> <li>Configure pipestat through the <code>pipestat</code> section on the looper config.</li> <li>You can control where the results of your pipeline are stored by configuring pipestat's back-end.</li> </ul>"},{"location":"looper/how-to/containers/","title":"How to run jobs in containers","text":"<p>Because <code>looper</code> uses <code>divvy</code> for computing configuration, running jobs in containers is easy! <code>Divvy</code> can use the same template system to do either cluster computing or to run jobs in linux containers (for example, using <code>docker</code> or <code>singularity</code>). The way to do this is similar to how you would run jobs on a cluster, but you use templates that run jobs in containers. You can even run jobs in a container on a cluster.</p> <p>Learning objectives</p> <ul> <li>How does looper interact with linux containers?</li> <li>How do I use looper to run a job in a container?</li> </ul>"},{"location":"looper/how-to/containers/#1-get-your-container-image","title":"1. Get your container image.","text":"<p>This could be a docker image (hosted on dockerhub), which you would download via <code>docker pull</code>, or it could be a <code>singularity</code> image you have saved in a local folder. This is pipeline-specific, and you'll need to download the image recommended by the authors of the pipeline or pipelines you want to run.</p>"},{"location":"looper/how-to/containers/#2-specify-the-image-in-your-pipeline_interface","title":"2. Specify the image in your <code>pipeline_interface</code>","text":"<p>The <code>pipeline_interface.yaml</code> file will need a <code>compute</code> section for each pipeline that can be run in a container, specifying the image. For example:</p> pipeline_interface.yaml<pre><code>compute:\n  singularity_image: ${SIMAGES}myimage\n  docker_image: databio/myimage\n</code></pre> <p>For singularity images, you just need to make sure that the images indicated in the <code>pipeline_interface</code> are available in those locations on your system. For docker, make sure you have the docker images pulled.</p>"},{"location":"looper/how-to/containers/#3-add-a-new-compute-package-for-containerized-jobs","title":"3. Add a new compute package for containerized jobs","text":"<p>You will just need to add a new compute package that uses. Here's an example of how to add one for using singularity in a SLURM environment:</p> <pre><code>compute_packages:\n  default:\n    submission_template: divvy_templates/local_template.sub\n    submission_command: sh\n  singularity_slurm:\n    submission_template: templates/slurm_singularity_template.sub\n    submission_command: sbatch\n    singularity_args: -B /sfs/lustre:/sfs/lustre,/nm/t1:/nm/t1\n</code></pre> <p>In <code>singularity_args</code> you'll need to pass any mounts or other settings to be passed to singularity. The actual <code>slurm_singularity_template.sub</code> file looks something like this:</p> slurm_singularity_template.sub<pre><code>#!/bin/bash\n#SBATCH --job-name='{JOBNAME}'\n#SBATCH --output='{LOGFILE}'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='{CORES}'\n#SBATCH --time='{TIME}'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\nsingularity instance.start {SINGULARITY_ARGS} {SINGULARITY_IMAGE} {JOBNAME}_image\nsrun singularity exec instance://{JOBNAME}_image {CODE}\n\nsingularity instance.stop {JOBNAME}_image\n</code></pre> <p>Notice how these values will be used to populate a template that will run the pipeline in a container. Now, submit a job that will use singularity, you activate this compute package with <code>looper run --package singularity_slurm</code>.</p> <p>Here's another example divvy config file that adds a compute package to run jobs in docker on a local computer:</p> divvy_config.yaml<pre><code>compute:\n  default:\n    submission_template: templates/localhost_template.sub\n    submission_command: sh\n  singularity:\n    submission_template: templates/localhost_singularity_template.sub\n    submission_command: sh\n    singularity_args: --bind /ext:/ext\n  docker:\n    submission_template: templates/localhost_docker_template.sub\n    submission_command: sh\n    docker_args: |\n      --user=$(id -u) \\\n      --env=\"DISPLAY\" \\\n      --volume ${HOME}:${HOME} \\\n      --volume=\"/etc/group:/etc/group:ro\" \\\n      --volume=\"/etc/passwd:/etc/passwd:ro\" \\\n      --volume=\"/etc/shadow:/etc/shadow:ro\"  \\\n      --volume=\"/etc/sudoers.d:/etc/sudoers.d:ro\" \\\n      --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n      --workdir=\"`pwd`\" \\\n</code></pre> <p>Notice the <code>--volume</code> arguments, which mount disk volumes from the host into the container. This should work out of the box for most docker users.</p> <p>Looper and bulker</p> <p>Many other pipeline frameworks wrap containers into the pipeline. Looper avoids this approach because it tightly couples the computing environment to the pipeline. Instead, Looper's approach is to level these things separated. Specifying a container in a submission template, as described here, works well for pipelines that use monolithic containers -- that is, there is one giant container, and the whole pipeline runs in that container. An alternative approach is to have a pipeline submit container jobs for each individual task. This approach allows you to use smaller, lightweight containers with only one tool, which makes them more reusable. If you want to use this modular approach, you might be interested in bulker, the computing environment manager in PEPkit. </p> <p>Summary</p> <ul> <li>To looper, containers are just another computing environment setting. You can use the submission template to wrap the command in whatever boilerplate you want, which can be used either for cluster submission or for containers.</li> </ul>"},{"location":"looper/how-to/link-objects/","title":"Link similar results","text":""},{"location":"looper/how-to/link-objects/#create-a-folder-of-linked-similar-objects","title":"Create a folder of linked similar objects","text":"<p>Looper has the ability to create a folder containing symlinks to similar objects (e.g. images). This is helpful if your pipeline reports many objects, and you would like to see all the same type of object in one place.</p> <p>You must configure pipestat to use this feature. </p> <p>Once your pipeline has finished running, you can run the command:</p> <pre><code>looper link\n</code></pre> <p>This command will create symlinks of all objects of the same type and place them into a subfolder within the results folder for convenient browsing. For a how-to guide on reporting objects such as images using pipestat, see report objects.</p>"},{"location":"looper/how-to/project-level-pipelines/","title":"Running project-level pipelines","text":""},{"location":"looper/how-to/project-level-pipelines/#running-project-level-pipelines","title":"Running project-level pipelines","text":"<p>So far, our example has been what we call a sample-level pipeline. The <code>count_lines</code> pipeline from our basic tutorial runs one job per sample, and we want to do the same thing to each sample.</p> <p>This is the most common use case for looper. </p> <p>But sometimes, we need to run a single job on an entire project. Looper calls that a project-level pipeline.</p> <p>Here, we'll walk through a basic example of running a project-level pipeline using looper.</p> <p>A project-level pipeline is meant to run on the entire group of your samples, not each sample individually.</p> <p>First we need to create a project-level pipeline. We would like to take the number of regions from each sample (file) and collect them into a single plot. This how-to guide assumes you've completed the tutorial, using a pep for derived attributes. You can always download the completed tutorial files from the hello looper repository.</p> <p>Ensure you are in the <code>pep_derived_attrs</code> folder from the previous tutorial or after downloading the folder from the hello looper repository.</p> <pre><code>cd pep_derived_attrs\n</code></pre> <p>Next run the sample level pipeline so that the pipeline log files are available:</p> <pre><code>looper run\n</code></pre> <p>You can confirm the existence of log files by checking the <code>results/submission</code> folder:</p> <p><code>ls results/submission/</code></p> <pre><code>count_lines_canada.log  count_lines_mexico.log  count_lines_switzerland.log \ncount_lines_canada.sub  count_lines_mexico.sub  count_lines_switzerland.sub\n</code></pre> <p>If you open one of the <code>.log</code> files, you'll see that each contains the pipeline's results for this sample:</p> <pre><code>Number of lines: 10\n</code></pre> <p>This is because our sample-level pipeline prints the number of lines to the terminal. Looper logs this terminal output to a <code>.log</code> file. While not intended to be used as a results file, we can use the <code>.log</code> file in this case as an example.</p> <p>First, we will need to add a <code>project_interface</code> to the <code>pipeline_interface</code> contained within <code>pep_derived_attrs/pipeline</code>:</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\nproject_interface:\n  command_template: &gt;\n    python3 {looper.piface_dir}/count_lines_plot.py {looper.output_dir}/submission/\n</code></pre> <p>Next, we must create our pipeline. Create a python file in the pipeline folder named <code>count_lines_plot.py</code>:</p> count_lines_plot.py<pre><code>import matplotlib.pyplot as plt\nimport os\nimport sys\n\nresults_dir = sys.argv[1] # Obtain the looper results directory passed via the looper command template\n\n# Extract the previously reported sample-level data from the .log files\ncountries = []\nnumber_of_regions = []\nfor filename in os.listdir(results_dir):\n    if filename.endswith('.log'):\n        file = os.path.join(results_dir, filename)\n        with open(file, 'r') as f:\n            for line in f:\n                if line.startswith('Number of lines:'):\n                    region_count = int(line.split(':')[1].strip())\n                    number_of_regions.append(region_count)\n                    country = filename.split('_')[2].split('.')[0]\n                    countries.append(country)\n\n# Create a bar chart of regions per country\nplt.figure(figsize=(8, 5))\nplt.bar(countries, number_of_regions, color=['blue', 'green', 'purple'])\nplt.xlabel('Countries')\nplt.ylabel('Number of regions')\nplt.title('Number of regions per country')\n\n# Save the image locally\nsave_location =  os.path.join(os.path.dirname(results_dir), \"regions_per_country.png\")\nplt.savefig(save_location, dpi=150)\n</code></pre> <p>Make sure <code>count_lines_plot.py</code> is executable:</p> <pre><code>chmod 755 pipeline/count_lines_plot.py\n</code></pre> <p>If you run the project-level pipeline with the following command, <code>looper runp -c .looper.yaml</code>, the project-level pipeline will run and a <code>regions_per_country.png</code> should appear in your results folder:</p> <p></p> <p>That is how you can run a project-level pipeline via looper!</p> <p>You can find even more details about project-level pipelines in advanced run options.</p>"},{"location":"looper/how-to/report-objects/","title":"Report objects","text":""},{"location":"looper/how-to/report-objects/#reporting-objects-using-pipestat","title":"Reporting objects using pipestat","text":"<p>This how-to guide assumes you have already completed the pipestat tutorial as well as the how-to guide for running project-level pipelines.</p> <p>So far, our examples have been reporting primitives such as integers. But what if our pipeline generates an image, and we wish to report this more complex object? We can leverage looper's integration with pipestat to accomplish this.</p> <p>As an example, we will take the previous pipestat tutorial and modify it to run a project-level pipeline that will create and then report an image result. You can always download the completed tutorial files from the hello looper repository.</p> <p>Ensure you are in the <code>pipestat_example</code> folder from the previous tutorial or after downloading the folder from the hello looper repository.</p> <pre><code>cd pipestat_example\n</code></pre> <p>Next run the sample level pipeline so that the sample-level results are reported to the pipestat backend, <code>results.yaml</code>:</p> <pre><code>looper run\n</code></pre> <p>We would like to take the number of regions from each sample (file) and collect them into a single plot. This plot will be stored as an image locally, and the path of this image will be reported to the pipestat backend.</p> <p>We will need to add a <code>project_interface</code> to the <code>pipeline_interface</code>:</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\noutput_schema: pipestat_output_schema.yaml\nsample_interface:\n  command_template: &gt;\n    python3 {looper.piface_dir}/count_lines.py {sample.file_path} {sample.sample_name} {pipestat.results_file} {pipestat.output_schema}\nproject_interface:\n  command_template: &gt;\n    python3 {looper.piface_dir}/count_lines_plot.py {pipestat.results_file} {pipestat.output_schema}\n</code></pre> <p>We only need to pass the <code>pipestat.results_file</code> and the <code>pipestat.output_schema</code> to pipestat in this project-level example.</p> <p>Next, we will modify the <code>.looper.yaml</code> config file to add a project name to the pipestat section. When running project-level pipelines with pipestat, we must supply a project name.</p> .looper.yaml<pre><code>pep_config: ./metadata/pep_config.yaml # pephub registry path or local path\noutput_dir: ./results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  project_name: count_lines\n  results_file_path: results.yaml\n  flag_file_dir: results/flags\n</code></pre> <p>We will also need to modify the pipestat output schema to report an image result. This is a more complex object than our previous integer results. For example, it has required fields. Let's modify our output schema now:</p> <p>pipestat_output_schema.yaml<pre><code>title: Pipestat output schema for counting lines\ndescription: A pipeline that uses pipestat to report sample level results.\ntype: object\nproperties:\n  pipeline_name: count_lines\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        number_of_lines:\n          type: integer\n          description: \"Number of lines in the input file.\"\n  project:\n    type: object\n    properties:\n      regions_plot:\n        description: \"This a path to the output image\"\n        type: object\n        object_type: image\n        properties:\n          path:\n            type: string\n          thumbnail_path:\n            type: string\n          title:\n            type: string\n        required:\n          - path\n          - thumbnail_path\n          - title\n</code></pre> We have three required fields in this schema, <code>path</code>, <code>thumbnail_path</code>, and <code>title</code>.</p> <p>Finally, we must create our pipeline. Create a python file in the pipeline folder named <code>count_lines_plot.py</code>:</p> count_lines_plot.py<pre><code>import matplotlib.pyplot as plt  # be sure to `pip install matplotlib`\nimport os\nimport pipestat\nimport sys\n\n# A pipeline that retrieves previously reported pipestat results\n# and plots them in a bar chart\nresults_file = sys.argv[1]\nschema_path = sys.argv[2]\n\n# Create pipestat manager\npsm = pipestat.PipestatManager(\n    schema_path=schema_path,\n    results_file_path=results_file,\n    pipeline_type=\"project\"\n)\n\n# Extract the previously reported data\nresults = psm.select_records() # pipestat object holds the data after reading the results file\ncountries = [record['record_identifier'] for record in results['records']]\nnumber_of_regions = [record['number_of_lines'] for record in results['records']]\n\n# Create a bar chart of regions per country\nplt.figure(figsize=(8, 5))\nplt.bar(countries, number_of_regions, color=['blue', 'green', 'purple'])\nplt.xlabel('Countries')\nplt.ylabel('Number of regions')\nplt.title('Number of regions per country')\n\n# Save the image locally AND report that location via pipestat\n# we can place it next to the results file for now\nsave_location =  os.path.join(os.path.dirname(results_file), \"regions_per_country.png\")\n\nplt.savefig(save_location, dpi=150)\n\nresult_to_report = {\"regions_plot\":\n                        {\"path\": save_location,\n                         \"thumbnail_path\": save_location,\n                         \"title\": \"Regions Plot\"}}\n\npsm.report(record_identifier=\"count_lines\", values=result_to_report)\n</code></pre> <p>Looper passes the schema and results file to the pipeline. Pipestat then uses these items to create a PipestatManager object (<code>psm</code>) which can access the previously reported results.</p> <p>Once we've plotted the results in a bar chart, we can report the location of the saved plot using pipestat. The structure of the reported image, <code>regions_plot</code>, is a nested dictionary and contains all the required fields from our modified pipestat output schema.</p> <p>Make sure <code>count_lines_plot.py</code> is executable:</p> <pre><code>chmod 755 pipeline/count_lines_plot.py\n</code></pre> <p>If you run the project-level pipeline with the following command, <code>looper runp -c .looper.yaml</code>, two things will happen, a <code>regions_per_country.png</code> should appear in your results folder:</p> <p></p> <p>And the path to this image will be added to the <code>results.yaml</code> file as a reported result:</p> results.yaml<pre><code>count_lines:\n  project:\n    count_lines:\n      meta:\n        pipestat_modified_time: '2024-09-19 15:36:38'\n        pipestat_created_time: '2024-09-19 15:36:38'\n      regions_plot:\n        path: /home/drc/GITHUB/hello_looper/hello_looper/pipestat_example/./results/count_lines/regions_per_country.png\n        thumbnail_path: /home/drc/GITHUB/hello_looper/hello_looper/pipestat_example/./results/count_lines/regions_per_country.png\n        title: Regions Plot\n  sample:\n    mexico:\n      meta:\n        pipestat_modified_time: '2024-09-19 15:36:33'\n        pipestat_created_time: '2024-09-19 15:36:33'\n      number_of_lines: 31\n    switzerland:\n      meta:\n        pipestat_modified_time: '2024-09-19 15:36:33'\n        pipestat_created_time: '2024-09-19 15:36:33'\n      number_of_lines: 23\n    canada:\n      meta:\n        pipestat_modified_time: '2024-09-19 15:36:34'\n        pipestat_created_time: '2024-09-19 15:36:34'\n      number_of_lines: 10\n</code></pre> <p>That is how you can run a project-level pipeline via looper and report an object result via pipestat!</p>"},{"location":"looper/how-to/rerun/","title":"How to rerun samples","text":""},{"location":"looper/how-to/rerun/#introduction","title":"Introduction","text":"<p>It is recommended that you have read and completed the basic pipestat tutorial as well as configuring pipestat backends. </p> <p>This document will briefly cover details about how to configure pipestat in looper:</p> <p>Learning objectives</p> <ul> <li>How do I rerun samples?</li> </ul> <p>There may be a time when you need to re-submit samples. Looper's <code>rerun</code> command can be used to achieve this. By default, the <code>rerun</code> command will gather any samples with a <code>failed</code> status and submit again.</p> <p>However, Looper does not create flags for samples. It only reads them. Creating flags is the job of the pipeline via either the pipeline manager (e.g. pypiper) or pipestat.</p> <p>Therefore, to create submission flags, you must:</p> <ol> <li>Configure looper to use pipestat</li> <li>Add the <code>flag_file_dir</code> key and a flag directory path under the pipestat key within the looper config: .looper.yaml<pre><code>pep_config: project_config_pipestat.yaml\noutput_dir: results\npipeline_interfaces:\n  - ./pipeline_interface1_sample_pipestat.yaml\npipestat:\n  flag_file_dir: output/results_pipeline\n  results_file_path: tmp_pipestat_results.yaml\n</code></pre></li> <li>Ensure your pipeline is using pipestat to set the status upon sample processing.</li> </ol> <p><pre><code>currentPipestatManager.set_status(record_identifier=\"sample1\", status_identifier=\"running\")\ncurrentPipestatManager.set_status(record_identifier=\"sample2\", status_identifier=\"failed\")\n</code></pre> More info on setting status can be found here.</p> <p>Now that your pipeline is setting statuses, you can check the status of all samples using: <pre><code>looper check\n</code></pre> This command will show you a list of samples and their associated status,e.g:</p> <pre><code>     'count_lines' pipeline status summary     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503    Status     \u2503    Jobs count/total jobs    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502   completed   \u2502             1/3             \u2502\n\u2502    failed     \u2502             2/3             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can resubmit all failed jobs with: <pre><code>looper rerun\n</code></pre></p> <p>By default, this command will only submit failed jobs. However, you can use the <code>--ignore-flags</code> flag to resubmit all jobs. <pre><code>looper rerun --ignore-flags\n</code></pre></p>"},{"location":"looper/user-tutorial/compute-settings/","title":"Configuring cluster computing","text":""},{"location":"looper/user-tutorial/compute-settings/#configuring-compute-settings","title":"Configuring compute settings","text":""},{"location":"looper/user-tutorial/compute-settings/#introduction","title":"Introduction","text":"<p>So far, we\u2019ve been running jobs with Looper in the simplest way -- sequentially on the local computer. While this approach is easy and straightforward, Looper truly shines when you need to scale up and submit jobs to a compute cluster.</p> <p>To facilitate this, Looper integrates a tool called <code>divvy</code>, which handles job submission configuration. <code>Divvy</code> is automatically installed alongside Looper but can also be used independently. By leveraging Divvy, Looper provides a powerful nested template system that simplifies running jobs on any cluster resource manager (e.g., SLURM, SGE, LFS) using templates.  Switching between different computing environments is also seamless. The best part is that this setup works for any Looper-compatible pipeline. Once you\u2019ve configured your computing environment to your liking, Looper will help you deploy any pipeline in the same way.</p> <p>In this tutorial, we\u2019ll show you how to configure Looper and Divvy, giving you full control over your computing resources. You'll learn how the templates work, and how you can customize them to fit just about any computing scenario. To demonstrate, we'll show you how to configure looper to submit jobs to a SLURM cluster. If you have a different system, you can edit templates and use a similar approach for any cluster resource manager.</p> <p>Learning objectives</p> <ul> <li>How does looper actually run jobs?</li> <li>How can submit my jobs to a cluster instead of running them locally?</li> <li>What is looper's nested template system and how can I use it to have total control over my compute settings?</li> <li>Can I submit my pipeline to different types of clusters or in different computing environments?</li> <li>How do I specify required resources like time or number of cores for a cluster job?</li> </ul>"},{"location":"looper/user-tutorial/compute-settings/#how-looper-job-submission-works","title":"How looper job submission works","text":"<p>To start, let's go deeper into the details of how jobs are run in the simple case. We'll stay in the <code>pep_derived_attrs</code> workspace created in the previous tutorial. If you need to, you can download the folder from the hello looper repository.</p> <p>Two arguments you can pass to <code>looper run</code> for testing are these:</p> <ul> <li><code>--dry-run</code>/<code>-d</code>. In dry-run mode, looper will create job scripts, but will not actually run them.</li> <li><code>--limit</code>/<code>-l</code>. This will limit the number of samples looper submits.</li> </ul> <p>To demonstrate, run looper using these arguments:</p> <pre><code>cd pep_derived_attrs\nlooper run -d -l 1\n</code></pre> <p>If you run this, you should see output similar to this:</p> <pre><code>Command: run\nUsing looper config (.looper.yaml).\n## [1 of 3] sample: mexico; pipeline: count_lines\nWriting script to /home/nsheff/code/hello_looper/pep_derived_attrs/results/submission/count_lines_mexico.sub\nJob script (n=1; 0.00Gb): /home/nsheff/code/hello_looper/pep_derived_attrs/results/submission/count_lines_mexico.sub\nDry run, not submitted\n</code></pre> <p>Notice that:</p> <ul> <li>only 1 sample was processed (because we used <code>-l 1</code>)</li> <li>A job script was created, but not submitted (because we used <code>-d</code>)</li> </ul> <p>This brings up a key point about how looper works. It does not really run commands directly. Instead, it creates a job script, and then executes that script. This is true even in the simplest case, with the default compute settings. Let's look more closely at the script looper created:</p> <pre><code>cat results/submission/count_lines_mexico.sub \n</code></pre> results/submission/count_lines_mexico.sub<pre><code>#!/bin/bash\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{\npipeline/count_lines.sh data/mexico.txt \n} | tee /home/nsheff/code/hello_looper/pep_derived_attrs/results/submission/count_lines_mexico.log\n</code></pre> <p>Let's see how looper is creating this file. You may recognize the highlighted line, because we've seen it before in the earlier tutoirials. It's the command we specified for how to run the pipeline, and it's coming from the pipeline interface file.</p> <p>Recall in the first tutorial we created a <code>pipeline/pipeline_interface.yaml</code>, with this content:</p> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\n</code></pre> <p>The <code>command_template</code>, after populating the variable for the first sample, becomes <code>pipeline/count_lines.sh data/mexico.txt</code> -- the line highlighted in the submission script.</p> <p>But where is the rest of the submission script coming from? It's not hard coded looper boilerplate -- this is actually coming from a different template that can be configured with divvy. It's called the submission template, and what we're seeing here is the default submission template divvy uses when nothing else is specified.</p> <p>Here's the default submission template provided by divvy:</p> localhost_template.sub<pre><code>#!/bin/bash\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{\n{CODE} \n} | tee {LOGFILE}\n</code></pre> <p>This template has 2 variables, which are populated by looper. The <code>{CODE}</code> slot is the one being populated from the pipeline interface's <code>command_template</code> variable. The <code>{LOGFILE}</code> variable is a special variable provided by looper, which we'll get into later, but you can ignore for now.</p> <p>So, these are the steps looper takes to create the final job submission script:</p> <ol> <li>It uses the <code>command_template</code> from the pipeline interface, populating it with any sample-specific information specified (like <code>{sample.file_path}</code>)</li> <li>It takes the resulting value and uses it to populate the <code>{CODE}</code> variable in the <code>submission_template</code>.</li> <li>It writes the final template to a file, named after the pipeline and sample.</li> </ol> <p>Key point</p> <ul> <li>Looper isn't running anything directly. Rather, it's creating a script (called a submission script), and then it executes that.</li> <li>If you use dry run mode, then it just creates the script without executing it.</li> <li>The submission script is created in two steps, with two templates. First, the command is constructed, using the pipeline interface. Then, this command is inserted into a submission template provided by divvy. This two-layered template system is what we call looper's nested template system, and it provides a lot of  powerful benefits.</li> </ul> <p>Next, we'll learn how to change the submission template, so that the job gets submitted to a cluster instead of run locally.</p>"},{"location":"looper/user-tutorial/compute-settings/#configuring-looper-for-cluster-submission","title":"Configuring looper for cluster submission","text":""},{"location":"looper/user-tutorial/compute-settings/#initializing-divvy-configuration","title":"Initializing divvy configuration","text":"<p>To configure <code>divvy</code> (and therefore <code>looper</code>) for cluster computing, first create a <code>divvy</code> computing configuration file using <code>divvy init</code>. Looper looks for the divvy configuration file in the environment variable <code>$DIVCFG</code>, so it's best to set that up to point to your divvy configuration file, so you don't have to provide it every time you run <code>looper</code>. This <code>init</code> command will create a default config file, along with a folder of computing templates:</p> <pre><code>export DIVCFG=\"divvy_config.yaml\"\ndivvy init --config $DIVCFG\n</code></pre> <p>Looper will now have access to your computing configuration. Add the <code>export...</code> line to your <code>.bashrc</code> or <code>.profile</code> to ensure the <code>DIVCFG</code> variable persists for future command-line sessions.  You can override the <code>DIVCFG</code> environment variable by specifying a path with <code>--divvy</code> argument, like this:</p> <pre><code>looper run\n  --divvy /path/to/divvy_cfg.yaml ...\n</code></pre> <p>On a fresh init, <code>divvy</code> comes pre-loaded with some built-in compute packages, which you can explore by typing <code>divvy list</code>. This will display the available compute packages:</p> <pre><code>divvy list\n</code></pre> <pre><code>Divvy config: divvy_config.yaml\n\ndocker\ndefault\nsingularity_slurm\nsingularity\nlocal\nslurm\n</code></pre> <p>For example, there's a package called 'slurm'. The template for this package looks a little different from the one we saw above:</p> slurm_template.sub<pre><code>#!/bin/bash\n#SBATCH --job-name='{JOBNAME}'\n#SBATCH --output='{LOGFILE}'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='{CORES}'\n#SBATCH --time='{TIME}'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\n{CODE}\n</code></pre> <p>This template includes directives that are understood by the SLURM scheduler (that's what the <code>#SBATCH lines are</code>). So the resulting submission file is going to be a SLURM script, rather than simply a shell script.</p> <p>So let's re-run looper but as we did before, but this time using this new package. You select a package in a <code>looper run</code> command with the <code>--package</code> argument:</p> <pre><code>looper run -d -l 1 \\\n  --package slurm\n</code></pre> <p>Now when we view the resulting submission script, we get a very different result:</p> results/submission/count_lines_mexico.sub<pre><code>#!/bin/bash\n#SBATCH --job-name='count_lines_mexico'\n#SBATCH --output='/home/nsheff/code/hello_looper/pep_derived_attrs/results/submission/count_lines_mexico.log'\n#SBATCH --mem='{MEM}'\n#SBATCH --cpus-per-task='{CORES}'\n#SBATCH --time='{TIME}'\n#SBATCH --partition='{PARTITION}'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\npipeline/count_lines.sh data/mexico.txt \n</code></pre> <p>Of course, the template has changed to the slurm template. The <code>{CODE}</code> variable was correctly populated, same as before, from the pipeline interface's <code>command_template</code> field. But the other variables, like <code>{MEM}</code> and <code>{TIME}</code>, have not been populated. Why not? Because we haven't provided looper any values for these variables. So, how do we tell looper what to use?</p>"},{"location":"looper/user-tutorial/compute-settings/#parameterizing-job-templates-through-the-command-line","title":"Parameterizing job templates through the command-line","text":"<p>The simplest way is that we can provide them on the command line using <code>--compute</code>, e.g.:</p> <pre><code>looper run -d -l 1 \\\n  --package slurm  \\\n  --compute partition=standard time='01-00:00:00' cores='32' mem='32000'\n</code></pre> <p>This command will populate the variables as you expect:</p> results/submission/count_lines_mexico.sub<pre><code>#!/bin/bash\n#SBATCH --job-name='count_lines_mexico'\n#SBATCH --output='/home/nsheff/code/hello_looper/pep_derived_attrs/results/submission/count_lines_mexico.log'\n#SBATCH --mem='32000'\n#SBATCH --cpus-per-task='32'\n#SBATCH --time='01-00:00:00'\n#SBATCH --partition='standard'\n#SBATCH -m block\n#SBATCH --ntasks=1\n\necho 'Compute node:' `hostname`\necho 'Start time:' `date +'%Y-%m-%d %T'`\n\npipeline/count_lines.sh data/mexico.txt \n</code></pre>"},{"location":"looper/user-tutorial/compute-settings/#parameterizing-job-templates-through-the-looper-config","title":"Parameterizing job templates through the looper config","text":"<p>It can be annoying to provide compute parameters every time you want to run your jobs.  It's convenient to store those settings somewhere. Luckily, looper provides a solution to this exact problem! You can specify any command-line arguments through the looper config by adding a <code>cli</code> section. For example, if we want to add these compute settings, we could do it like this:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\ncli:\n  run:\n    compute: \n      partition: standards\n      time: '01-00:00:00'\n      cores: '32'\n      mem: '32000'\n</code></pre> <p>This works for other arguments as well. For example, if we want <code>looper run</code> to default to only running a single sample and using the <code>slurm</code> package, we could add these lines to our looper config:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\ncli:\n  run:\n    limit: 1\n    package: slurm\n    compute: \n      partition: standards\n      time: '01-00:00:00'\n      cores: '32'\n      mem: '32000'\n</code></pre> <p>Now, we could override these by passing <code>-p</code> or <code>-l</code> on the command line. Putting default arguments into the looper config <code>cli</code> section allows you to avoid having to type your common settings every time. It also makes it easier to parameterize the pipeline differently for different projects.</p>"},{"location":"looper/user-tutorial/compute-settings/#parameterizing-job-templates-through-the-pipeline-interface","title":"Parameterizing job templates through the pipeline interface","text":"<p>Sometimes, you want the pipeline to be parameterized globally, so you don't have to worry about re-parameterizing it for different projects. You can also provide compute settings in the pipeline interface. Let's add a <code>compute</code> section in the pipeline interface file, like this:</p> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\ncompute:\n  partition: standard\n  time: '01-00:00:00'\n  cores: '32'\n  mem: '32000'\n</code></pre> <p>Now, with this new pipeline interface, you can leave off the <code>--compute</code> argument to <code>looper run</code>, as we did before, but the variables will be populated from the pipeline interface:</p> <pre><code>looper run -d -l 1 \\  # this works!\n  --package slurm  \\\n</code></pre> <p>Parameterizing through the pipeline interface is useful if the parameters are going to be the same for this pipeline, regardless of the project. But sometimes, you need to set the parameters separately for each project...</p> <p>Warning</p> <p>One problem with all of these methods is that they just provide the same compute parameters for every sample. What if your samples are widely different in input file size, and therefore require different parameters? If you need to change the parameters by sample, you can solve the problem through pipeline interface size-dependent variables. In many cases, the samples are roughly similar, so a single parameterization for the whole project will suffice.</p> <p>So, there are several ways to provide compute parameters. Why do we need all these different places to provide compute parameters?</p> <p>Because different people/roles will be editing different things.</p> <ul> <li>the pipeline author is not necessarily the same as the person running the pipeline.</li> <li>the pipeline author may want to provide some basic compute guidance, which can do so in the pipeline interface. </li> <li>the person running might way to configure something for the workspace as a whole. These go in the <code>.looper.yaml</code> file.</li> <li>but for a one-off run, that's different from normal, you can override with the <code>--compute</code> command-line argument.</li> </ul>"},{"location":"looper/user-tutorial/compute-settings/#the-submission-command","title":"The submission command","text":"<p>There's one final important point about how looper job submission happens. To looper, different compute packages are really not that different; if we pick the <code>slurm</code> package, mostly, it just means we're using a different template. But there is one other very important difference: for the local package, looper needs to submit the job using <code>sh</code>, whereas for the slurm package, looper submits the job with <code>sbatch</code> -- the command used to assign a task to the SLURM scheduler. This submission command is actually part of the compute package, and something you can specify if you create your own package. For these built-in packages, the submission commands are already appropriately populated.</p>"},{"location":"looper/user-tutorial/compute-settings/#custom-compute-packages","title":"Custom compute packages","text":"<p>For many use cases, the built-in packages will be sufficient. But a great thing about looper is that these compute packages are totally customizable. You can edit the templates, and even create your own custom compute packages. This allows you to tailor divvy/looper to any compute environment. You can find out more in the documentation on custom compute packages</p> <p>Summary</p> <ul> <li>Looper runs jobs by creating job scripts and then running them.</li> <li>You can control the job script by choosing a different submission package</li> <li>You can view the list of available compute packages with <code>divvy list</code> and choose one with <code>looper run --package</code> or <code>-p</code>.</li> <li>You pass parameters to the submission template in several ways, such as the pipeline interface, the looper config, or on the command line with <code>--compute</code>.</li> </ul>"},{"location":"looper/user-tutorial/conclusion/","title":"Tutorial conclusion","text":""},{"location":"looper/user-tutorial/conclusion/#recap","title":"Recap","text":"<p>We've come to the end of the basic tutorial. Starting from a basic looper workspace, we walked through how to add successively more powerful features. You learned how to use PEP for metadata management beyond a simple CSV. You learned how to use pipestat to help you with powerful result reports and job monitoring. You learned how to use divvy to submit your jobs to use different compute resources.</p> <p>One key takeaway from this should be how modular everything is. Each of the more powerful features is an option.  You don't have to use any of them if you don't want to. You can also use all of them simultaneously if you need it. You can also use each of these tools independent of looper in a completely different system.</p> <p>That modularity is the core design philosophy of PEPkit.</p> <p>What we've covered so far is still just the beginning of what you can use these tools to do.  In the advanced user guides, you'll find details on how to configure run options, more advanced computing guides, and more useful metadata management. There are also how-to guides with specific recipes for common use cases. So at this point, it's probably best to just dive in and then consult the more advanced documentation when you find yourself wanting to do something specific.</p> <p>Good luck!</p>"},{"location":"looper/user-tutorial/initialize/","title":"Setting up a looper project","text":""},{"location":"looper/user-tutorial/initialize/#introduction","title":"Introduction","text":"<p>This guide will walk you through creating a Looper workspace to run a pipeline on a dataset. Looper is ideal for processing data split into independent samples, allowing you to run the same pipeline in parallel on each sample. In our demonstration, we'll work with three sample files, each representing a different country: Canada, Mexico, and Switzerland. Each file contains a list of the states or provinces in that country. In this context, each country is considered a \"sample,\" and our goal is to run a pipeline on each sample. The sample pipeline we\u2019ll use is a shell script that counts the number of lines in its input file. By running this pipeline, we\u2019ll compute the number of states or provinces in each country.</p> <p>After setting things up, it will be easy to launch all our jobs like this:</p> <pre><code>cd looper_csv_example\nlooper run\n</code></pre> <p>In a real workspace, you would likely have hundreds or thousands of samples, each with multiple input files. Your pipeline could be an advanced Python or R script that takes hours or days to run. You might be running the jobs on a compute cluster. The principles of setting up and running looper will be the same as for our simple demonstration project.</p> <p>This User Tutorial does not cover how to actually create a looper-compatible pipeline. If you want to create a new pipeline, that's the goal of the Pipeline Developer Tutorial. Here, we assume you already have a looper-compatible pipeline and and you're trying to run it on some data.</p> <p>Let's get started!</p> <p>Learning objectives</p> <ul> <li>How do I get started using looper?</li> <li>What components do I need to run looper?</li> <li>What is a looper config file, a pipeline interface, and a metadata table?</li> </ul>"},{"location":"looper/user-tutorial/initialize/#components-of-a-looper-workspace","title":"Components of a looper workspace","text":"<p>A typical Looper workspace includes the following components:</p> <ol> <li> <p>Sample data: Usually stored as one or more files per sample.</p> </li> <li> <p>Sample metadata: A <code>.csv</code> file that describes the samples. Looper will run your pipeline once for each row in this table. In a future guide, we\u2019ll explore how to leverage more advanced features of <code>PEP</code> to describe your sample metadata.</p> </li> <li> <p>Pipeline: The script or command you want to run on each sample file. You'll also need a pipeline interface file that tells looper how to execute the pipeline.</p> </li> <li> <p>Looper configuration file: This file points to the other components and includes any additional configuration options you want to pass to looper.</p> </li> </ol> <p>We'll walk through how to create each of these components. If you'd rather skip creating the files yourself, you can download the completed project from the hello world git repository, which has the files as you'd end with in this tutorial. But we recommend following these instructions the first time because they will help you get a feel for the contents of the files.</p>"},{"location":"looper/user-tutorial/initialize/#step-1-set-up-a-directory","title":"Step 1: Set up a directory","text":"<p>Usually, a looper workspace corresponds to a directory. So the first step is to create a new folder for this tutorial. We'll run the commands from within the folder:</p> <pre><code>mkdir looper_csv_example\ncd looper_csv_example\n</code></pre> <p>In practice, looper can handle any file paths for any of the components, but to keep things simple in this tutorial, we'll keep all the components in this workspace folder, in subfolders for data, metadata, and pipeline.</p>"},{"location":"looper/user-tutorial/initialize/#step-2-grab-the-data","title":"Step 2: Grab the data","text":"<p>First, we'll need some data. Download these 3 files from looper csv example and save them in a subfolder called <code>data/</code>:</p> <pre><code>mkdir data\nwget -P data https://raw.githubusercontent.com/pepkit/hello_looper/refs/heads/master/looper_csv_example/data/mexico.txt  \nwget -P data https://raw.githubusercontent.com/pepkit/hello_looper/refs/heads/master/looper_csv_example/data/canada.txt\nwget -P data https://raw.githubusercontent.com/pepkit/hello_looper/refs/heads/master/looper_csv_example/data/switzerland.txt\n</code></pre> <p>Take a peak at the contents of the files. Each lists the states or provinces of the country, with one per line, like this:</p> <pre><code>head data/mexico.txt -n 5\n\nAguascalientes\nBaja California\nBaja California Sur\nCampeche\nChiapas\n</code></pre>"},{"location":"looper/user-tutorial/initialize/#step-3-create-a-metadata-table","title":"Step 3: Create a metadata table","text":"<p>Looper needs a list of samples in the form of a sample metadata table, which names each sample and includes paths to the data files. Looper will accept a PEP, which we'll discuss more later, or just a simple CSV file, which is where we'll start. Create a new file called <code>metadata/sample_table.csv</code> with this content:</p> ViewCreate metadata/sample_table.csv<pre><code>sample_name,area_type,file_path\nmexico,state,data/mexico.txt\nswitzerland,canton,data/switzerland.txt\ncanada,province,data/canada.txt\n</code></pre> <pre><code>mkdir -p metadata &amp;&amp; cat &gt; metadata/sample_table.csv &lt;&lt; 'EOF'\nsample_name,area_type,file_path\nmexico,state,data/mexico.txt\nswitzerland,canton,data/switzerland.txt\ncanada,province,data/canada.txt\nEOF\n</code></pre> <p>Each row corresponds to a sample, with a unique identifier under <code>sample_name</code>, a pointer to its corresponding file in <code>file_path</code>, and any other information you want to include about the sample (in this case, <code>area_type</code>). These will be the different values available to pass to your pipeline.</p>"},{"location":"looper/user-tutorial/initialize/#step-4-create-the-pipeline","title":"Step 4: Create the pipeline","text":"<p>Our example pipeline is a shell script that counts the lines in an input file. Since our data has one line per province, this script will tell us how many provinces there are in each country. Create a file at <code>pipeline/count_lines.sh</code> with this content:</p> ViewCreate pipeline/count_lines.sh<pre><code>#!/bin/bash\nlinecount=`wc -l $1 | sed -E 's/^[[:space:]]+//' | cut -f1 -d' '`\necho \"Number of lines: $linecount\"\n</code></pre> <pre><code>mkdir -p pipeline &amp;&amp; cat &gt; pipeline/count_lines.sh &lt;&lt; 'EOF'\n#!/bin/bash\nlinecount=`wc -l $1 | sed -E 's/^[[:space:]]+//' | cut -f1 -d' '`\necho \"Number of lines: $linecount\"\nEOF\nchmod 755 pipeline/count_lines.sh\n</code></pre> <p>All this script does is run the unix <code>wc</code> command, and then parse the output using <code>sed</code> and <code>cut</code>, and then print the result with <code>echo</code>. In a real workspace, your pipeline is more likely to be a powerful Python script or something else. The important thing for looper is just that there's a command you can run to execute the pipeline, and you can pass arguments on the command line.</p>"},{"location":"looper/user-tutorial/initialize/#step-5-create-a-pipeline-interface","title":"Step 5: Create a pipeline interface","text":"<p>In order to run a pipeline, looper needs to know how to construct the command, which will be different for each sample. A pipeline developer does this through the pipeline interface. Our <code>count_lines.sh</code> pipeline just takes a single argument, which is the file path, so the command would be <code>count_lines.sh path/to/file.txt</code>. Here's how to create the appropriate pipeline interface for this pipeline. Create a file at <code>pipeline/pipeline_interface.yaml</code> with this content:</p> ViewCreate pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\n</code></pre> <pre><code>cat &gt; pipeline/pipeline_interface.yaml &lt;&lt; 'EOF'\npipeline_name: count_lines\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path}\nEOF\n</code></pre> <p>This is the pipeline interface file. It specifies the <code>pipeline_name</code>, which looper uses to keep track of pipelines in case it is running multiple of them. It also specifies the <code>command_template</code>, which uses <code>{sample.file_path}</code> to tell looper to use the value of the <code>file_path</code> attribute on samples. Later we'll cover more powerful features you can add to the pipeline interface. For now, the important part is just that the <code>command_template</code> is telling looper how to run the <code>count_lines.sh</code> script we just created, passing it a single argument as a parameter, which it will get from the <code>file_path</code> column from the sample metadata table.</p>"},{"location":"looper/user-tutorial/initialize/#step-6-create-the-looper-config","title":"Step 6: Create the looper config","text":"<p>Now you have all the components finished for your looper project. The last step is to create the looper configuration file. The easiest way to do this is to run the looper init wizard from within your workspace folder. This wizard will walk you through building the file.</p> <pre><code>looper init  # Run looper initialization wizard\n</code></pre> <p>Here's how to answer the questions</p> Prompt You should respond: Project name: (new_csv) (leave blank to accept the default) Registry path or file path to PEP: (databio/example) metadata/sample_table.csv Path to output directory: (results) (leave blank to accept the default) Add each path to a pipeline interface: (pipeline_interface.yaml) pipeline/pipeline_interface.yaml <p>This will create a file at <code>.looper.yaml</code>, with this content:</p> ViewCreate .looper.yaml<pre><code>pep_config: metadata/sample_table.csv\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\n</code></pre> <pre><code>cat &gt; .looper.yaml &lt;&lt; 'EOF'\npep_config: metadata/sample_table.csv\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\nEOF\n</code></pre> <p>Right now, this file is basically just 3 pointers. The first line points to the metadata table we just created. The second line points to the <code>output_dir</code> where looper will store results. Finally, the <code>pipeline_interfaces</code> is a list that includes the path to the pipeline interface we just created. Future tutorials will cover how to use this to run multiple pipelines on each sample, but most of the time you'll just have a single entry here. You could also create this <code>.looper.yaml</code> file manually, but the <code>init</code> function makes it so you don't have to remember the config syntax. Now that the project is configured, you can run it like this:</p> <pre><code>looper run\n</code></pre> <p>This will submit a job for each sample. You've just run your first looper project! </p> <p>Basically, all looper does is load you metadata table, and then for each row, populate the <code>command_template</code> for the sample, and then run the command. That is the gist of it. After this, there's just more options and tweaks you can do to control your jobs. For example, here are some looper arguments:</p> <ul> <li>Dry runs. You can use <code>-d, --dry-run</code> to create the job submission scripts, but not actually run them. This is really useful for testing that everything is set up correctly before you commit to submitting hundreds of jobs.</li> <li>Limiting the number of jobs. You can <code>-l, --limit</code> to test a few before running all samples. You can also use the <code>--selector-*</code> arguments to select certain samples to include or exclude.</li> <li>Grouping jobs. You can use <code>-u, --lump</code>, <code>-n, --lumpn</code>, <code>-j, --lumpj</code> to group jobs. More details on grouping jobs.</li> <li>Changing compute settings. You can use <code>-p, --package</code>, <code>-s, --settings</code>, or <code>--compute</code> to change the compute templates. Read more in advanced computing.</li> <li>Time delay. You can stagger submissions to not overload a submission engine using <code>--time-delay</code>.</li> <li>Use rerun to resubmit jobs. To run only jobs that previously failed, try <code>looper rerun</code>.</li> <li>Tweak the command on-the-fly. The <code>--command-extra</code> arguments allow you to pass extra arguments to every command straight through from looper. See advanced run options.</li> </ul> <p>In the upcoming tutorials, we'll cover these and other features in more detail.</p> <p>Summary</p> <ul> <li>To use looper, you'll create a looper configuration file which points to: 1. data, 2. metadata, and 3. a pipeline with pipeline interface.</li> <li>Looper is best suited for data that is split into samples or jobs, where you're trying to run a pipeline independently on each sample.</li> <li>Once you've configured looper, you run jobs with the command <code>looper run</code>.</li> </ul>"},{"location":"looper/user-tutorial/metadata/","title":"Configuring your metadata","text":""},{"location":"looper/user-tutorial/metadata/#introduction","title":"Introduction","text":"<p>Previously, we used a <code>.csv</code> file to store metadata. This tutorial will guide you through how to use powerful features of PEP and PEPhub to make managing your metadata easier.</p> <p>Learning objectives</p> <ul> <li>How can I use PEPs as input instead of a <code>.csv</code>, and what benefit does this provide?</li> <li>What is PEPhub, and how can it simplify my data analysis?</li> </ul>"},{"location":"looper/user-tutorial/metadata/#recap","title":"Recap","text":"<p>In the first guide, we used a simple CSV file to specify our sample metadata in <code>metadata/sample_table.csv</code>, which looked like this:</p> metadata/sample_table.csv<pre><code>sample_name,area_type,file_path\nmexico,state,data/mexico.txt\nswitzerland,canton,data/switzerland.txt\ncanada,province,data/canada.txt\n</code></pre> <p>Then, we pointed to the CSV in the <code>.looper.yaml</code> config file:</p> .looper.yaml<pre><code>pep_config: metadata/sample_table.csv  # pointer to metadata file\noutput_dir: results\npipeline_interfaces:\n  - ['pipeline/pipeline_interface.yaml']\n</code></pre> <p>Now, we'll show you how to use more advanced features to make this better. </p>"},{"location":"looper/user-tutorial/metadata/#using-a-pep-for-derived-attributes","title":"Using a PEP for derived attributes","text":"<p>Looper can read sample metadata as a PEP instead of as a CSV file. This is a powerful framework for specifying sample metadata that allows you to make your metadata table easier to manage, easier to re-use, and more portable. You can read the detailed PEP documentation for a complete description of features. In this tutorial, we'll demonstrate one of the most useful PEP features: derived attributes. Derived attributes will allow us to remove the file paths from the metadata table. We will keep everything else about the project the same (the data, pipeline and interface, and looper configuration) and replace the <code>metadata/sample_table.csv</code> with a PEP.</p> <p>The new sample metadata will use 2 files instead of one. We will still have the main <code>sample_table.csv</code> file, but we will add a <code>pep_config.yaml</code> file. </p>"},{"location":"looper/user-tutorial/metadata/#set-up-a-working-directory","title":"Set up a working directory","text":"<p>To start, let's set up a new root folder for this tutorial. We'll call it <code>pep_derived_attrs</code> since we'll be showing you how to use PEP derived attributes. Just copy over the previous tutorial folder (or download it from the hello looper repository), since we're going to just make a few changes to that project.</p> <pre><code>cd ..\ncp -r looper_csv_example pep_derived_attrs\nrm -rf pep_derived_attrs/results  # remove results folder\n</code></pre>"},{"location":"looper/user-tutorial/metadata/#remove-paths-from-sample_tablecsv","title":"Remove paths from <code>sample_table.csv</code>","text":"<p>One of our goals is to remove the paths from the sample table. The hard-coded paths tie the sample table to a particular compute environment, which is undesirable. What if we change where the data are stored? Or what if someone else wanted to download our project and run it locally? They'd need to put the dataset in exactly the spot we did, relative to our metadata table. That's not a major problem for a couple of small text files, but if you're dealing with hundreds of large biological data files, you probably don't want that data in the same physical location as your metadata and pipeline files. It might even be on a different file system. The PEP will help us simplify the sample table and make it portable.</p> <p>First, edit the sample table to remove all the paths. Just replace them all with a variable, which we'll call <code>source1</code> (it can be whatever you like). Here's our revised table:</p> metadata/sample_table.csv<pre><code>sample_name,area_type,file_path\nmexico,state,source1\nswitzerland,canton,source1\ncanada,province,source1\n</code></pre> <p>One nice thing about this table is that it's more portable. Now, it will be valid, even if it is removed from the data, because it no longer has paths that tie it to a specific computing environment. But, wait -- we've lost the path information, which we will need to process the files. We will move that information into the new <code>pep_config.yaml</code> file.</p>"},{"location":"looper/user-tutorial/metadata/#create-metadatapep_configyaml","title":"Create <code>metadata/pep_config.yaml</code>","text":"<p>Next, create <code>metadata/pep_config.yaml</code> file, with this content:</p> metadata/pep_config.yaml<pre><code>pep_version: 2.1.0\nsample_table: sample_table.csv\nsample_modifiers:\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: \"data/{sample_name}.txt\"\n</code></pre> <p>This is using a PEP feature called a \"sample modifier\", which will programmatically modify your sample metadata. The specific modifier is called the \"derive\" modifier, which has two arguments: <code>attributes</code> specifies which of the sample attributes (corresponding to your table's column headers) will be derived. In this case, it's the <code>file_path</code> attribute. Then, the <code>sources</code> argument provides a key-value pair. Any value with a source (here, <code>source1</code>) will be replaced with the corresponding value (<code>data/{sample_name}.txt</code>). These values are actually templates, and variables like <code>{sample_name}</code> will be replaced by the corresponding attribute from the sample. The result of this will be the same final metadata table we had earlier.</p>"},{"location":"looper/user-tutorial/metadata/#change-looperyaml-to-point-to-the-pep","title":"Change <code>.looper.yaml</code> to point to the PEP","text":"<p>Finally, we need to change the <code>.looper.yaml</code> file to point to the new <code>pep_config.yaml</code> file, instead of to the <code>.csv</code> file directly:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\n</code></pre> <p>And now we can run the project as we did before:</p> <pre><code>looper run\n</code></pre> <p>The results should be the same. </p>"},{"location":"looper/user-tutorial/metadata/#simplifying-constant-attributes","title":"Simplifying constant attributes","text":"<p>Now, let's use another PEP sample modifier: the append modifier. This allows you to add sample attributes with constant values. Notice that our sample table now has a constant column. Every sample has <code>file_path</code> set to <code>source1</code>: </p> metadata/sample_table.csv<pre><code>sample_name,area_type,file_path\nmexico,state,source1\nswitzerland,canton,source1\ncanada,province,source1\n</code></pre> <p>We can use the append modifier to simplify the table and save space. We do that with two quick steps. First, remove the <code>file_path</code> column from the table.</p> metadata/sample_table.csv<pre><code>sample_name,area_type\nmexico,state\nswitzerland,canton\ncanada,province\n</code></pre> <p>Then, add it back in to the config using an append sample modifier:</p> metadata/pep_config.yaml<pre><code>pep_version: 2.1.0\nsample_table: sample_table.csv\nsample_modifiers:\n  append:\n    file_path: source1\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: \"data/{sample_name}.txt\"\n</code></pre> <p>Since the append modifier happens before the derive modifier, the samples will first have the <code>file_path</code> attribute added, with the <code>source1</code> value. Then, these will be derived as before. This configuration thus produces the same result as before, but now with a very streamlined sample table.</p> <p>But what if your data sources are not the same? Say you have a set of samples where some of them are stored in one location, and others are stored in a separate location. You can still use the derive modifier, just specifying different sources in both your table and config, say, <code>source1</code> and <code>source2</code>, with different paths. The append trick wouldn't work, though because it adds constant attributes. In that case, you might use another sample modifier: the imply modifier. The imply modifier adds sample attributes with values that depend on the value of an existing attribute. We will cover more details on the imply modifier in the advanced guides.</p>"},{"location":"looper/user-tutorial/metadata/#why-is-this-useful","title":"Why is this useful?","text":"<p>In this simple example, we don't benefit much from using the derived columns, because there's no real harm in distributing a few text files along with your pipeline and sample metadata. But in a real-world case, things can get much more complicated. With big data, it starts to make a lot of sense to keep data separate from metadata, because it can be re-used for multiple projects. We want to share our projects with others, and we're not sure they will necessarily store the data in the same location we do. Paths start to get really long and unwieldy. They might point to different places. File systems can change, necessitating adjusting lots of variables. The PEP sample modifiers can help us corral all this chaos to keep sample tables clean and portable. They allow you to isolate any of these changes to the project configuration file, and minimize them to editing a single template variable. For more information and a more detailed example, see: How to eliminate paths from a sample table.</p>"},{"location":"looper/user-tutorial/metadata/#pephub","title":"PEPhub","text":"<p>What if you could point looper to a remote file, instead of having to store your metadata locally on disk? This is where PEPhub comes in. PEPhub is a web interface for storing PEPs. You can read more in the PEPhub documentation.</p> <p>This tutorial PEP is saved on PEPhub at https://pephub.databio.org/databio/pep_derived_attrs.</p> <p>Looper allows us to point to these remote PEPs instead of to local files. Just change the <code>pep_config</code> to point to a registry path on PEPhub, like this:</p> .looper.yaml<pre><code>pep_config: \"pephub.databio.org://databio/pep_derived_attrs:default\"\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\n</code></pre> <p>Now you can delete the <code>metadata</code> subfolder entirely, because we're not using it at all, but the <code>looper run</code> will give the same results. Now with your metadata living on a remote server, it becomes even more clear how we can benefit from separating file paths from the sample table.</p> <p>The next thing step is to have the pipeline results actually populate the table on PEPhub. This is possible with looper, but it requires that we learn one more important tool: pipestat. This is what we'll study next.</p> <p>Summary</p> <ul> <li>Looper will accept metadata as a <code>.csv</code>, but you can also use a PEP for more powerful management features.</li> <li>PEPs allow you to simplify your table with programmatic sample modifiers, like <code>derive</code>, <code>append</code>, or <code>imply</code>. </li> <li>One common use of PEP sample modifiers is to remove file paths from sample tables, making them more portable.</li> <li>PEPhub provides a web-based way to edit sample metadata, which can be consumed directly by looper.</li> </ul>"},{"location":"looper/user-tutorial/user-pipestat/","title":"Setting up pipestat","text":""},{"location":"looper/user-tutorial/user-pipestat/#introduction","title":"Introduction","text":"<p>In our previous tutorials, we deployed the <code>count_lines.sh</code> pipeline. The result of that pipeline was the number of provinces in several countries, which was simply printed into the log file. In a real-life pipeline, we usually don't just want to dig results out of log files. We would like to view the results in nice, tabular or HTML-based report.  Looper does this through pipestat, another component in PEPkit.</p> <p>Like the other PEPkit components, pipestat is a standalone tool. You can read the complete details about pipestat as a standalone tool in the pipestat documentation. You can use pipestat without using looper, and vice versa, but using pipestat alongside looper unlocks a set of helpful tools such as html reports via <code>looper report</code>.</p> <p>This tutorial will show you how to do that.</p> <p>Learning objectives</p> <ul> <li>What is pipestat? Why is it useful?</li> <li>What is a pipestat-compatible pipeline?</li> <li>How can I configure my looper workspace to use a pipestat-compatible pipeline effectively?</li> <li>Where are pipestat results saved? How can I store results of my pipeline somewhere else?</li> <li>How can I make my pipeline store results in PEPhub?</li> <li>Can looper monitor whether my jobs are still running, failed, or already completed?</li> </ul>"},{"location":"looper/user-tutorial/user-pipestat/#a-basic-pipestat-compatible-pipeline","title":"A basic pipestat-compatible pipeline","text":"<p>All the options and features in this tutorial require a pipestat-compatible pipeline. What does that mean? Configuring a pipeline to use pipestat will go into detail about how to make a pipeline pipestat-compatible. Briefly, it just means these 3 criteria are fulfilled:</p> <ol> <li>The pipeline specifies a pipestat output schema. This just tells pipestat what results a pipeline can report.</li> <li>The pipeline uses <code>pipestat</code> to report results.</li> <li>The looper pipeline interface specifies the path to that output schema in the <code>output_schema</code> key, like this:</li> </ol> pipeline/pipeline_interface.yaml<pre><code>pipeline_name: count_lines\noutput_schema: pipestat_output_schema.yaml\nsample_interface:\n  command_template: &gt;\n    pipeline/count_lines.sh {sample.file_path} {sample.sample_name} {pipestat.config_file}\n</code></pre> <p>A pipeline that satisfies these criteria is pipestat-compatible, and for these pipelines, looper can give you a nice, web browsable report of results. It can also help you manage job status of your runs.</p> <p>To demonstrate, let's use a modified version of our <code>count_lines</code> pipeline that has been made pipestat-compatible.</p> <p>Navigate to the pipestat_example from the hello_looper repo.</p>"},{"location":"looper/user-tutorial/user-pipestat/#configure-where-pipestat-results-will-be-stored","title":"Configure where pipestat results will be stored","text":"<p>One goal of pipestat is that it allows you to configure a pipeline to store results in different places. You can either store results in a simple file, in a database, or in PEPhub. We'll start with the simplest option and configure pipestat to use a results file. Configure pipestat to use a results file with these lines in the looper config file:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  results_file_path: results.yaml\n</code></pre> <p>This instructs looper to configure pipestat to store the results in a <code>.yaml</code> file. Looper will now configure the pipeline to report results into a <code>results.yaml</code> file.</p> <p>Execute the run with: <pre><code>looper run\n</code></pre></p> <p>You can now see the results reported in the <code>results.yaml</code> output file.</p>"},{"location":"looper/user-tutorial/user-pipestat/#reporting-results-back-to-a-database","title":"Reporting results back to a database","text":"<p>Using results file and a database backend</p> <p>If you provide database credentials and a results file path, the results file path will take priority and results will only be reported to the local file.</p>"},{"location":"looper/user-tutorial/user-pipestat/#postgresql","title":"PostgreSQL","text":"<p>Pipestat also supports PostgreSQL databases as a backend. You will need to set up your own database or be provided the credentials to an existing database.</p> <p>Using docker to set up a temporary PostgreSQL database</p> <p>If you are comfortable using docker, you can quickly set up an instance of a PostgreSQL database using the following command: <code>docker run --rm -it --name looper_tutorial \\ -e POSTGRES_USER=looper_test_user \\ -e POSTGRES_PASSWORD=looper_test_pw \\ -e POSTGRES_DB=looper-test-db \\ -p 127.0.0.1:5432:5432 \\ postgres</code></p> <p>Once you have those credentials, you can configure pipestat to use those credentials in the looper config file: .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml \noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  database:\n    dialect: postgresql\n    driver: psycopg2\n    name: looper-test-db\n    user: looper_test_user\n    password: looper_test_pw\n    host: 127.0.0.1\n    port: 5432\n</code></pre></p>"},{"location":"looper/user-tutorial/user-pipestat/#sqlite","title":"SQLite","text":"<p>You can also report results to a SQLite database. You will need to provide a path to the local SQLite database. .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  database:\n    sqlite_url: \"sqlite:///yourdatabase.sqlite3\"\n</code></pre></p> <p>Once the database credentials are added for either PostgreSQL or SQLite backends, execute the run with: <pre><code>looper run\n</code></pre></p> <p>Using a database browser, you will now be able to view the reported results within the database of your choice.</p>"},{"location":"looper/user-tutorial/user-pipestat/#reporting-results-back-to-pephub","title":"Reporting results back to PEPhub","text":"<p>In the previous tutorial, you configured looper to read sample metadata from PEPhub. Now, by adding in pipestat integration, we can also report pipeline results back to PEPhub. In this example, we'll report the results back to the demo PEP we used earlier, <code>databio/pipestat_demo:default</code>. But you won't be able to report the results back to the demo repository because you don't have permissions. So if you want to follow along, you'll first need to create your own PEP on PEPHub to hold these results. Then, you can run this section yourself by replacing <code>databio/pipestat_demo:default</code> with the registry path to a PEP you control.</p> <p>To configure pipestat to report results to PEPhub instead of to a file, we just change our looper config to point to a <code>pephub_path</code>:</p> .looper.yaml<pre><code>pep_config: metadata/pep_config.yaml\noutput_dir: results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  pephub_path: \"databio/pipestat_demo:default\"\n  flag_file_dir: results/flags\n</code></pre> <p>No other changes are necessary. You will have to authenticate with PEPhub using <code>phc login</code>, and then looper will pass along the information in the generated pipestat config file. Pipestat will read the <code>pephub_path</code> from the config file and report results directly to PEPhub using its API!</p>"},{"location":"looper/user-tutorial/user-pipestat/#generating-result-reports","title":"Generating result reports","text":"<p>Now that you have your first pipestat pipeline configured with looper, there are many other, more powerful things you can add to make this even more useful. For example, now that looper knows the structure of results your pipeline reports, it can automatically generate beautiful, project-wide results summary HTML pages for you.</p>"},{"location":"looper/user-tutorial/user-pipestat/#html-reports","title":"HTML reports","text":"<p>Looper provides an easy <code>report</code> command that creates an html report of all reported results. You've already configured everything above. To get the report, run the command:</p> <pre><code>looper report\n</code></pre> <p>This command will call <code>pipestat summarize</code> on the results located in your results location. In this case, the <code>results.yaml</code> file.</p> <p>Here is an example html report for the above tutorial examples: count lines report</p> <p>A more advanced example of an html report using <code>looper report</code> can be found here: PEPATAC Gold Summary</p>"},{"location":"looper/user-tutorial/user-pipestat/#create-tables-and-stats-summaries","title":"Create tables and stats summaries","text":"<p>Having a nice HTML-browsable record of results is great for human browsing, but you may also want the aggregated results in a machine-readable form for downstream analysis. Looper can also create summaries in a computable format as  <code>.tsv</code> and <code>.yaml</code> files.</p> <p>Run: <pre><code>looper table\n</code></pre></p> <p>This will produce a <code>.tsv</code> file for aggregated primitive results (integers, strings, etc), as well as a <code>.yaml</code> file for any aggregated object results: <pre><code>Looper version: 2.0.0\nCommand: table\nUsing looper config (.looper.yaml).\nCreating objects summary\n'count_lines' pipeline stats summary (n=4): results/count_lines_stats_summary.tsv\n'count_lines' pipeline objects summary (n=0): results/count_lines_objs_summary.yaml\n</code></pre></p>"},{"location":"looper/user-tutorial/user-pipestat/#setting-and-checking-status","title":"Setting and checking status","text":"<p>Besides reporting results, another feature of pipestat is that it allows users to set pipeline status. If your pipeline uses pipestat to set status flags, then looper can be used to check the status of pipeline runs. To check the status of all samples, use:</p> <pre><code>looper check\n</code></pre> <p>For this example, the 'running' flag doesn't really help because the pipeline runs so fast that it immediately finishes. But in a pipeline that will take minutes or hours to complete, it can be useful to know how many and which jobs are running. That's why <code>looper check</code> can be helpful for these long-running pipelines.</p> <p>Do I have to use pipestat?</p> <p>No. You can use looper just as we did in the first two tutorials to run any command. Often, you'll want to use looper to run an existing pipeline that you didn't create. In that case, you won't have the option of using pipestat, since you're unlikely to go to the effort of adapting someone else's pipeline to use it. For non-pipestat-compatible pipelines, you can still use looper to run pipelines, but you won't be able to use <code>looper report</code> or <code>looper check</code> to manage their output.</p> <p>What benefits does pipestat give me? If you are developing your own pipeline, then you might want to consider using pipestat in your pipeline. This will allow users to use <code>looper check</code> to check on the status of pipelines. It will also enable <code>looper report</code> and <code>looper table</code> to create summarized outputs of pipeline results.</p> <p>Summary</p> <ul> <li>Pipestat is a standalone tool that can be used with or without looper.</li> <li>Pipestat standardizes reporting of pipeline results. It provides a standard specification for how pipeline outputs should be stored; and an implementation to easily write results to that format from within Python or from the command line.</li> <li>A pipeline user can configure a pipestat-compatible pipeline to record results in a file, in a database, or in PEPhub.</li> <li>Looper synergizes with pipestat to add powerful features such as checking job status and generating html reports.</li> </ul>"},{"location":"looper/user-tutorial/var_templates_deprecated/","title":"Var templates deprecated","text":"<p>This is old, outdated docs. Really you should only use var_templates to pass template-rendered variables to presubmission functions.</p> <p>var templates</p> <p>The value of the var_templates section is that it's the only way to pass a derived parameter to a python presubmission function.</p>"},{"location":"looper/user-tutorial/var_templates_deprecated/#variable-templates-via-var_templates","title":"Variable templates via <code>var_templates</code>","text":"<p>Let us circle back to the <code>var_templates</code> key. You can use <code>var_templates</code> to render re-usable variables for use in the command template.</p> pipeline_interface.yaml<pre><code>pipeline_name: count_lines\noutput_schema: output_schema.yaml\nvar_templates:\n  pipeline: '{looper.piface_dir}/count_lines.sh'\nsample_interface:\n  command_template: {pipeline.var_templates.pipeline} {sample.file} --output-parent {looper.sample_output_folder}\n</code></pre> <p>You can add anything to the var_templates that you need to later access in the command templates: pipeline_interface.yaml<pre><code>pipeline_name: count_lines\nvar_templates:  \n  pipeline: '{looper.piface_dir}/count_lines.sh' \n  refgenie_config: \"$REFGENIE\"\nsample_interface:\n  command_template: &gt;\n    {pipeline.var_templates.pipeline} --config {pipeline.var_templates.refgenie_config} {sample.file_path} --output-parent {looper.sample_output_folder}\n</code></pre></p> <p>There are two primary uses for <code>var_templates</code>:</p>"},{"location":"looper/user-tutorial/var_templates_deprecated/#syntactic-sugar-for-repeated-use-of-variables","title":"Syntactic sugar for repeated use of variables","text":"<p>The <code>var_templates</code> allows you to define templates that will be populated, which can then be used as variables in other templates in the pipeline interface, such as the <code>command_template</code>.</p> <p>For example, we can define </p> var_templates<pre><code>var_templates:  \n  pipeline_path: \"{looper.piface_dir}/pipelines/pipeline1.py\"  \n</code></pre> <p>Then, we can use this in our command template, under <code>{pipeline.var_templates.pipeline_path}</code>.</p> command_template<pre><code>command_template: &gt;  \n  {pipeline.var_templates.pipeline_path} {sample.file_path}\n</code></pre> <p>How is this useful? Well, we might want to re-use this variable in a pre-submission script. Say our pipeline also has a function that runs some preparation. We could do this:</p> command_template<pre><code>var_templates:  \n  pipeline_path: \"{looper.piface_dir}/pipelines/pipeline1.py\"  \ncommand_template: &gt;  \n  {pipeline.var_templates.pipeline_path} {sample.file_path}\npre_submit:\n  command_templates:\n    - \"{pipeline.var_templates.pipeline_path} --pre {sample.sample_name}\"\n</code></pre> <p>This just makes the pipeline interface a little cleaner, since we only specify the relative path to the pipeline once.</p>"},{"location":"looper/user-tutorial/var_templates_deprecated/#parameterizing-plugins-with-variable-templates","title":"Parameterizing plugins with variable templates","text":"<p>Pre-submission hooks can run either CLI commands or they can run Python functions. When they run python functions, they </p> <p>Sometimes plugins require a parameter, that benefits from having access to the looper variable namespaces.</p> <p>For example, the <code>write_sample_yaml</code> plugin will write a sample yaml file for each sample.  Where should this file be saved? We can just spit it out somewhere, but it would be nice to have a parameter. So, we have a parameter. Now, we could just pass that parameter in any of the templates</p> <p>like <code>{pipeline.my_param}</code>. </p> <p>Great. But wait, what if we want to use another looper variable in there? The problem is, this isn't a template, so we can't</p> <p>That's what <code>var_templates</code> are. they are populated first through the template system, so they are rendered, then, they are provided.</p> <p>Example using var_templates: <pre><code>pipeline_name: example_pipeline  \noutput_schema: output_schema.yaml  \nvar_templates:  \n  pipeline_path: \"{looper.piface_dir}/pipelines/pipeline1.py\"  \ncommand_template: &gt;  \n  {pipeline.var_templates.pipeline_path} --sample-name {sample.sample_name} --req-attr {sample.attr} \n</code></pre></p>"},{"location":"pephub/","title":"PEPhub","text":"<p>PEPhub is an open-source database, web interface, and API for sharing, retrieving, and validating metadata. PEPhub consists of:</p> <ul> <li>Public user interface: https://pephub.databio.org/</li> <li>API: https://pephub-api.databio.org/api/v1/docs</li> <li>DEV API: https://pephub-api-dev.databio.org/api/v1/docs</li> </ul>"},{"location":"pephub/#features-at-a-glance","title":"Features at-a-glance","text":"<ul> <li> <p>Validation. Users specify a schema for a project (using an extended JSON Schema), and use it to validate sample metadata. </p> </li> <li> <p>Semantic search. The PEPhub search interface provides an extremely fast and powerful semantic search of sample metadata. It is built using cutting-edge machine learning (sentence transformers) and stored in a fast vector databases. </p> </li> <li> <p>Authorization. PEPhub has a robust user authorization system to allow users to submit and edit their own metadata. Users authenticate via GitHub. Data may be either public or private, and can be restricted to individual or group-level permissions using GitHub organizations. </p> </li> <li> <p>Re-processing of GEO metadata. The public PEPhub instance geo namespace  holds metadata from nearly 99% of the Gene Expression Omnibus. PEPhub is updated weekly using GEOfetch to produce standardized PEP sample tables, providing a convenient API interface to GEO metadata.</p> </li> <li> <p>Command-line client. You can use PEPHubClient for  command-line tool and Python API, which allows authentication, download, upload of public or private projects.</p> </li> <li> <p>Group PEPs with using a PEP of PEPs (POP). A PEP of PEPs, or simply a POP, is a type of PEP in which each row is itself a PEP. POPs allow users to organize projects into groups.</p> </li> </ul>"},{"location":"pephub/#next-steps","title":"Next steps","text":"<p>Choose your adventure:</p> <ul> <li> <p>User guide   Teaches you how to use PEPhub to manage, share, and validate your sample metadata.</p> </li> <li> <p>Developer guide   Teaches you how to contribute to PEPhub, build tools on the PEPhub API, or deploy your own instance.</p> </li> </ul>"},{"location":"pephub/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"pephub/changelog/#0140-18-09-2024","title":"[0.14.0] - 18-09-2024","text":"<ul> <li>Added BEDMS standardizer to PEPhub</li> <li>Updated schema endpoints </li> <li>Added archive namespace endpoint</li> <li>Fixed multiple bugs</li> <li>A lot of UI work</li> </ul>"},{"location":"pephub/changelog/#0130-25-07-2024","title":"[0.13.0] - 25-07-2024","text":"<ul> <li>Added schemas to the PEPhub</li> <li>Fixed a lot of ui issues #360</li> </ul>"},{"location":"pephub/changelog/#0120-18-07-2024","title":"[0.12.0] - 18-07-2024","text":"<ul> <li>Fixed CTRL+K and CTRL+S not working  #350</li> <li>Added Project history #349</li> <li>Fixed PEPhub won't save a table without sample_name #347</li> <li>Fixed Moving to 'config' tab loses all unsaved changes in 'samples' tab #346</li> <li>Fixed Pasting csv file into pephub yields error #345</li> <li>Fixed UI is creating bogus column names when you remove existing columns. #344</li> <li>Fixed UI invents a column name #343</li> <li>Fixed UI issues with pasting a column header named 'sample_name' #342</li> <li>Fixed UI glitch reloads data on save #340</li> <li>Added Feature request: Ctrl+S to save #339</li> <li>Fixed pephub UI removes samples in progress if you switch to a different tab without saving #338</li> <li>Fixed pephub UI cannot delete a sample the intuitive way #337</li> </ul>"},{"location":"pephub/changelog/#0119-02-07-2024","title":"[0.11.9] - 02-07-2024","text":""},{"location":"pephub/changelog/#0118-02-26-2024","title":"[0.11.8] - 02-26-2024","text":"<ul> <li>Fix some bugs introduced as a result of the last release:</li> <li>tag's were being removed from the URL params when selecting a project view</li> <li>stabilized query params along the way on the namespace page</li> </ul>"},{"location":"pephub/changelog/#0117-02-22-2024","title":"[0.11.7] - 02-22-2024","text":"<ul> <li>Added interface for selecting and viewing project views</li> <li>optimized loading of very large sample tables</li> </ul>"},{"location":"pephub/changelog/#0116-02-08-2024","title":"[0.11.6] - 02-08-2024","text":""},{"location":"pephub/changelog/#fixed","title":"Fixed","text":"<ul> <li>Docs and docs links</li> <li>Bug in handsontable</li> <li>Response errors in samples and views</li> </ul>"},{"location":"pephub/changelog/#added","title":"Added","text":"<ul> <li>Namespace endpoint</li> </ul>"},{"location":"pephub/changelog/#0115-02-02-2024","title":"[0.11.5] - 02-02-2024","text":""},{"location":"pephub/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>POP updated</li> <li>Bug in updating project config file</li> <li>Subsample endpoint</li> <li>Lots more UI bugs that include some security vulnerabilities and stability issues</li> </ul>"},{"location":"pephub/changelog/#0114-01-22-2024","title":"[0.11.4] - 01-22-2024","text":""},{"location":"pephub/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Downloading zip files</li> </ul>"},{"location":"pephub/changelog/#0112-01-17-2024","title":"[0.11.2] - 01-17-2024","text":""},{"location":"pephub/changelog/#added_1","title":"Added","text":"<ul> <li>Add section to <code>/about</code> discussing browser support</li> </ul>"},{"location":"pephub/changelog/#0111-01-17-2024","title":"[0.11.1] - 01-17-2024","text":""},{"location":"pephub/changelog/#added_2","title":"Added","text":"<ul> <li><code>browserslist</code> support</li> </ul>"},{"location":"pephub/changelog/#changed","title":"Changed","text":"<ul> <li><code>useBiggestNamespaces</code> no longer cached.</li> </ul>"},{"location":"pephub/changelog/#0110-01-16-2024","title":"[0.11.0] - 01-16-2024","text":""},{"location":"pephub/changelog/#added_3","title":"Added","text":"<ul> <li>Group of PEPs, create a new type of PEP called a \"POP\" (a PEP of PEPs)</li> <li>Ability to star/favorite a PEP</li> <li>Updated search functionality to be more robust</li> </ul>"},{"location":"pephub/changelog/#changed_1","title":"Changed","text":"<ul> <li>Switch to <code>fastembed</code> for query embeddings to lower container size</li> <li>Minor UI updates</li> </ul>"},{"location":"pephub/changelog/#0105-12-04-2023","title":"[0.10.5] - 12-04-2023","text":""},{"location":"pephub/changelog/#changed_2","title":"Changed","text":"<ul> <li>optimized web interface fetching of PEP annotation data.</li> </ul>"},{"location":"pephub/changelog/#added_4","title":"Added","text":"<ul> <li>project annotation endpoint (#234)</li> </ul>"},{"location":"pephub/changelog/#0104-10-02-2023","title":"[0.10.4] - 10-02-2023","text":""},{"location":"pephub/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>PEP zip downloading</li> </ul>"},{"location":"pephub/changelog/#0103-08-31-2023","title":"[0.10.3] - 08-31-2023","text":""},{"location":"pephub/changelog/#changed_3","title":"Changed","text":"<ul> <li>Add support for twitter cards, change some things.</li> </ul>"},{"location":"pephub/changelog/#0102-08-31-2023","title":"[0.10.2] - 08-31-2023","text":""},{"location":"pephub/changelog/#changed_4","title":"Changed","text":"<ul> <li>Changed image link for open graph image</li> </ul>"},{"location":"pephub/changelog/#0101-08-30-2023","title":"[0.10.1] - 08-30-2023","text":""},{"location":"pephub/changelog/#changed_5","title":"Changed","text":"<ul> <li>Add opengraph image link</li> </ul>"},{"location":"pephub/changelog/#0100-08-24-2023","title":"[0.10.0] - 08-24-2023","text":""},{"location":"pephub/changelog/#added_5","title":"Added","text":"<ul> <li>Date filter to project annotation endpoint</li> </ul>"},{"location":"pephub/changelog/#099-08-22-2023","title":"[0.9.9] - 08-22-2023","text":""},{"location":"pephub/changelog/#changed_6","title":"Changed","text":"<ul> <li>schema tag URL to route to schema splash page</li> </ul>"},{"location":"pephub/changelog/#098-07-24-2023","title":"[0.9.8] - 07-24-2023","text":""},{"location":"pephub/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>cant add a PEP to a namespace if you don't have any to begin with</li> </ul>"},{"location":"pephub/changelog/#097-07-24-2023","title":"[0.9.7] - 07-24-2023","text":""},{"location":"pephub/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>sample table would exhibit odd, erratic behavior if column names were left blank</li> <li>alnding page styling was not optimal</li> </ul>"},{"location":"pephub/changelog/#096-07-20-2023","title":"[0.9.6] - 07-20-2023","text":""},{"location":"pephub/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Upload raw project errors</li> </ul>"},{"location":"pephub/changelog/#changed_7","title":"Changed","text":"<ul> <li>More stylish landing page that exemplifies pephub features</li> <li>Better error handling on queries</li> </ul>"},{"location":"pephub/changelog/#095-07-19-2023","title":"[0.9.5] - 07-19-2023","text":""},{"location":"pephub/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Changing sample_name error</li> </ul>"},{"location":"pephub/changelog/#added_6","title":"Added","text":"<ul> <li>Landing sample table</li> <li>UI tweaks</li> <li>About page (In progress)</li> <li>Sample, subsample, config update simultaneously when saved</li> </ul>"},{"location":"pephub/changelog/#changed_8","title":"Changed","text":"<ul> <li>Landing page</li> </ul>"},{"location":"pephub/changelog/#094-07-18-2023","title":"[0.9.4] - 07-18-2023","text":""},{"location":"pephub/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Typo in tooltip for search bar</li> </ul>"},{"location":"pephub/changelog/#added_7","title":"Added","text":"<ul> <li>Tooltip on landing page namespace list</li> </ul>"},{"location":"pephub/changelog/#changed_9","title":"Changed","text":"<ul> <li>Styling of landing namespaces to more clearly indicate they are links</li> </ul>"},{"location":"pephub/changelog/#093-07-17-2023","title":"[0.9.3] - 07-17-2023","text":""},{"location":"pephub/changelog/#changed_10","title":"Changed","text":"<ul> <li>Styling updates</li> <li>Landing tooltips</li> <li>Minor UI updates</li> </ul>"},{"location":"pephub/changelog/#092-07-12-2023","title":"[0.9.2] - 07-12-2023","text":""},{"location":"pephub/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>validating was not firing when updating sample table, subsample table, or config</li> </ul>"},{"location":"pephub/changelog/#added_8","title":"Added","text":"<ul> <li>github organizations UI visibility</li> <li>schema tag has link to schema</li> </ul>"},{"location":"pephub/changelog/#091-07-11-2023","title":"[0.9.1] - 07-11-2023","text":""},{"location":"pephub/changelog/#fixed_10","title":"Fixed","text":"<ul> <li>forking was broken</li> <li>order in config file was incorrect</li> </ul>"},{"location":"pephub/changelog/#added_9","title":"Added","text":"<ul> <li>config endpoint</li> </ul>"},{"location":"pephub/changelog/#090-07-05-2023","title":"[0.9.0] - 07-05-2023","text":""},{"location":"pephub/changelog/#fixed_11","title":"Fixed","text":"<ul> <li>description updating was broken</li> <li>strip markdown in description of projects in project list</li> <li>sample table stability updates</li> </ul>"},{"location":"pephub/changelog/#added_10","title":"Added","text":"<ul> <li>better authentication awareness, app now checks for login status on every render, removes session if no longer valid</li> <li>added basic subsample table editing</li> <li>better validation error messages for universal validator</li> </ul>"},{"location":"pephub/changelog/#084-06-21-2023","title":"[0.8.4] - 06-21-2023","text":""},{"location":"pephub/changelog/#fixed_12","title":"Fixed","text":"<ul> <li>lots of sample table editing bugs</li> <li>sample table editing now works as expected</li> <li>logging in sent you to the homepage no matter what</li> </ul>"},{"location":"pephub/changelog/#changed_11","title":"Changed","text":"<ul> <li>authentication uses <code>localStorage</code> instead of browser cookies</li> <li>forking a PEP brings in the description of the PEP</li> <li>landing page changes</li> </ul>"},{"location":"pephub/changelog/#083-06-11-2023","title":"[0.8.3] - 06-11-2023","text":""},{"location":"pephub/changelog/#fixed_13","title":"Fixed","text":"<ul> <li>schema validation error causing crash in production.</li> </ul>"},{"location":"pephub/changelog/#082-06-11-2023","title":"[0.8.2] - 06-11-2023","text":""},{"location":"pephub/changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Dockerfile build error due to missing <code>gcc</code> dependency.</li> </ul>"},{"location":"pephub/changelog/#081-06-11-2023","title":"[0.8.1] - 06-11-2023","text":""},{"location":"pephub/changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Dockerfile build error due to <code>psycopg2</code> issue.</li> </ul>"},{"location":"pephub/changelog/#080-06-09-2023","title":"[0.8.0] - 06-09-2023","text":""},{"location":"pephub/changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Sample table editing had bugs</li> <li>Width of sample table was incorrect</li> <li>Not found projects would load forever</li> <li>Case-sensitivity was causing errors</li> <li>Search links were not working</li> </ul>"},{"location":"pephub/changelog/#added_11","title":"Added","text":"<ul> <li>Interface updated to support new features</li> <li>Markdwon support for PEPs</li> <li>Assign schemas to PEPs</li> <li>Ability to sort PEPs on namespace page</li> <li>Database setup is now streamlined</li> </ul>"},{"location":"pephub/changelog/#removed","title":"Removed","text":"<ul> <li><code>cli</code> is now deprecated</li> </ul>"},{"location":"pephub/changelog/#074-2022-04-15","title":"[0.7.4] - 2022-04-15","text":""},{"location":"pephub/changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Couldnt remove columns from sample table</li> <li>Drag n Drop was broken -&gt; now fixed</li> <li>Fix sample table editing due to trailing commas</li> <li>File upload now fixed</li> </ul>"},{"location":"pephub/changelog/#changed_12","title":"Changed","text":"<ul> <li>Redesign project page for a better user experience</li> <li>Key binding for global search is now better</li> </ul>"},{"location":"pephub/changelog/#added_12","title":"Added","text":"<ul> <li>CSS utility classes for better styling</li> </ul>"},{"location":"pephub/changelog/#074-2022-04-15_1","title":"[0.7.4] - 2022-04-15","text":""},{"location":"pephub/changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Dockerfile not pulling in <code>/public</code> from <code>web/</code></li> </ul>"},{"location":"pephub/changelog/#073-2022-04-15","title":"[0.7.3] - 2022-04-15","text":""},{"location":"pephub/changelog/#changed_13","title":"Changed","text":"<ul> <li>Fixed dependencies to actually install the <code>cpu</code> only version of <code>torch</code></li> <li>Added new macOS dependencies to support native development/deployment</li> </ul>"},{"location":"pephub/changelog/#072-2022-04-14","title":"[0.7.2] - 2022-04-14","text":""},{"location":"pephub/changelog/#changed_14","title":"Changed","text":"<ul> <li>Fix bug that was deleting images from the frontend</li> </ul>"},{"location":"pephub/changelog/#071-2022-04-14","title":"[0.7.1] - 2022-04-14","text":""},{"location":"pephub/changelog/#changed_15","title":"Changed","text":"<ul> <li>Optimized Dockerfile for faster, slimmer deployments</li> </ul>"},{"location":"pephub/changelog/#070-2022-04-14","title":"[0.7.0] - 2022-04-14","text":""},{"location":"pephub/changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Private projects were viewable to those not logged in</li> <li>Some projects were causing 500 server errors</li> </ul>"},{"location":"pephub/changelog/#added_13","title":"Added","text":"<ul> <li>Device authentication flow</li> <li>Ability to \"fork\" a PEP</li> <li>Web interface lets you download a zip file of a PEP</li> <li>New submission flow for JSON representation of a PEP</li> </ul>"},{"location":"pephub/changelog/#changed_16","title":"Changed","text":"<ul> <li>Reimplemented web interface in React.js</li> <li>New deployment strategy that uses <code>uvicorn</code> instead of <code>pip install .</code></li> </ul>"},{"location":"pephub/changelog/#060-2022-03-06","title":"[0.6.0] - 2022-03-06","text":""},{"location":"pephub/changelog/#fixed_20","title":"Fixed","text":"<ul> <li>Buffering configuration file</li> <li>Saving failures in production</li> <li>Renewed login bug</li> </ul>"},{"location":"pephub/changelog/#added_14","title":"Added","text":"<ul> <li>Advanced searching features like score, offset, limit</li> <li>Ability to add a \"blank\" PEP</li> <li>Web-based metadata builder UI basics</li> </ul>"},{"location":"pephub/changelog/#changed_17","title":"Changed","text":"<ul> <li>Use <code>Authorization Bearer</code> headers to authenticate requests with the API</li> <li>Include nunjucks through CDN</li> <li>Switch <code>env</code> files to take advantage of <code>pass</code></li> </ul>"},{"location":"pephub/changelog/#050-2022-01-17","title":"[0.5.0] - 2022-01-17","text":""},{"location":"pephub/changelog/#added_15","title":"Added","text":"<ul> <li>New landing page</li> <li>New namespace page</li> <li>Ability to edit PEPs with the new project edit page</li> <li>Vector search for PEPs</li> </ul>"},{"location":"pephub/changelog/#changed_18","title":"Changed","text":"<ul> <li>Moved api endpoints to <code>/api/v1</code></li> </ul>"},{"location":"pephub/changelog/#removed_1","title":"Removed","text":"<ul> <li>Removed <code>/view</code> endpoints</li> </ul>"},{"location":"pephub/changelog/#040-2022-11-09","title":"[0.4.0] - 2022-11-09","text":""},{"location":"pephub/changelog/#added_16","title":"Added","text":"<ul> <li>Sort PEP's by authentication state (private PEPs)</li> <li>New and improbed web validator (thanks Alip!)</li> </ul>"},{"location":"pephub/changelog/#changed_19","title":"Changed","text":"<ul> <li>Revamped <code>pepdbagent</code> with better stability and type safety</li> </ul>"},{"location":"pephub/changelog/#030-2022-09-07","title":"[0.3.0] - 2022-09-07","text":""},{"location":"pephub/changelog/#added_17","title":"Added","text":"<ul> <li>User authentication to submit PEPs</li> <li>More thorough out <code>/view</code> endpoints</li> <li>More PEPs \ud83c\udf89</li> <li>Users can now specify a <code>?tag=</code> query parameter to fetch a PEP by its tag.</li> </ul>"},{"location":"pephub/changelog/#changed_20","title":"Changed","text":"<ul> <li>PEPhub is now backed by a postgres database</li> <li>Utilizes <code>pepgbagent</code> to interface with database</li> </ul>"},{"location":"pephub/changelog/#020-2022-06-16","title":"[0.2.0] - 2022-06-16","text":""},{"location":"pephub/changelog/#added_18","title":"Added","text":"<ul> <li>Better <code>/view</code> endpoints (switch from cards to tables)</li> <li>More namespace endpoints</li> <li>Sample view endpoints</li> </ul>"},{"location":"pephub/changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Poor output of <code>/pep-list</code></li> <li>filters wrapped in <code>json</code></li> <li><code>pephub</code> version missing</li> </ul>"},{"location":"pephub/changelog/#010-2022-05-16","title":"[0.1.0] - 2022-05-16","text":""},{"location":"pephub/changelog/#added_19","title":"Added","text":"<ul> <li>Endpoints for converting stored PEPs using filters.</li> <li>dotfiles (<code>.pep.yaml</code>) are now standardized</li> <li>removed redundant <code>_project</code> representation from endpoints</li> </ul>"},{"location":"pephub/changelog/#fixed_22","title":"Fixed:","text":"<ul> <li>project endpoints are no longer case-sensitive</li> <li>don't open demo links in new window</li> <li>better splash page styling</li> <li>no more pinned requirements file</li> </ul>"},{"location":"pephub/changelog/#001-2021-11-03","title":"[0.0.1] - 2021-11-03","text":""},{"location":"pephub/changelog/#added_20","title":"Added","text":"<ul> <li>Initial release</li> </ul>"},{"location":"pephub/developer/authentication-device/","title":"PEPhub device authentication","text":""},{"location":"pephub/developer/authentication-device/#introduction","title":"Introduction","text":"<p>PEPhub supports device authentication using a simplified version of the OAuth 2.0 Device Authorization Grant.  This authentication method is intended for devices that do not have a browser, or for command-line tools, in order to simplify the authentication process.  PEPhub device authentication is based on the general PEPhub Authentication  , with additional steps added specifically for device authentication.</p>"},{"location":"pephub/developer/authentication-device/#authorization-flow","title":"Authorization Flow","text":"<p>Authorization Flows contains steps that are described in the figure below.</p> <ol> <li> <p>The device initiates the login flow by sending a POST request to the PEPhub server. (The server creates a temporary device code and saves the user's IP address as one of the values in a dictionary.)</p> </li> <li> <p>In response, the device receives a code to be used for further authorization through a browser by the user. </p> </li> <li>The user opens a browser and enters the URL with the device code received in the previous step. </li> <li>The server checks if the device code exists and redirects the user to the GitHub OAuth page. </li> <li>GitHub redirects the user to the PEPhub callback with a GitHub user token. </li> <li>PEPhub sends requests to GitHub to get the user's information.</li> <li>PEPhub retrieves a JSON payload and creates a JWT token and temporarily saves it linked to the device code in the dictionary. </li> <li>The device sends a POST request to PEPhub with the device code. The server returns the JWT and deletes the temporarily stored information about it. </li> <li>The device receives the JWT.</li> </ol> <p></p>"},{"location":"pephub/developer/authentication/","title":"PEPhub Authentication","text":""},{"location":"pephub/developer/authentication/#introduction","title":"Introduction","text":"<p>PEPhub supports authentication. We use GitHub OAuth as an authorization provider and user/namespace management system. There are two kinds of namespaces in pephub: user namespaces and organization namespaces. User namespaces are just that: namespaces that contain PEPs submitted by a user who has authenticated from GitHub. For example, my GitHub username/namespace is nleroy917. So, my pephub namespace (once I authenticate) is also nleroy917. Organization namespaces contain PEPs submitted by users who belong to that organization on GitHub. For example, I (nleroy917) belong to the databio organization on GitHub. As such, once authenticated I can read and write PEPs to this namespace on pephub.</p> <p>pephub supports both reading and writing PEPs. Just like GitHub, all PEPs are by default available to view by all users. Users may choose to mark a PEP as private and any attempts to read or write to this PEP will require prior authorization. For example, if I submit a new PEP at <code>nleroy917/yeast-analysis:latest</code> and mark it as private. I must first authenticate, and then I will be able to read and write to this PEP.</p>"},{"location":"pephub/developer/authentication/#authorization-flow","title":"Authorization Flow","text":""},{"location":"pephub/developer/authentication/#overview","title":"Overview","text":"<p>PEPhub has a specific authorization flow to support software that integrates with the API and make authorized requests. We use JSON Web Tokens (JWTs) to authorize requests that access private information. Once the JWT is obtained, you can include it as a header to make requests to the API and retrieve private data for a particular user. JWTs are user-specific, and require GitHub log in to obtain. We take advantage of the authorization code flow to achieve this.</p>"},{"location":"pephub/developer/authentication/#obtaining-a-jwt","title":"Obtaining a JWT","text":"<p>Obtaining a JWT for a user happens in three steps: 1) Login request with PEPhub, 2) Redirect with authorization code, 3) Exchange authorization code for a JWT. Details and examples of each step are given below:</p> <p></p>"},{"location":"pephub/developer/authentication/#login-request-with-pephub","title":"Login request with PEPhub","text":"<p>The first step is to request authorization from the user, so your app can access to PEPhub resources on behalf of that user. To do so, your application must build and send a <code>GET</code> request to the <code>/auth/login</code> endpoint. An optional <code>client_redirect_uri</code> query parameter may be supplied so PEPhub knows where to redirect the request once an authorization code has been generated.</p> <p>FastAPI example: <pre><code>from typing import Optional\nfrom fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse\n\nBASE_URL = \"https://pephub.databio.org\"\n\napp = FastAPI()\n\n@app.get(\"/login\")\nasync def login(client_redirect_uri: Optional[str]):\n    if client_redirect_uri:\n        auth_url = f\"{BASE_URL}/auth/login?client_redirect_uri={client_redirect_uri}\"\n    else:\n        auth_url = f\"{BASE_URL}/auth/login\"\n    return RedirectResponse(auth_url)\n</code></pre></p> <p>React SPA example: <pre><code>const LoginButton = () =&gt; {\n  const BASE_URL = 'https://pephub.databio.org'\n  const client_redirect_uri = 'https://my-domain.com/callback'\n  return (\n    &lt;a href={`${BASE_URL}/auth/login?client_redirect_uri=${client_redirect_uri}`}&gt;\n        Log in to PEPhub\n    &lt;/a&gt;\n  )\n}\n</code></pre></p> <p>The user will be directed to GitHub to login and authorize PEPhub to obtain information on their account.</p>"},{"location":"pephub/developer/authentication/#redirect-with-authorization-code","title":"Redirect with authorization code","text":"<p>If the GitHub log in is successful, PEPhub will either redirect to its <code>/auth/login/success</code> endpoint, or the requesting client's <code>client_redirect_uri</code> if supplied with an authorization code as a query parameter. For example: <code>https://my-domain.com/callback?code=NApCCg..BkWtQ</code>. The app is now ready to exchange the authorization code for a JWT. It can do this by making a <code>POST</code> request to the <code>/auth/token</code> endpoint. The body of this <code>POST</code> request must contain the following parameters encoded in <code>application/json</code>:</p> Field Value <code>code</code> Required The authorization code obtained from the previous request <code>client_redirect_uri</code> Optional The <code>client_redirect_value</code> (if any) used to obtain the authorization code <p>FastAPI <pre><code>import httpx\nfrom fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse\n\nBASE_URL = \"https://pephub.databio.org\"\nCLIENT_REDIRECT_URI = \"https://my-domain.com/callback\"\n\napp = FastAPI()\n\n@app.get(\"/callback\")\nasync def login(code: str):\n    payload = {\n      'code': code,\n      'client_redirect_uri': CLIENT_REDIRECT_URI\n    }\n    res = httpx.post(f\"{BASE_URL}/auth/token\", json=payload)\n    return {\n        'token': res.json()['token']\n    }\n</code></pre></p> <p>React SPA <pre><code>import { useEffect, useMemo } from 'react'\nimport { useLocation, redirect } from 'react-router-dom'\nimport { useCookies } from 'react-cookie'\nimport { exchangeCodeForToken } from '../utils'\n\nconst API_BASE = import.meta.env.VITE_BASE_URL\nconst CLIENT_REDIRECT = import.meta.env.VITE_CLIENT_REDIRECT_URI\n\nconst LoginSuccess = () =&gt; {\n    // cookies\n    const [cookies, setCookie] = useCookies(['pephub_session']);\n    const [token, setToken] = useState(undefined)\n\n    // url params\n    const location = useLocation()\n    const params = useMemo(() =&gt; {\n        const searchParams = new URLSearchParams(location.search)\n        return Object.fromEntries(searchParams.entries())\n    }, [location.search])\n\n    // watch for changes and get JWT to set cookie\n    useEffect(() =&gt; {\n        if(params.code) {\n            exchangeCodeForToken(params.code)\n            .then((token) =&gt; setToken(token))\n        }\n    }, [params.code])\n\n    return (\n        &lt;div className=\"container mx-auto px-4 h-screen\"&gt;\n            &lt;div className=\"h-full flex flex-col items-center justify-center\"&gt;\n                &lt;img className=\"animate-bounce\" height=\"100\" width=\"100\" src=\"/pep-dark.svg\" /&gt;\n                &lt;p className=\"text-4xl font-bold\"&gt;Logging in...&lt;/p&gt;\n                &lt;p className=\"mt-3\"&gt;\n                    If you are not redirected in 5 seconds... \n                    &lt;a className=\"text-blue-600 ml-2\" href=\"/\"&gt;click here&lt;/a&gt;\n                &lt;/p&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    )\n}\n\nexport default LoginSuccess\n</code></pre></p>"},{"location":"pephub/developer/authentication/#using-jwt-to-make-requests","title":"Using JWT to make requests","text":"<p>Once the JWT is obtained, you can use it to make authorized requests on the user's behalf. To do so, simply include the token as an authorization header. For example: <code>Authorization: Bearer user-jwt-token</code></p> <p>Python <pre><code>import httpx\n\nhdrs = {\n   'Authorization': \"Bearer ey38fh421r....fqw\"\n}\n\nres = httpx.get(\n  \"https://pephub.databio.org/api/v1/projects/nleroy917/my-private-pep\",\n  headers=hdrs\n)\n</code></pre></p> <p>JavaScript/TypeScript <pre><code>fetch(\n  'https://pephub.databio.org/api/v1/projects/nleroy917/my-private-pep',\n  {\n    method: 'GET',\n    headers: {\n      Authorization: 'Bearer ey54y29r....3r31'\n    }\n  }\n)\n.then((res) =&gt; res.json())\n.then((data) =&gt; console.log(res.data))\n</code></pre></p>"},{"location":"pephub/developer/authentication/#examples","title":"Examples","text":"<p>We have provided two examples that utilize the PEPhub authorization flow: 1) a python command-line interface and 2) and React-based single-page application.</p>"},{"location":"pephub/developer/authentication/#setting-up-github-oauth-for-your-own-server","title":"Setting Up GitHub OAuth For Your Own Server","text":"<p>If you wish to run your own pephub server, you will need to set up your own GitHub OAuth application. This is a simple process. First, you will need to create a new GitHub OAuth application. Detailed instructions can be found in the GitHub documentation. You will need to set the following fields: - Homepage URL: <code>http://localhost:8000</code>. - Authorization callback URL: <code>http://localhost:8000/auth/callback</code>.</p> <p>You do not need to enable device flow. Once you have created your application, you will be given a <code>Client ID</code> and can now generate a <code>Client secret</code>. Finally, you will need to set these values in your pephub server's environment variables. You can do this by manually exporting them with <code>export GH_CLIENT_ID=...</code> and <code>export GH_CLIENT_SECRET=...</code> or by curating your <code>.env</code> file with the following variables: - <code>GH_CLIENT_ID</code>: The client ID of your GitHub OAuth application. - <code>GH_CLIENT_SECRET</code>: The client secret of your GitHub OAuth application. - <code>REDIRECT_URI</code>: The redirect URI of your GitHub OAuth application.</p>"},{"location":"pephub/developer/authentication/#reading-peps","title":"Reading PEPs","text":"<p>Anyone can read all PEPs that are not marked as private without any authentication. If a user wishes to read a PEP marked as private, they must 1) Authenticate, and 2) be the owner of that PEP or belong to the organization that owns that PEP. In the interest of privacy, any access to a PEP that is marked as private without prior authorization will result in a <code>404</code> response.</p> <pre><code>flowchart LR\n    A[GET project] --&gt; B{Project is Private?}\n    B -- No --&gt; C[Return PEP]\n    B -- Yes --&gt; D{User is authenticated &lt;br/&gt; &lt;b&gt;and&lt;/b&gt; &lt;br/&gt; owns PEP?}\n    D -- No --&gt; E[404 Not Found]\n    D -- Yes --&gt; F[Return PEP]</code></pre> <p>This flow should be identical to the flow that GitHub uses to protect repositories.</p>"},{"location":"pephub/developer/authentication/#writing-peps","title":"Writing PEPs","text":""},{"location":"pephub/developer/authentication/#submitting-a-new-pep","title":"Submitting a new PEP","text":"<p>There are two scenarios for PEP submission: 1) A user submits to their namespace, and 2) A user submits to an organization. Both cases must require authentication. A user may freely submit PEPs to their own namespace. However, only members of an organization may submit PEPs to that organization. See below chart:</p> <pre><code>flowchart LR\n    A[POST project] --&gt; B{User is authenticated?}\n    B -- No --&gt; C[401 Unauthorized]\n    B -- Yes --&gt; D{User == namespace &lt;br/&gt;&lt;b&gt;or&lt;/b&gt;&lt;br/&gt; User belongs to org?}\n    D -- No --&gt; I[403 Forbidden]\n    D -- Yes --&gt; J[201 Success] </code></pre>"},{"location":"pephub/developer/authentication/#editing-an-existing-pep","title":"Editing an existing PEP","text":"<p>If a user wishes to edit an existing PEP, they must authenticate and satisfy one of two requirements: 1) The PEP belongs to their namespace, or 2) The PEP belongs to organization of which that user is a member.</p>"},{"location":"pephub/developer/authentication/#deleting-a-pep","title":"Deleting a PEP","text":"<p>If a user wishes to delete an existing PEP, they must authenticate and satisfy one of two requirements: 1) The PEP belongs to their namespace, or 2) The PEP belongs to organization of which that user is a member. See the below flow chart:</p> <pre><code>flowchart LR\n    _A[PATCH Project] --&gt; Ab{Project exists?}\n    A[DELETE Project] --&gt; Ab\n    Ab -- No --&gt; Ac[404 Not Found]\n    Ab -- Yes --&gt; B{Project is private?}\n    B -- Yes --&gt; Bd{User is logged in?}\n    Bd -- No --&gt; Be[404 Not Found]\n    Bd -- Yes --&gt; C{User == namespace &lt;br/&gt;&lt;b&gt;or&lt;/b&gt;&lt;br/&gt; User belongs to org?}\n    C -- No --&gt; H[404 Not Found]\n    C -- Yes --&gt; I[204 Success]\n    B -- No --&gt; Bb{User logged in?}\n    Bb -- No --&gt; Bc[401 Unauthorized]\n    Bb -- Yes --&gt; J{User == namespace &lt;br/&gt;&lt;b&gt;or&lt;/b&gt;&lt;br/&gt; User belongs to org?}\n    J -- No --&gt; O[403 Unauthorized]\n    J -- Yes --&gt; P[204 Success]</code></pre>"},{"location":"pephub/developer/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"pephub/developer/changelog/#0110-2024-01-16","title":"[0.11.0] - 2024-01-16","text":""},{"location":"pephub/developer/changelog/#added","title":"Added","text":"<ul> <li>Group of PEPs, create a new type of PEP called a \"POP\" (a PEP of PEPs)</li> <li>Ability to star/favorite a PEP</li> <li>Updated search functionality to be more robust</li> </ul>"},{"location":"pephub/developer/changelog/#changed","title":"Changed","text":"<ul> <li>Switch to <code>fastembed</code> for query embeddings to lower container size</li> <li>Minor UI updates</li> </ul>"},{"location":"pephub/developer/changelog/#0105-2023-12-04","title":"[0.10.5] - 2023-12-04","text":""},{"location":"pephub/developer/changelog/#changed_1","title":"Changed","text":"<ul> <li>optimized web interface fetching of PEP annotation data.</li> </ul>"},{"location":"pephub/developer/changelog/#added_1","title":"Added","text":"<ul> <li>project annotation endpoint (#234)</li> </ul>"},{"location":"pephub/developer/changelog/#0104-2023-10-02","title":"[0.10.4] - 2023-10-02","text":""},{"location":"pephub/developer/changelog/#fixed","title":"Fixed","text":"<ul> <li>PEP zip downloading</li> </ul>"},{"location":"pephub/developer/changelog/#0103-2023-08-31","title":"[0.10.3] - 2023-08-31","text":""},{"location":"pephub/developer/changelog/#changed_2","title":"Changed","text":"<ul> <li>Add support for twitter cards, change some things.</li> </ul>"},{"location":"pephub/developer/changelog/#0102-2023-08-31","title":"[0.10.2] - 2023-08-31","text":""},{"location":"pephub/developer/changelog/#changed_3","title":"Changed","text":"<ul> <li>Changed image link for open graph image</li> </ul>"},{"location":"pephub/developer/changelog/#0101-2023-08-30","title":"[0.10.1] - 2023-08-30","text":""},{"location":"pephub/developer/changelog/#changed_4","title":"Changed","text":"<ul> <li>Add opengraph image link</li> </ul>"},{"location":"pephub/developer/changelog/#0100-2023-08-24","title":"[0.10.0] - 2023-08-24","text":""},{"location":"pephub/developer/changelog/#added_2","title":"Added","text":"<ul> <li>Date filter to project annotation endpoint</li> </ul>"},{"location":"pephub/developer/changelog/#099-2023-08-22","title":"[0.9.9] - 2023-08-22","text":""},{"location":"pephub/developer/changelog/#changed_5","title":"Changed","text":"<ul> <li>schema tag URL to route to schema splash page</li> </ul>"},{"location":"pephub/developer/changelog/#098-2023-07-24","title":"[0.9.8] - 2023-07-24","text":""},{"location":"pephub/developer/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>cant add a PEP to a namespace if you don't have any to begin with</li> </ul>"},{"location":"pephub/developer/changelog/#097-2023-07-24","title":"[0.9.7] - 2023-07-24","text":""},{"location":"pephub/developer/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>sample table would exhibit odd, erratic behavior if column names were left blank</li> <li>landing page styling was not optimal</li> </ul>"},{"location":"pephub/developer/changelog/#096-2023-07-20","title":"[0.9.6] - 2023-07-20","text":""},{"location":"pephub/developer/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Upload raw project errors</li> </ul>"},{"location":"pephub/developer/changelog/#changed_6","title":"Changed","text":"<ul> <li>More stylish landing page that exemplifies pephub features</li> <li>Better error handling on queries</li> </ul>"},{"location":"pephub/developer/changelog/#095-2023-07-19","title":"[0.9.5] - 2023-07-19","text":""},{"location":"pephub/developer/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Changing sample_name error</li> </ul>"},{"location":"pephub/developer/changelog/#added_3","title":"Added","text":"<ul> <li>Landing sample table</li> <li>UI tweaks</li> <li>About page (In progress)</li> <li>Sample, subsample, config update simultaneously when saved</li> </ul>"},{"location":"pephub/developer/changelog/#changed_7","title":"Changed","text":"<ul> <li>Landing page</li> </ul>"},{"location":"pephub/developer/changelog/#094-2023-07-18","title":"[0.9.4] - 2023-07-18","text":""},{"location":"pephub/developer/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Typo in tooltip for search bar</li> </ul>"},{"location":"pephub/developer/changelog/#added_4","title":"Added","text":"<ul> <li>Tooltip on landing page namespace list</li> </ul>"},{"location":"pephub/developer/changelog/#changed_8","title":"Changed","text":"<ul> <li>Styling of landing namespaces to more clearly indicate they are links</li> </ul>"},{"location":"pephub/developer/changelog/#093-2023-07-17","title":"[0.9.3] - 2023-07-17","text":""},{"location":"pephub/developer/changelog/#changed_9","title":"Changed","text":"<ul> <li>Styling updates</li> <li>Landing tooltips</li> <li>Minor UI updates</li> </ul>"},{"location":"pephub/developer/changelog/#092-2023-07-12","title":"[0.9.2] - 2023-07-12","text":""},{"location":"pephub/developer/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>validating was not firing when updating sample table, subsample table, or config</li> </ul>"},{"location":"pephub/developer/changelog/#added_5","title":"Added","text":"<ul> <li>github organizations UI visibility</li> <li>schema tag has link to schema</li> </ul>"},{"location":"pephub/developer/changelog/#091-2023-07-11","title":"[0.9.1] - 2023-07-11","text":""},{"location":"pephub/developer/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>forking was broken</li> <li>order in config file was incorrect</li> </ul>"},{"location":"pephub/developer/changelog/#added_6","title":"Added","text":"<ul> <li>config endpoint</li> </ul>"},{"location":"pephub/developer/changelog/#090-2023-07-05","title":"[0.9.0] - 2023-07-05","text":""},{"location":"pephub/developer/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>description updating was broken</li> <li>strip markdown in description of projects in project list</li> <li>sample table stability updates</li> </ul>"},{"location":"pephub/developer/changelog/#added_7","title":"Added","text":"<ul> <li>better authentication awareness, app now checks for login status on every render, removes session if no longer valid</li> <li>added basic subsample table editing</li> <li>better validation error messages for universal validator</li> </ul>"},{"location":"pephub/developer/changelog/#084-2023-06-21","title":"[0.8.4] - 2023-06-21","text":""},{"location":"pephub/developer/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>lots of sample table editing bugs</li> <li>sample table editing now works as expected</li> <li>logging in sent you to the homepage no matter what</li> </ul>"},{"location":"pephub/developer/changelog/#changed_10","title":"Changed","text":"<ul> <li>authentication uses <code>localStorage</code> instead of browser cookies</li> <li>forking a PEP brings in the description of the PEP</li> <li>landing page changes</li> </ul>"},{"location":"pephub/developer/changelog/#083-2023-06-11","title":"[0.8.3] - 2023-06-11","text":""},{"location":"pephub/developer/changelog/#fixed_10","title":"Fixed","text":"<ul> <li>schema validation error causing crash in production.</li> </ul>"},{"location":"pephub/developer/changelog/#082-2023-06-11","title":"[0.8.2] - 2023-06-11","text":""},{"location":"pephub/developer/changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Dockerfile build error due to missing <code>gcc</code> dependency.</li> </ul>"},{"location":"pephub/developer/changelog/#081-2023-06-11","title":"[0.8.1] - 2023-06-11","text":""},{"location":"pephub/developer/changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Dockerfile build error due to <code>psycopg2</code> issue.</li> </ul>"},{"location":"pephub/developer/changelog/#080-2023-06-09","title":"[0.8.0] - 2023-06-09","text":""},{"location":"pephub/developer/changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Sample table editing had bugs</li> <li>Width of sample table was incorrect</li> <li>Not found projects would load forever</li> <li>Case-sensitivity was causing errors</li> <li>Search links were not working</li> </ul>"},{"location":"pephub/developer/changelog/#added_8","title":"Added","text":"<ul> <li>Interface updated to support new features</li> <li>Markdwon support for PEPs</li> <li>Assign schemas to PEPs</li> <li>Ability to sort PEPs on namespace page</li> <li>Database setup is now streamlined</li> </ul>"},{"location":"pephub/developer/changelog/#removed","title":"Removed","text":"<ul> <li><code>cli</code> is now deprecated</li> </ul>"},{"location":"pephub/developer/changelog/#074-2022-04-15","title":"[0.7.4] - 2022-04-15","text":""},{"location":"pephub/developer/changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Couldnt remove columns from sample table</li> <li>Drag n Drop was broken -&gt; now fixed</li> <li>Fix sample table editing due to trailing commas</li> <li>File upload now fixed</li> </ul>"},{"location":"pephub/developer/changelog/#changed_11","title":"Changed","text":"<ul> <li>Redesign project page for a better user experience</li> <li>Key binding for global search is now better</li> </ul>"},{"location":"pephub/developer/changelog/#added_9","title":"Added","text":"<ul> <li>CSS utility classes for better styling</li> </ul>"},{"location":"pephub/developer/changelog/#074-2022-04-15_1","title":"[0.7.4] - 2022-04-15","text":""},{"location":"pephub/developer/changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Dockerfile not pulling in <code>/public</code> from <code>web/</code></li> </ul>"},{"location":"pephub/developer/changelog/#073-2022-04-15","title":"[0.7.3] - 2022-04-15","text":""},{"location":"pephub/developer/changelog/#changed_12","title":"Changed","text":"<ul> <li>Fixed dependencies to actually install the <code>cpu</code> only version of <code>torch</code></li> <li>Added new macOS dependencies to support native development/deployment</li> </ul>"},{"location":"pephub/developer/changelog/#072-2022-04-14","title":"[0.7.2] - 2022-04-14","text":""},{"location":"pephub/developer/changelog/#changed_13","title":"Changed","text":"<ul> <li>Fix bug that was deleting images from the frontend</li> </ul>"},{"location":"pephub/developer/changelog/#071-2022-04-14","title":"[0.7.1] - 2022-04-14","text":""},{"location":"pephub/developer/changelog/#changed_14","title":"Changed","text":"<ul> <li>Optimized Dockerfile for faster, slimmer deployments</li> </ul>"},{"location":"pephub/developer/changelog/#070-2022-04-14","title":"[0.7.0] - 2022-04-14","text":""},{"location":"pephub/developer/changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Private projects were viewable to those not logged in</li> <li>Some projects were causing 500 server errors</li> </ul>"},{"location":"pephub/developer/changelog/#added_10","title":"Added","text":"<ul> <li>Device authentication flow</li> <li>Ability to \"fork\" a PEP</li> <li>Web interface lets you download a zip file of a PEP</li> <li>New submission flow for JSON representation of a PEP</li> </ul>"},{"location":"pephub/developer/changelog/#changed_15","title":"Changed","text":"<ul> <li>Reimplemented web interface in React.js</li> <li>New deployment strategy that uses <code>uvicorn</code> instead of <code>pip install .</code></li> </ul>"},{"location":"pephub/developer/changelog/#060-2022-03-06","title":"[0.6.0] - 2022-03-06","text":""},{"location":"pephub/developer/changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Buffering configuration file</li> <li>Saving failures in production</li> <li>Renewed login bug</li> </ul>"},{"location":"pephub/developer/changelog/#added_11","title":"Added","text":"<ul> <li>Advanced searching features like score, offset, limit</li> <li>Ability to add a \"blank\" PEP</li> <li>Web-based metadata builder UI basics</li> </ul>"},{"location":"pephub/developer/changelog/#changed_16","title":"Changed","text":"<ul> <li>Use <code>Authorization Bearer</code> headers to authenticate requests with the API</li> <li>Include nunjucks through CDN</li> <li>Switch <code>env</code> files to take advantage of <code>pass</code></li> </ul>"},{"location":"pephub/developer/changelog/#050-2022-01-17","title":"[0.5.0] - 2022-01-17","text":""},{"location":"pephub/developer/changelog/#added_12","title":"Added","text":"<ul> <li>New landing page</li> <li>New namespace page</li> <li>Ability to edit PEPs with the new project edit page</li> <li>Vector search for PEPs</li> </ul>"},{"location":"pephub/developer/changelog/#changed_17","title":"Changed","text":"<ul> <li>Moved api endpoints to <code>/api/v1</code></li> </ul>"},{"location":"pephub/developer/changelog/#removed_1","title":"Removed","text":"<ul> <li>Removed <code>/view</code> endpoints</li> </ul>"},{"location":"pephub/developer/changelog/#040-2022-11-09","title":"[0.4.0] - 2022-11-09","text":""},{"location":"pephub/developer/changelog/#added_13","title":"Added","text":"<ul> <li>Sort PEP's by authentication state (private PEPs)</li> <li>New and improbed web validator (thanks Alip!)</li> </ul>"},{"location":"pephub/developer/changelog/#changed_18","title":"Changed","text":"<ul> <li>Revamped <code>pepdbagent</code> with better stability and type safety</li> </ul>"},{"location":"pephub/developer/changelog/#030-2022-09-07","title":"[0.3.0] - 2022-09-07","text":""},{"location":"pephub/developer/changelog/#added_14","title":"Added","text":"<ul> <li>User authentication to submit PEPs</li> <li>More thorough out <code>/view</code> endpoints</li> <li>More PEPs \ud83c\udf89</li> <li>Users can now specify a <code>?tag=</code> query parameter to fetch a PEP by its tag.</li> </ul>"},{"location":"pephub/developer/changelog/#changed_19","title":"Changed","text":"<ul> <li>PEPhub is now backed by a postgres database</li> <li>Utilizes <code>pepgbagent</code> to interface with database</li> </ul>"},{"location":"pephub/developer/changelog/#020-2022-06-16","title":"[0.2.0] - 2022-06-16","text":""},{"location":"pephub/developer/changelog/#added_15","title":"Added","text":"<ul> <li>Better <code>/view</code> endpoints (switch from cards to tables)</li> <li>More namespace endpoints</li> <li>Sample view endpoints</li> </ul>"},{"location":"pephub/developer/changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Poor output of <code>/pep-list</code></li> <li>filters wrapped in <code>json</code></li> <li><code>pephub</code> version missing</li> </ul>"},{"location":"pephub/developer/changelog/#010-2022-05-16","title":"[0.1.0] - 2022-05-16","text":""},{"location":"pephub/developer/changelog/#added_16","title":"Added","text":"<ul> <li>Endpoints for converting stored PEPs using filters.</li> <li>dotfiles (<code>.pep.yaml</code>) are now standardized</li> <li>removed redundant <code>_project</code> representation from endpoints</li> </ul>"},{"location":"pephub/developer/changelog/#fixed_19","title":"Fixed:","text":"<ul> <li>project endpoints are no longer case-sensitive</li> <li>don't open demo links in new window</li> <li>better splash page styling</li> <li>no more pinned requirements file</li> </ul>"},{"location":"pephub/developer/changelog/#001-2021-11-03","title":"[0.0.1] - 2021-11-03","text":""},{"location":"pephub/developer/changelog/#added_17","title":"Added","text":"<ul> <li>Initial release</li> </ul>"},{"location":"pephub/developer/deployment/","title":"Deployment","text":"<p>We provide a publicly available instance of pephub free to use. However, there are many reasons why you might wish to deploy your own instance. Because of this, we provide detailed instructions and docker containers to spin up your own instance of pephub.</p>"},{"location":"pephub/developer/deployment/#build-the-container","title":"Build the container","text":"<pre><code>docker build -t pephub.databio.org .\n</code></pre>"},{"location":"pephub/developer/deployment/#run","title":"Run","text":"<p>PEPhub requires many parameters to run. You can read more about those here. These must be injected as environment variables. You can manually do this and inject one-by-one. There is an example script in the repo called <code>launch_docker.sh</code>.</p> <pre><code>launch_docker.sh\n</code></pre> <p>The basic steps are:</p> <ol> <li>Initialize env vars</li> </ol> <pre><code>source /home/nsheff/code/pephub/environment/production.env\n</code></pre> <ol> <li>Run with docker:</li> </ol> <pre><code>docker run --rm -p 80:80 \\\n    --env POSTGRES_HOST=$POSTGRES_HOST \\\n    --env POSTGRES_DB=$POSTGRES_DB \\\n    --env POSTGRES_USER=$POSTGRES_USER \\\n    --env POSTGRES_PASSWORD=$POSTGRES_PASSWORD \\\n    --env QDRANT_HOST=$QDRANT_HOST \\\n    --env QDRANT_PORT=$QDRANT_PORT \\\n    --env QDRANT_ENABLED=$QDRANT_ENABLED \\\n    --env QDRANT_API_KEY=$QDRANT_API_KEY \\\n    --env HF_MODEL=$HF_MODEL \\\n    --env GH_CLIENT_ID=$GH_CLIENT_ID \\\n    --env GH_CLIENT_SECRET=$GH_CLIENT_SECRET \\\n    --env REDIRECT_URI=$REDIRECT_URI \\\n    --env SERVER_ENV=$SERVER_ENV \\\n    --name pephub pephub\n</code></pre> <ol> <li>Visit http://localhost:80 to view the server.</li> </ol>"},{"location":"pephub/developer/development/","title":"PEPhub development","text":""},{"location":"pephub/developer/development/#introduction","title":"Introduction","text":"<p>The following assumes you have already setup a database. If you have not, please see here.</p> <p>There are two components to PEPhub: a FastAPI backend, and a React frontend. As such, when developing, you will need to run both the backend and frontend development servers. Full API documentation can be found at https://pephub-api.databio.org/api/v1/docs.</p>"},{"location":"pephub/developer/development/#backend-development","title":"Backend development","text":"<p><code>uvicorn</code> is used to run the backend development server. To start the backend server, run the following:</p> <pre><code>uvicorn pephub.main:app --reload\n</code></pre> <p>The backend server should now be running at http://localhost:8000. If you wish to debug the backend server, the repository contains a <code>launch.json</code> file for VSCode. You can use this to debug the backend server.</p>"},{"location":"pephub/developer/development/#frontend-development","title":"Frontend development","text":"<p>Before beginning, ensure you are using a <code>nodejs</code> version &gt; 16. To manage <code>node</code> versions, most people recommend <code>nvm</code>.</p> <p>We use vite as our development and build tool for the frontend. Before starting, make sure you point the development server at the already running backend server. To do this, create a <code>.env.local</code> file inside the <code>web/</code> directory with the following contents:</p> <pre><code>VITE_API_HOST=http://localhost:8000\n</code></pre> <p>Now, to start the frontend development server <code>cd</code> into the <code>web/</code> directory, and run the following:</p> <pre><code>npm install # yarn install\nnpm start # yarn dev\n</code></pre> <p>The frontend development server should now be running at http://localhost:5173/. The React development server comes with a lot of nice features, such as hot reloading, and debugging. You can read more about these features here.</p>"},{"location":"pephub/developer/geopephub/","title":"geopephub","text":"geopephub"},{"location":"pephub/developer/geopephub/#automatic-uploader-of-geo-metadata-projects-to-pephub","title":"Automatic uploader of GEO metadata projects to PEPhub.","text":"<p>This repository contains <code>geopephub</code> CLI, that enables to automatic upload GEO projects to PEPhub based on date and scheduled automatic uploading using GitHub actions.  Additionally, the CLI includes a download command, enabling users to retrieve projects from specified namespace directly from the PEPhub database. This feature is particularly helpful for downloading all GEO projects at once.</p> <p>Documentation: https://pep.databio.org/pephub</p> <p>Source Code: https://github.com/pepkit/geopephub</p>"},{"location":"pephub/developer/geopephub/#installation","title":"Installation","text":"<p>To install <code>geopephub</code> use this command:  <pre><code>pip install git+https://github.com/pepkit/geopephub.git\n</code></pre></p>"},{"location":"pephub/developer/geopephub/#overview","title":"Overview:","text":"<p>The <code>geopephub</code> consists of 4 main functionalities:</p> <p>1) Queuer: This module comprises functions that scan for new projects in GEO, generate a new cycle for the current run, and log details for each GEO project. It sets the project status to <code>queued</code> and adds it to the database. 2) Uploader: Checks if there are any queued cycles in the <code>cycle_status</code> table. It retrieves a list of queued projects, executes <code>GEOfetch</code> to download them, and uploads the results to PEPhub database using <code>pepdbagent</code>. <code>geopephub</code> updates the project upload status at each step, allowing for later checks to determine why the upload failed and what occurred. 3) Checker: This component examines previous cycles, verifies their status, and determines if they were executed. If a cycle was not executed or was unsuccessful, it triggers a rerun. In cases where only one project was unsuccessful, it attempts to upload it again. Additionally, if the cycle does not exist, it creates one using the queuer and uploads files using the uploader. 4) Downloader: Retrieves projects from the specified namespace, filters by uploading or updating date, and optionally sorts by name or date. It also allows setting a limit on the number of downloaded projects. Projects can be downloaded locally or to a specified S3 bucket. For more information, use the  <code>geopephub --help</code> command</p> <p>More information about these processes can be found in the flowcharts and overview below.</p> <p></p>"},{"location":"pephub/developer/geopephub/#queuer-flowchart","title":"Queuer Flowchart:","text":"<pre><code>%%{init: {'theme':'forest'}}%%\nstateDiagram-v2\n    s1 --&gt; s2 \n    s2 --&gt; s3\n    s3 --&gt; s4\n    s4 --&gt; s5\n    s1: Create a new cycle\n    s2: Find GEO updated projects with geofetch Finder\n    s3: Add projects to the queue in sample status table\n    s4: Change cycle status to queued\n    s5: Exit</code></pre>"},{"location":"pephub/developer/geopephub/#uploader-flowchart","title":"Uploader Flowchart:","text":"<pre><code>%%{init: {'theme':'forest'}}%%\nstateDiagram-v2\n    s1 --&gt; s2 \n    s2 --&gt; s3\n    s3 --&gt; s4\n    s4 --&gt; s5\n    s5 --&gt; s6\n    s6 --&gt; s7\n    s7 --&gt; s8\n\n    s7 --&gt; s2\n    s6 --&gt; s3\n\n    s1: Get queued cycles by specifying namespace\n    s2: Change status of the cycle\n    s2: Get each element from list of queued cycle\n    s3: Get each project (GSE) from one cycle\n    s4: Change status of the project in project_status_table\n    s5: Get specified project by running Geofetcher\n    s6: Using pepdbagent add project to the DB\n    s6: Change status of the project in project_status_table\n    s7: Change status of cycle in cycle_status_table\n    s8: Exit</code></pre>"},{"location":"pephub/developer/geopephub/#checker-flowchart","title":"Checker Flowchart:","text":"<pre><code>graph TD\n    A[Choose cycle to check] --&gt; B{Did it run?}\n    B --&gt;|Yes| C{Was it successful?}\n    B --&gt;|No| D[Run Queuer for the cycle]\n    C --&gt;|Yes| E{Did all samples succeed?}\n    C --&gt;|No| D\n\n    D --&gt; D1[Run Uploader for the cycle]\n    D1 --&gt; K\n\n    E --&gt; |Yes| K[Exit]\n    E --&gt; |No| G[Retrieve failed samples]\n\n    G --&gt; H[Run Queuer for samples]\n    H --&gt; F[Run Uploader for queued samples]\n\n    F --&gt; I[Change samples status in the table]\n\n    I --&gt; J[Change cycle status in the table]\n\n    J --&gt; K[Exit]\n</code></pre>"},{"location":"pephub/developer/semantic-search/","title":"Semantic Search with Sentence Transformers","text":"<p>pephub includes a semantic search feature powered by sentence transformer models. These models are trained to understand the meaning of text and create vector representations of sentences and paragraphs. These vector representations can be used to find semantically similar text by measuring the similarity between the vectors [Fig 1.]. </p> <p>By using sentence transformer based semantic search, pephub can understand the meaning behind queries and return results that are semantically similar to the query, rather than just matching keywords. This allows for more accurate and relevant search results.</p> <p>We leverage Qdrant to store, manage, and search through PEP description embeddings. Qdrant (read: quadrant ) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.</p> <p></p>"},{"location":"pephub/developer/server-settings/","title":"Server Settings","text":"<p>Server settings are supplied through environment variables. You may choose to manually export/inject these variables, or you can provide the server with a <code>.env</code> file. Upon server start-up, these variables will be injected into the environment.</p>"},{"location":"pephub/developer/server-settings/#quick-find","title":"Quick Find","text":"<ul> <li>Server Settings</li> <li>Quick Find</li> <li>General</li> <li>Postgres<ul> <li><code>POSTGRES_HOST</code></li> <li><code>POSTGRES_DB</code></li> <li><code>POSTGRES_USER</code></li> <li><code>POSTGRES_PASSWORD</code></li> <li><code>POSTGRES_PORT</code></li> </ul> </li> <li>Qdrant<ul> <li><code>QDRANT_HOST</code></li> <li><code>QDRANT_PORT</code></li> <li><code>QDRANT_ENABLED</code></li> <li><code>QDRANT_API_KEY</code></li> </ul> </li> <li>Github Client<ul> <li><code>GH_CLIENT_ID</code></li> <li><code>GH_CLIENT_SECRET</code></li> <li><code>REDIRECT_URI</code></li> </ul> </li> <li>Sentence Transformer<ul> <li><code>HF_MODEL</code></li> </ul> </li> </ul>"},{"location":"pephub/developer/server-settings/#general","title":"General","text":"<p>Coming soon...</p>"},{"location":"pephub/developer/server-settings/#postgres","title":"Postgres","text":""},{"location":"pephub/developer/server-settings/#postgres_host","title":"<code>POSTGRES_HOST</code>","text":"<ul> <li>Description: Hostname of the PostgreSQL server</li> <li>Required: Yes</li> <li>Default Value: localhost</li> </ul>"},{"location":"pephub/developer/server-settings/#postgres_db","title":"<code>POSTGRES_DB</code>","text":"<ul> <li>Description: Name of the PostgreSQL database</li> <li>Required: Yes</li> <li>Default Value: pephub</li> </ul>"},{"location":"pephub/developer/server-settings/#postgres_user","title":"<code>POSTGRES_USER</code>","text":"<ul> <li>Description: PostgreSQL username</li> <li>Required: Yes</li> <li>Default Value: postgres</li> </ul>"},{"location":"pephub/developer/server-settings/#postgres_password","title":"<code>POSTGRES_PASSWORD</code>","text":"<ul> <li>Description: PostgreSQL password</li> <li>Required: Yes</li> <li>Default Value: docker</li> </ul>"},{"location":"pephub/developer/server-settings/#postgres_port","title":"<code>POSTGRES_PORT</code>","text":"<ul> <li>Description: PostgreSQL port</li> <li>Required: Yes</li> <li>Default Value: 5432</li> </ul>"},{"location":"pephub/developer/server-settings/#qdrant","title":"Qdrant","text":""},{"location":"pephub/developer/server-settings/#qdrant_host","title":"<code>QDRANT_HOST</code>","text":"<ul> <li>Description: Hostname of the Qdrant server</li> <li>Required: No</li> <li>Default Value: localhost</li> </ul>"},{"location":"pephub/developer/server-settings/#qdrant_port","title":"<code>QDRANT_PORT</code>","text":"<ul> <li>Description: Port of the Qdrant server</li> <li>Required: No</li> <li>Default Value: 6333</li> </ul>"},{"location":"pephub/developer/server-settings/#qdrant_enabled","title":"<code>QDRANT_ENABLED</code>","text":"<ul> <li>Description: Enable/disable Qdrant functionality</li> <li>Required: No</li> <li>Default Value: False</li> </ul>"},{"location":"pephub/developer/server-settings/#qdrant_api_key","title":"<code>QDRANT_API_KEY</code>","text":"<ul> <li>Description: API key for connecting to Qdrant</li> <li>Required: No</li> <li>Default Value: Not applicable</li> </ul>"},{"location":"pephub/developer/server-settings/#github-client","title":"Github Client","text":""},{"location":"pephub/developer/server-settings/#gh_client_id","title":"<code>GH_CLIENT_ID</code>","text":"<ul> <li>Description: GitHub client ID</li> <li>Required: Yes</li> <li>Default Value: Not applicable</li> </ul>"},{"location":"pephub/developer/server-settings/#gh_client_secret","title":"<code>GH_CLIENT_SECRET</code>","text":"<ul> <li>Description: GitHub client secret</li> <li>Required: Yes</li> <li>Default Value: Not applicable</li> </ul>"},{"location":"pephub/developer/server-settings/#redirect_uri","title":"<code>REDIRECT_URI</code>","text":"<ul> <li>Description: URI to redirect to after GitHub OAuth flow</li> <li>Required: Yes</li> <li>Default Value: http://localhost:8000/auth/callback</li> </ul>"},{"location":"pephub/developer/server-settings/#sentence-transformer","title":"Sentence Transformer","text":""},{"location":"pephub/developer/server-settings/#hf_model","title":"<code>HF_MODEL</code>","text":"<ul> <li>Description: HuggingFace model name</li> <li>Required: No</li> <li>Default Value: \"BAAI/bge-small-en-v1.5\"</li> </ul>"},{"location":"pephub/developer/setup/","title":"Organization","text":""},{"location":"pephub/developer/setup/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>PEPhub consists of 3 components: 1) A postgres database; 2) the PEPhub API; 3) the PEPhub UI.</p>"},{"location":"pephub/developer/setup/#1-database-setup","title":"1. Database setup","text":"<p>pephub stores PEPs in a POSTGRES database. Create a new pephub-compatible postgres instance locally:</p> <pre><code>docker pull postgres\ndocker run \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_PASSWORD=docker \\\n  -e POSTGRES_DB=pep-db \\\n  -p 5432:5432 \\\n  postgres\n</code></pre> <p>You should now have a pephub-compatible postgres instance running at http://localhost:5432. You can use load_db.py to load a directory of PEPs into the database.</p>"},{"location":"pephub/developer/setup/#2-pephub-api-setup","title":"2. <code>pephub</code> API setup","text":""},{"location":"pephub/developer/setup/#install","title":"Install","text":"<p>Install dependencies using <code>pip</code> (We suggest using virtual environments):</p> <pre><code>python -m venv venv &amp;&amp; source venv/bin/activate\npip install -r requirements/requirements-all.txt\n</code></pre>"},{"location":"pephub/developer/setup/#running","title":"Running","text":"<p>pephub may be run in several ways. In every case, pephub requires configuration. Configuration settings are supplied to pephub through environment variables. The following settings are required. While pephub has built-in defaults for these settings, you should provide them to ensure compatibility:</p> <ul> <li><code>POSTGRES_HOST</code>: The hostname of the PEPhub database server</li> <li><code>POSTGRES_DB</code>: The name of the database inside the postgres server</li> <li><code>POSTGRES_USER</code>: Username for the database</li> <li><code>POSTGRES_PASSWORD</code>: Password for the user</li> <li><code>POSTGRES_PORT</code>: Port for postgres database</li> <li><code>GH_CLIENT_ID</code>: Client ID for the GitHub application that authenticates users</li> <li><code>GH_CLIENT_SECRET</code>: Client secret for the GitHub application that authenticates users</li> <li><code>BASE_URI</code>: A BASE URI of the PEPhub (e.g. localhost:8000)</li> </ul> <p>You must set these environment variables prior to running PEPhub. We've provided <code>env</code> files inside <code>environment</code> which you may <code>source</code> to load your environment. Alternatively, you may store them locally in a <code>.env</code> file. This file will get loaded and exported to your environment when the server starts up. We've included an example <code>.env</code> file with this repository. You can read more about server settings and configuration here.</p> <p>Once the configuration variables are set, run pephub natively with:</p> <pre><code>uvicorn pephub.main:app --reload\n</code></pre> <p>The pephub API should now be running at http://localhost:8000.</p>"},{"location":"pephub/developer/setup/#3-react-pephub-ui-setup","title":"3. React PEPhub UI setup","text":"<p>Important: To make the development server work, you must include a <code>.env.local</code> file inside <code>web/</code> with the following contents:</p> <pre><code>VITE_API_HOST=http://localhost:8000\n</code></pre> <p>This ensures that the frontend development server will proxy requests to the backend server. You can now run the frontend development server:</p> <pre><code>cd web\nnpm install # yarn install\nnpm start # yarn dev\n</code></pre> <p>The pephub frontend development server should now be running at http://localhost:5173/.</p>"},{"location":"pephub/developer/setup/#3-optional-github-authentication-client-setup","title":"3. (Optional) GitHub Authentication Client Setup","text":"<p>pephub uses GitHub for namespacing and authentication. As such, a GitHub application capable of logging in users is required. We've included instructions for setting up GitHub authentication locally using your own GitHub account.</p>"},{"location":"pephub/developer/setup/#4-optional-vector-database-setup","title":"4. (Optional) Vector Database Setup","text":"<p>We've added semantic-search capabilities to pephub. Optionally, you may host an instance of the qdrant vector database to store embeddings computed using a sentence transformer that has mined and processed any relevant metadata from PEPs. If no qdrant connection settings are supplied, pephub will default to SQL search. Read more here. To run qdrant locally, simply run the following:</p> <pre><code>docker pull qdrant/qdrant\ndocker run -p 6333:6333 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n    qdrant/qdrant\n</code></pre>"},{"location":"pephub/developer/setup/#running-with-docker","title":"Running with docker:","text":""},{"location":"pephub/developer/setup/#option-1-standalone-docker","title":"Option 1. Standalone <code>docker</code>:","text":"<p>If you already have a public database instance running, you can choose to build and run the server container only. A note to Apple Silicon (M1/M2) users: If you have issues running, try setting your default docker platform with <code>export DOCKER_DEFAULT_PLATFORM=linux/amd64</code> to get the container to build and run properly. See this issue for more information.</p> <p>1. Environment: Ensure that you have your environment properly configured. To manage secrets in your environment, we leverage <code>pass</code> and curated <code>.env</code> files. You can use our <code>launch_docker.sh</code> script to start your container with these <code>.env</code> files.</p> <p>2. Build and start container:</p> <pre><code>docker build -t pephub .\n./launch_docker.sh\n</code></pre> <p>Alternatively, you can inject your environment variables one-by-one:</p> <pre><code>docker run -p 8000:8000 \\\n -e POSTGRES_HOST=localhost \\\n -e POSTGRES_DB=pep-db \\\n ...\npephub\n</code></pre> <p>Or, provide your own <code>.env</code> file:</p> <pre><code>docker run -p 8000:8000 \\\n --env-file path/to/.env \\\n pephub\n</code></pre>"},{"location":"pephub/developer/setup/#option-2-docker-compose","title":"Option 2. <code>docker compose</code>:","text":"<p>The server has been Dockerized and packaged with a postgres image to be run with <code>docker compose</code>. This lets you run everything at once and develop without having to manage database instances.</p> <p>You can start a development environment in two steps:</p> <p>1. Curate your environment: Since we are running in <code>docker</code>, we need to supply environment variables to the container. The <code>docker-compose.yaml</code> file is written such that you can supply a <code>.env</code> file at the root with your configurations. See the example env file for reference. See here for a detailed explanation of all configurable server settings. For now, you can simply copy the <code>env</code> file:</p> <pre><code>cp environment/template.env .env\n</code></pre> <p>2. Build and start the containers:</p> <pre><code>docker compose up --build\n</code></pre> <p><code>pephub</code> now runs/listens on http://localhost:8000 <code>postgres</code> now runs/listens on http://localhost:5432</p> <p>3. (Optional) Utilize the <code>load_db</code> script to populate the database with <code>examples/</code>:</p> <pre><code>cd scripts\npython load_db.py \\\n--username docker \\\n--password password \\\n--database pephub\n../examples\n</code></pre> <p>4. (Optional) GitHub Authentication Client Setup</p> <p>pephub uses GitHub for namespacing and authentication. As such, a GitHub application capable of logging in users is required. We've included instructions for setting this up locally using your own GitHub account.</p> <p>5. (Optional) Vector Database Setup</p> <p>We've added semantic-search capabilities to pephub. Optionally, you may host an instance of the qdrant vector database to store embeddings computed using a sentence transformer that has mined and processed any relevant metadata from PEPs. If no qdrant connection settings are supplied, pephub will default to SQL search. Read more here. To run qdrant locally, simply run the following:</p> <pre><code>docker pull qdrant/qdrant\ndocker run -p 6333:6333 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n    qdrant/qdrant\n</code></pre> <p>Note: If you wish to run the development environment with a pubic database, curate your <code>.env</code> file as such.</p>"},{"location":"pephub/developer/tar_namespace/","title":"Archiving namespaces","text":""},{"location":"pephub/developer/tar_namespace/#creating-an-archive-of-specified-namespaces","title":"Creating an archive of specified namespaces","text":"<p>Developers can create an archive of specified namespaces by following these steps:</p>"},{"location":"pephub/developer/tar_namespace/#1-set-environment-variables-for-the-database-connection","title":"1. Set environment variables for the database connection.","text":"<p>Template of the environment file</p> <pre><code>POSTGRES_HOST=localhost:5432\nPOSTGRES_DB=pep-db\nPOSTGRES_USER=docker\nPOSTGRES_PASSWORD=password\n\nAWS_ACCESS_KEY_ID=key_id\nAWS_SECRET_ACCESS_KEY=secret_key\nAWS_ENDPOINT_URL=https://s3.us-west-002.backblazeb2.com/\n</code></pre> <p>Env file can be found here: https://github.com/pepkit/geopephub/blob/main/environment/dev.env</p>"},{"location":"pephub/developer/tar_namespace/#2-install-geopephub-package","title":"2. Install <code>geopephub</code> package:","text":"<pre><code>pip install geopephub\n</code></pre>"},{"location":"pephub/developer/tar_namespace/#3-use-geopephub-help","title":"3. Use geopephub help:","text":"<pre><code>geopephub auto-download --help\n</code></pre> help <pre><code>\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 auto-download  Automatically download projects from geo namespace, tar them and upload to s3. Don't forget to set up AWS credentials                                                                                             \u2502\n\u2502 bedbase-stats  Download projects from the bedbase namespace and save them to the database                                                                                                                                        \u2502\n\u2502 check-by-date  Check if all projects were uploaded successfully in specified period and upload them if not. Additionally, you can download projects from huge period of time. e.g. if you want to download projects from         \u2502\n\u2502 check-by-date  Check if all projects were uploaded successfully in specified period and upload them if not. Additionally, you can download projects from huge period of time. e.g. if you want to download projects from         \u2502\n\u2502                2020/02/25 to 2021/05/27, you should set start_period=2020/02/25, and end_period=2021/05/27                                                                                                                       \u2502\n\u2502 clean-history  Clean history of the pephub                                                                                                                                                                                       \u2502\n\u2502 download       Download projects from the particular namespace. You can filter projects, order them, and download only part of them.                                                                                             \u2502\n\u2502 run-checker    Check if all projects were uploaded successfully in specified period and upload them if not. To check if all projects were uploaded successfully 3 periods ago, where one period is 1 day, and cycles are         \u2502\n\u2502                happening every day, you should set cycle_count=3, and period_length=1. (geopephub run_checker --cycle-count 3 --period-length 1)                                                                                 \u2502\n\u2502 run-queuer     Queue GEO projects that were uploaded or updated in the last period                                                                                                                                               \u2502\n\u2502 run-uploader   Upload projects that were queued, but not uploaded yet.                                                                                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>To automatically download projects from the namespace, and register them in the database, use the following command  we will use <code>auto-download</code> command. It will download all PEPs to the chosen directory, and then tar them if needed, and upload to the s3 bucket.</p> <pre><code>geopephub auto-download --help\n</code></pre> help <pre><code>\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502    --namespace                        TEXT  Namespace of the projects that have to be downloaded. Default: geo [default: geo]                                                                                                    \u2502\n\u2502 *  --destination                      TEXT  Output directory or s3 bucket. By default set current directory [default: None] [required]                                                                                           \u2502\n\u2502    --compress       --no-compress           zip downloaded projects, default: True [default: compress]                                                                                                                           \u2502\n\u2502    --tar-all        --no-tar-all            tar all the downloaded projects into a single file [default: tar-all]                                                                                                                \u2502\n\u2502    --upload-s3      --no-upload-s3          upload tar file to s3 bucket [default: upload-s3]                                                                                                                                    \u2502\n\u2502    --bucket                           TEXT  S3 bucket name [default: pephub]                                                                                                                                                     \u2502\n\u2502    --help                                   Show this message and exit.                                                                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p><code>--tar-all</code> and <code>upload-s3</code> is defaulted to True, so if you want to download and tar all projects, and upload to the s3 bucket, you can skip these options.</p>"},{"location":"pephub/developer/tar_namespace/#4-run-the-command-with-the-desired-options","title":"4. Run the command with the desired options.","text":"<p>example</p> <pre><code>geopephub auto-download --namespace geo --destination /path/to/directory\n</code></pre> <p>Warning</p> <p>To keep everything up to date and avoid issues, the developer should set the destination to the same location every time this command is run. To archive a namespace without uploading to S3 and without saving the tar history to the pephub table, please use the <code>geopephub download</code> command.</p>"},{"location":"pephub/developer/pepdbagent/","title":"pepdbagent","text":"pepdbagent <p>Documentation: https://pep.databio.org/pephub/pepdbagent</p> <p>Source Code: https://github.com/pepkit/pepdbagent</p> <p><code>pepdbagent</code> is a Python library and toolkit that gives a user-friendly  interface to connect, upload, update and retrieve information from pep database. This library is designed to work  to be used by PEPhub, but it can be used for any other purpose, to manage data in pep database.</p> <p>pepdbagent creates a connection to the database and creates table schemas for the PEPhub database if necessary. Core database is <code>postgres</code> database, but it can be easily extended to other relational databases. To use <code>pepdbagent</code>, you need to have a database instance running with it's credentials. If the version of the database schema is not compatible with the version of <code>pepdbagent</code>, it will throw an exception.</p>"},{"location":"pephub/developer/pepdbagent/#installation","title":"Installation","text":"<p>To install <code>pepdbagent</code> use this command:  <pre><code>pip install pepdbagent\n</code></pre> or install the latest version from the GitHub repository: <pre><code>pip install git+https://github.com/pepkit/pepdbagent.git\n</code></pre></p>"},{"location":"pephub/developer/pepdbagent/#overview","title":"Overview:","text":"<p>The pepdbagent provides a core class called PEPDatabaseAgent. This class has 4 modules, divided  to increase readability, maintainability, and user experience of pepdbagent, which are:</p> <p>The <code>pepdbagent</code> consists of 6 main modules: - Namespace: Includes methods for searching namespaces, retrieving statistics, and fetching information. - Project: Provides functionality for retrieving, uploading, updating, and managing projects. - Annotation: Offers features for searching projects in the database and namespaces, retrieving annotations, and other related information. - Sample: Handles the creation, modification, and deletion of samples, without modification of the entire project. - View: Manages the creation, modification, and deletion of views for specific projects. - User: Contains user-related information such as favorites and other user-related data.</p>"},{"location":"pephub/developer/pepdbagent/#example","title":"Example:","text":""},{"location":"pephub/developer/pepdbagent/#instiantiate-a-pepdatabaseagent-object-and-connect-to-database","title":"Instiantiate a PEPDatabaseAgent object and connect to database:","text":"<pre><code>import pepdbagent\n# 1) By providing credentials and connection information:\nagent = pepdbagent.PEPDatabaseAgent(user=\"postgres\", password=\"docker\", )\n# 2) or By providing connection string:\nagent = pepdbagent.PEPDatabaseAgent(dsn=\"postgresql://postgres:docker@localhost:5432/pep-db\")\n</code></pre>"},{"location":"pephub/developer/pepdbagent/#example-of-usage-of-the-pepdbagent-modules","title":"Example of usage of the pepdbagent modules:","text":"<pre><code>import peppy\n\nprj_obj = peppy.Project(\"sample_pep/basic/project_config.yaml\")\n\n# create a project\nnamespace = \"demo\"\nname = \"basic_project\"\ntag = None\nagent.project.create(prj_obj, namespace, name, tag)\n\nupdate_dict = {\"is_private\" = True}\n# after creation of the dict, update record by providing update_dict and namespace, name and tag:\nagent.project.update(update_dict, namespace, name, tag)\n</code></pre>"},{"location":"pephub/developer/pepdbagent/#annotation-example","title":"Annotation example:","text":"<p>The <code>.annotation</code> module provides an interface to PEP annotations.  PEP annotations refers to the information about the PEPs (or, the PEP metadata).  Retrieved information contains: [number of samples, submission date, last update date, is private, PEP description, digest, namespace, name, tag]</p> <pre><code>```python\n# Get annotation of one project:\nagent.annotation.get(namespace, name, tag)\n\n# Get annotations of all projects from db:\nagent.annotation.get()\n\n# Get annotations of all projects within a given namespace:\nagent.annotation.get(namespace='namespace')\n\n# Search for a project with partial string matching, either within namespace or entire database\n# This returns a list of projects\nagent.annotation.get(query='query')\nagent.annotation.get(query='query', namespace='namespace')\n\n# Get annotation of multiple projects given a list of registry paths\nagent.annotation.get_by_rp([\"namespace1/project1:tag1\", \"namespace2/project2:tag2\"])\n\n# By default get function will return annotations for public projects,\n# To get annotation including private projects admin list should be provided.\n# admin list means list of namespaces where user has admin rights\n# For example:\nagent.annotation.get(query='search_pattern', admin=['databio', 'ncbi'])\n</code></pre>"},{"location":"pephub/developer/pepdbagent/#namespace","title":"Namespace","text":"<p>The <code>.namespace</code> module contains search namespace functionality that helps to find namespaces in database  and retrieve information: <code>number of samples</code>, <code>number of projects</code>.</p> <p>Example: <pre><code># Get info about namespace by providing query argument. Then pepdbagent will\n# search for a specified pattern of namespace in database.\nagent.namespace.get(query='Namespace')\n\n# By default all get function will return namespace information for public projects,\n# To get information with private projects, admin list should be provided.\n# admin list means list of namespaces where user has admin rights\n# For example:\nagent.namespace.get(query='search_pattern', admin=['databio', 'geo', 'ncbi'])\n</code></pre> For more information, developers should use <code>pepdbagent pytest</code> as documentation due to its natural language syntax and the  ability to write tests that serve as executable examples.  This approach not only provides detailed explanations but also ensures that code examples are kept  up-to-date with the latest changes in the codebase.</p>"},{"location":"pephub/developer/pepdbagent/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"pephub/developer/pepdbagent/changelog/#0111-2024-09-04","title":"[0.11.1] -- 2024-09-04","text":"<ul> <li>Added archive table of namespaces</li> <li>Added sort by stars</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#0110-2024-07-24","title":"[0.11.0] -- 2024-07-24","text":"<ul> <li>Added validation schemas</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#0100-2024-07-18","title":"[0.10.0] -- 2024-07-18","text":"<ul> <li>Added user delete method</li> <li>Added project history and restoring projects</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#090-2024-06-25","title":"[0.9.0] -- 2024-06-25","text":"<ul> <li>Introduced new sample ordering with linked list #133</li> <li>Efficiency improvements of project update function </li> <li>Test restructuring </li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#080-2024-02-26","title":"[0.8.0] -- 2024-02-26","text":"<ul> <li>Fixed forking schema</li> <li>Improved forking efficiency #129</li> <li>Added uploading project from dict </li> <li>Added get_config, get_samples, get_subsamples methods to project module #128</li> <li>Fixed error handling in views API #130</li> <li>Added no_fail to views API </li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#073-2024-02-08","title":"[0.7.3] -- 2024-02-08","text":"<ul> <li>Fixed POP update</li> <li>Improved error handling in views API</li> <li>Added stats method to Namespace module</li> <li>Updated docs</li> <li>Added coverage</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#072-2024-02-02","title":"[0.7.2] -- 2024-02-02","text":"<ul> <li>Fixed Unique Constraint in the Views</li> <li>Fixed update project pop method</li> <li>Fixed bug in duplicating samples</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#071-2024-01-22","title":"[0.7.1] -- 2024-01-22","text":"<ul> <li>Fixed bug in Stars annotation</li> <li>SQL efficiency improvements</li> <li>Added sort by date in stared projects</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#070-2024-01-17","title":"[0.7.0] -- 2024-01-17","text":"<ul> <li>Added <code>pop</code> to project table and annotation model #107</li> <li>Added <code>forked_from</code> feature #73</li> <li>Switched to pydantic2 #105</li> <li>Updated requirements (psycopg2 -&gt; psycopg3) #102</li> <li>Added sample module that contains functionality for updating, adding, and deleting samples in the project separately #111</li> <li>Added user and favorite tables with functionality #104</li> <li>Updated the sample update method when updating the whole project. Following this change, samples are updated without changing the ID in the database</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#060-2023-08-24","title":"[0.6.0] -- 2023-08-24","text":"<ul> <li>Added date filter to project annotation</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#055-2023-07-19","title":"[0.5.5] -- 2023-07-19","text":"<ul> <li>Updated requirements</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#054-2023-07-14","title":"[0.5.4] -- 2023-07-14","text":"<ul> <li>Added namespaces info method that returns list of namespaces with </li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#053-2023-07-13","title":"[0.5.3] -- 2023-07-13","text":"<ul> <li>Fixed bugs in updating dates and descriptions</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#052-2023-07-13","title":"[0.5.2] -- 2023-07-13","text":"<ul> <li>Fixed error in updating date in overwriting function</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#051-2023-07-10","title":"[0.5.1] -- 2023-07-10","text":"<ul> <li>Fixed errors in updating projects, that caused Unique Constraint violation error</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#050-2023-07-07","title":"[0.5.0] -- 2023-07-07","text":"<ul> <li>Introduced a new database schema (3 tables: projects, samples, subsamples)</li> <li>Added new tests</li> <li>Fixed the issue of sqlalchemy session already being closed</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#043-2023-06-30","title":"[0.4.3] -- 2023-06-30","text":"<ul> <li>Changed the orientation of the raw project dictionary to \"records\" and updated peppy version to 0.35.6.</li> <li>Added description column</li> <li>Added sql description search</li> <li>Deleted unused files</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#042-2023-06-27","title":"[0.4.2] -- 2023-06-27","text":"<ul> <li>Added validation of question mark in name and tag</li> <li>Fixed description and name in config file</li> <li>Fixed preserving order of columns in database</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#041-2023-06-09","title":"[0.4.1] -- 2023-06-09","text":"<ul> <li>Fixed project.update function</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#040-2023-06-09","title":"[0.4.0] -- 2023-06-09","text":"<ul> <li>Transitioned to SQLAlchemy ORM.</li> <li>Added a pep_schema column to the database.</li> <li>Implemented a new testing approach.</li> <li>Integrated sorting functionality into the annotations module.</li> <li>Temporarily disabled description-based searches to mitigate long processing times and reduce database load.</li> <li>Included timezone support in the database.</li> <li>Standardized namespace and names to lowercase for case-insensitivity.</li> <li>Streamlined the database schema creation process, with tables now created dynamically as needed.</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#031-2023-03-23","title":"[0.3.1] -- 2023-03-23","text":"<ul> <li>Fixed bug with peppy const dependencies</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#030-2023-01-19","title":"[0.3.0] -- 2023-01-19","text":"<ul> <li>Restructured pepdbagent: </li> <li>Renamed <code>Agent</code> class to <code>PEPDatabaseAgent</code></li> <li>created subclasses (project, annotation, namespace).</li> <li>Merged methods that have similar use case.</li> <li>Removed unused methods.</li> <li>Simplified pepdbagent API</li> <li>Restructured database (added columns: <code>private</code>, <code>number_of_samples</code>, <code>submission_date</code>, <code>last_update_date</code>)</li> <li>Improved logging</li> <li>Added new tests</li> <li>Improved documentation</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#023-2022-11-21","title":"[0.2.3] -- 2022-11-21","text":"<ul> <li>Added more logging information in uploading project methods</li> <li>Added <code>update_only</code> argument <code>upload_project</code> method</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#022-2022-11-09","title":"[0.2.2] -- 2022-11-09","text":"<ul> <li>Fixed error in pepannotation (privacy error)</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#021-2022-11-09","title":"[0.2.1] -- 2022-11-09","text":"<ul> <li>Fixed error with namespace user if user is unknown</li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#020-2022-11-08","title":"[0.2.0] -- 2022-11-08","text":"<ul> <li>Added limits and offset to function: <code>get_projects_in_namespace()</code></li> <li>Annotation changed to BaseModel from pydantic</li> <li>Added privacy information in annotations</li> <li>Added update feature to <code>upload_project</code> method with <code>overwrite</code> argument</li> <li>Added mock tests</li> <li>Deleted unused functions:</li> <li><code>get_projects_in_list</code></li> <li><code>get_projects_annotation_by_namespace</code></li> <li><code>project_status</code></li> <li><code>project_status_by_registry</code></li> <li> <p><code>get_registry_paths_by_digest</code></p> </li> <li> <p>Changed method names:</p> </li> <li><code>get_project_by_registry</code> --&gt; <code>get_project_by_registry_path</code></li> <li><code>get_project_annotation_by_registry</code> --&gt; <code>get_project_annotation_by_registry_path</code></li> </ul>"},{"location":"pephub/developer/pepdbagent/changelog/#010-2022-08-18","title":"[0.1.0] -- 2022-08-18","text":"<ul> <li>\ud83c\udf89 first release!</li> </ul>"},{"location":"pephub/developer/pepdbagent/database_version_migration/","title":"\ud83d\udd27 Database Version Migration and Upgrade Instructions","text":"<p>To change the database version smoothly and seamlessly, we use Alembic. This tool allows us to manage database schema changes in a consistent, version-controlled manner. As a result, users who have built their own PEPhub instances can easily upgrade to the latest version without losing  any data or needing to manually interact with the database.</p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#database-schema-change-for-database-developers","title":"\ud83d\udc77 Database schema change - for database developers","text":"<p>If you are changing database schema in <code>db_utils.py</code> file and schema in the database changed, you should  percied wiht the following steps:</p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#0-set-up-the-database-url-in-alembicini","title":"0. Set up the database URL in <code>alembic.ini</code>:","text":"<pre><code>sqlalchemy.url = postgresql+psycopg://user:password@localhost/dbname\n</code></pre> <p>By default, it is set to testing database. Credentials are in README.md in test folder of the repository.</p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#1-create-a-new-migration-script","title":"1. Create a new migration script:","text":"<p>When you modify your SQLAlchemy models, follow these steps to keep the database schema in sync:</p> <p>a. Modify Models: Update your SQLAlchemy models in your code. b. Generate Migration: <pre><code>alembic revision --autogenerate -m \"Describe your change\"\n</code></pre></p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#2-edit-the-migration-script","title":"2. Edit the migration script:","text":"<p>Open the newly created migration script and edit the <code>upgrade()</code> and <code>downgrade()</code> functions to define the changes you want to make to the database schema. - The <code>upgrade()</code> function should contain the code to apply the changes. - The <code>downgrade()</code> function should contain the code to revert the changes.</p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#3-apply-the-migration","title":"3. Apply the Migration","text":"<p>Run the migration to apply it to your database. Create small Python script with connection to the database  with <code>pepdbagent</code>, and parameter <code>run_migrations=True</code>:</p> <pre><code>from pepdbagent import PEPDatabaseAgent\n\ndb_agent = PEPDatabaseAgent(\n   host=\"localhost\",\n   port=5432,\n   database=\"pep-db\",\n   user=None,\n   password=None,\n   run_migrations=True,\n)\n</code></pre> <p>This will run all migrations of the database, including the one you just created.</p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#version-control","title":"Version Control","text":"<p>Each migration script has a unique identifier and tracks schema changes. Always commit these scripts to version control.</p>"},{"location":"pephub/developer/pepdbagent/database_version_migration/#database-schema-change-for-pephub-users","title":"\ud83e\uddd9\u200d\u2642\ufe0f Database schema change - for PEPhub users","text":"<p>If you are changing database schema in <code>db_utils.py</code> file and schema in the database changed, you should run the following script before connecting to the database:</p> <pre><code>from pepdbagent import PEPDatabaseAgent\n\ndb_agent = PEPDatabaseAgent(\n   host=\"localhost\",\n   port=5432,\n   database=\"pep-db\",\n   user=None,\n   password=None,\n   run_migrations=True,\n)\n</code></pre> <p>This will run all migrations of the database, including the one you just created.</p>"},{"location":"pephub/developer/pepdbagent/db_tutorial/","title":"Database setup","text":""},{"location":"pephub/developer/pepdbagent/db_tutorial/#container-installation","title":"Container installation:","text":"<p>0) Go to <code>pep_db</code>  directory and then run the following lines.  1) Build the docker:  <pre><code>docker build -t pep-db ./\n</code></pre> 2) Run the docker: <pre><code>docker run --name pep-db -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=docker -p 5432:5432 -d pep-db` \n</code></pre></p> <p>3) Start it: <pre><code>docker start pep-db\n</code></pre></p> <p>Now db is installed</p>"},{"location":"pephub/developer/pepdbagent/db_tutorial/#how-to-connect-to-the-docker-manually","title":"How to connect to the docker manually:","text":"<pre><code>docker exec -it 65f bash\n</code></pre> <pre><code>psql -U postgres -d pep-db\n</code></pre> <p>If you have your own database, you can initialize a connection using pepdbagent.  The pepdbagent will create a new database schema if it doesn't already exist, or throw an exception if the schema is incorrect.</p>"},{"location":"pephub/developer/pepembed/","title":"pepembed","text":""},{"location":"pephub/developer/pepembed/#overview","title":"Overview","text":"<p>PEPembed is a Python package for computing text-embeddings of sample metadata stored in pephub for search-and-retrieval tasks. It provides both a CLI and a Python API. It handles the long-running job of downloading projects inside pephub, mining any relevant metadata from them, computing a rich text embedding on that data, and finally upserting it into a vector database. We use qdrant as our vector database for its performance and simplicity and payload capabilities.</p> <p>Understand everything? Jump to running <code>pepembed</code>. Or view the quick start below.</p>"},{"location":"pephub/developer/pepembed/#quick-start","title":"Quick Start","text":"<pre><code>pip install .\n</code></pre> <pre><code>pepembed \\\n  --postgres-host $POSTGRES_HOST \\\n  --postgres-user $POSTGRES_USER \\\n  --postgres-password $POSTGRES_PASSWORD \\\n  --postgres-db $POSTGRES_DB \\\n</code></pre>"},{"location":"pephub/developer/pepembed/#architecture","title":"Architecture","text":"<p><code>pepembed</code> works in three steps: 1) Download PEPs from pephub, 2) Extract metadata from these PEPs and embeds them using a sentence transformer, and 3) inserts these PEPs into a qdrant instance.</p> <p>1. Download PEPs: <code>pepembed</code> downloads all PEPS from pephub. This is the most time-consuming process. Currently there is no way to parametrize this, but in the future we should. We should also allow for generating embeddings straight from files on disc.</p> <p>2. Extract Metadata from PEPs and embeddings: Once the PEPs are downloaded, we then extract any relevant metadata from them. This is done by looking for keywords in the project-level attributes. For each PEP, a pseudo-description is built by looking for these keywords and building a string. Some example keyword attributes might be: <code>cell_type</code>, <code>protocol</code>, <code>procedure</code>, <code>institution</code>, etc. You can specify your own keywords to <code>pepembed</code> if you wish.</p> <p> </p> <p>Once the pseudo-descriptions are mined, we can then utilize a <code>sentence-transformer</code> to generate low-dimensional representations of these descriptions. By default, we use a state-of-the-art transformer trained for the semantic textual similarity task (Reimers &amp; Gurevych, 2019). The embeddings are linked back to the original PEP registry path, along with other information like the mined pseudo-description and the row id in the database.</p> <p>3. Insert Embeddings: Finally, we insert the embeddings into a qdrant instance. qdrant is a vector database that is designed to store embeddings as first-class data types as well as supporting native graph-based indexing of these embeddings. The allows for near-instant search and retrieval of nearest embeddings neighbors given a new embedding (say an encoded search query on a web application). qdrant supports arming the embeddings with a payload where we store basic information on that PEP like registry path, row id, and its description.</p>"},{"location":"pephub/developer/pepembed/#install-and-run","title":"Install and Run","text":"<p>While simple to install and run, <code>pepembed</code> requires lots of information to function. There are three key aspects: 1) The pephub instance, 2) the qdrant instance, and 3) the keywords. Ensure the following before running the <code>cli</code>:</p>"},{"location":"pephub/developer/pepembed/#setup","title":"Setup","text":"<p>1. PEPhub instance: Make sure you have access to a running pephub instance store with peps. Once complete, you can use the following environment variables to tell <code>pepembed</code> where to get data. Alternatively, you can pass these as command-line args: * <code>POSTGRES_HOST</code> * <code>POSTGRES_DB</code> * <code>POSTGRES_USER</code> * <code>POSTGRES_PASSWORD</code></p> <p>2. Qdrant instance: In addition to a pephub instance, you will need a running instance of qdrant. It is quite simple and instructions can be found here. The TL;DR is:  </p> <pre><code>docker pull qdrant/qdrant\ndocker run -p 6333:6333 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n    qdrant/qdrant\n</code></pre> <p>This will give you a qdrant instance served at http://localhost:6333. You can pass this information to <code>pepembed</code> as environment variables. Alternatively, you may pass these as command-line args: * <code>QDRANT_HOST</code> * <code>QDRANT_PORT</code> * <code>QDRANT_API_KEY</code> * <code>QDRANT_COLLECTION_NAME</code></p> <p>Unless you are running this for production, you most likely do not need to specify any of these.</p> <p>3. Keywords: Finally, we need a keywords file. This is technically optional, and <code>pepembed</code> comes with default keywords, but you may supply your own as a plain text file. This can be supplied only as command-line args: * <code>KEYWORDS_FILE</code></p> <p>There are many other options as well (like specifying the transformer model to use), but the defaults work great for a first try. Use <code>pepembed --help</code> to see all options. If you are like me, and like to keep your secrets in a <code>.env</code> file, you can export them easily to the environment with <code>export $(cat .env | xargs)</code></p>"},{"location":"pephub/developer/pepembed/#install","title":"Install","text":"<p>Clone this repository and install with <code>pip</code>:</p> <pre><code>pip install .\n</code></pre>"},{"location":"pephub/developer/pepembed/#run","title":"Run","text":"<pre><code>pepembed \\\n  --keywords-file keywords.txt \\\n  --postgres-host $POSTGRES_HOST \\\n  --postgres-user $POSTGRES_USER \\\n  --postgres-password $POSTGRES_PASSWORD \\\n  --postgres-db $POSTGRES_DB \\\n</code></pre>"},{"location":"pephub/user/geo/","title":"Accessing GEO data through PEPhub","text":"<p>The Gene Expression Omnibus is a major source or biological sample data and metadata. However, accessing the metadata from GEO is challenging. Now, PEPhub provides an API-oriented access to processed tabular metadata from GEO.</p>"},{"location":"pephub/user/geo/#finding-geo-data-on-pephub","title":"Finding GEO data on PEPhub","text":"<p>Lots of options to find GEO metadata on PEPhub:</p> <ol> <li>You can browse or search GEO repositories from the GEO namespace, </li> <li>You can use the main PEPhub search interface.</li> <li>You can also just use the URL directly, of the form: <code>https://pephub.databio.org/geo/{gse_accession}</code> (with <code>gse</code> lowercase). For example: https://pephub.databio.org/geo/gse211892</li> </ol>"},{"location":"pephub/user/geo/#always-up-to-date","title":"Always up-to-date","text":"<p>PEPhub has a weekly update that keeps the PEPhub's GEO namespace in sync. So, you can be sure you're getting the latest metadata from PEPhub. You can think of PEPhub as a convenient mirror to GEO metadata. We are using geofetch to download any updated files, which processes the data into a more compact PEP sample table, which we then store in PEPhub.</p>"},{"location":"pephub/user/geo/#download-all-processed-data-from-geo","title":"Download all processed data from GEO","text":"<p>If you want to do a metadata analysis project that uses all the metadata from GEO, we also provide a tar archive. Just find the Download link on the GEO namespace page. This will provide processed PEPs of all GEO projects. </p> <p>If you are looking for the raw GEO metadata (not already processed into a PEP), then PEPhub can't really help; we process the data into PEP and discard the raw files, which are large. For most use cases, the processed PEP is a more convenient form. If you really need the raw SOFT files, there are two options:</p> <ul> <li>Use links to the files that are stored in the project sample table to download the data directly.</li> <li>Use geofetch yourself on a local machine to download these files. Example: <code>geofetch -i GSE95654 --processed</code>, where <code>--processed</code> indicates that you want to download processed data, not SRA. More information about PEP can be found on the  geofetch.</li> </ul>"},{"location":"pephub/user/getting-started/","title":"Getting started with PEPhub","text":""},{"location":"pephub/user/getting-started/#what-is-pephub","title":"What is PEPhub?","text":"<p>Just like GitHub allows you to share and edit projects that you are tracking with <code>git</code>, PEPhub allows you to share and edit biological metadata that is formatted as a PEP (a Portable Encapsulated Project). PEPhub allows you to:</p> <ul> <li>Upload your metadata to a central database</li> <li>Edit your metadata in a web interface</li> <li>Share your metadata with collaborators</li> <li>Validate your metadata using a schema</li> <li>Access and update metadata programmatically through an API, from Python or R.</li> </ul>"},{"location":"pephub/user/getting-started/#what-is-a-pep","title":"What is a PEP?","text":"<p>Portable Encapsulated Projects (PEPs) are standard format for biological sample metadata.  A PEP is simply a csv file representing a sample table, plus an optional YAML file for project-level metadata and sample modifiers.  For more details, read the PEP specification.</p> <p>PEPhub gives you a platform to store and collaborate on your PEPs. This makes it easier to work together for large or small teams. Instead of relying on local files that you send back-and-forth, PEPhub provides a centralized interface and API that simplifies sharing and collaboration.</p>"},{"location":"pephub/user/getting-started/#your-first-pep-on-pephub","title":"Your first PEP on PEPhub","text":""},{"location":"pephub/user/getting-started/#logging-in","title":"Logging in","text":"<p>You log in to PEPhub using your GitHub account. Just click the \"Log in\" button in the top right corner of the PEPhub home page. </p> <p></p> <p>You will be redirected to GitHub to authorize the PEPhub application. You are now logged in and can upload your first PEP! There are two mains ways to add a PEP to your PEPhub namespace: you can either upload a PEP directly, or you can create a new PEP from scratch using the web interface. This guide will walk you through both methods.</p>"},{"location":"pephub/user/getting-started/#uploading-a-pep","title":"Uploading a PEP","text":"<p>Navigate to your PEPhub namespace (<code>https://pephub.databio.org/{github username}</code>) and click the \"Add\" button in the top right. Click the \"Upload PEP\" tab. You will be prompted to select a PEP file from your local machine. Fill in the details about your PEP and then either drag files to the drop zone or click the drop zone to select files from your computer. Click \"Submit\" to add the PEP to your namespace.</p>"},{"location":"pephub/user/getting-started/#creating-a-new-pep-from-scratch","title":"Creating a new PEP from scratch","text":"<p>Navigate to your PEPhub namespace (<code>https://pephub.databio.org/{github username}</code>) and click the \"Add\" button in the top right. Click the \"Blank PEP\" tab. Again, fill in the details about your PEP and then you can start filling in the sample table. Click \"Submit\" to add the PEP to your namespace.</p> <p></p>"},{"location":"pephub/user/getting-started/#editing-a-pep","title":"Editing a PEP","text":"<p>Once you have uploaded or created a PEP, you can edit it. Editing a PEP is easy; just make changes in the table and click <code>Save</code> when you are finished.</p>"},{"location":"pephub/user/getting-started/#sharing-your-pep","title":"Sharing your PEP","text":"<p>By default, new PEPs are set to public access. This means anyone who has the link can read your table. You can share it by simply sharing the URL. You can restrict this by marking it as <code>private</code> in the Project edit menu.</p> <p>If you want to give another user write access, then you'll need to do a bit more work. Since PEPhub user permissions are inherited directly from GitHub, you'll use GitHub to manage these. You should transfer the PEP into an organization where both users are public members. This will give both users write access to the PEP.</p>"},{"location":"pephub/user/organization/","title":"PEPhub organizations","text":"<p>PEPhub allows users to create and manage PEPs within an organization. PEPhub organizations are linked to GitHub  organizations, and all members of a linked GitHub organization are designated as admins for the projects in the  corresponding PEPhub organization. By default, all projects in a PEPhub organization are public. However, admins  can make projects private, restricting visibility to members of the organization.</p>"},{"location":"pephub/user/organization/#logging-in","title":"Logging in","text":"<p>When you log in to PEPhub through GitHub, all your organizations are fetched, and you are automatically added as an admin.</p> <p>Warning!</p> <p>PEPhub currently does not support GitHub user organizations that are set to private (hidden) in the user settings.</p> <p>Set GitHub org Membership Visibility</p> <ol> <li>Go to the GitHub page.</li> <li>Navigate to the organization\u2019s page. (e.g. https://github.com/databio)</li> <li>Click People (in the organization's menu bar).</li> <li>Locate your username and ensure your membership is public.</li> </ol>"},{"location":"pephub/user/pipelines/","title":"Using your PEPs in a pipeline","text":"<p>Learn how you can seamlessly integrate your PEPs into your workflows.</p> <p>PEPs are a powerful tool for managing your workflows and pipelines. By using PEPhub, you can easily share your PEPs with collaborators and integrate them into your workflows. This guide will walk you through how to use your PEPs in a basic pipeline.</p>"},{"location":"pephub/user/pipelines/#using-a-pep-in-a-pipeline","title":"Using a PEP in a pipeline","text":"<p>Lets use <code>python</code> to grab out project samples, and iterate over them, running a simple command on each sample. We will use the <code>requests</code> library to interact with the PEPhub API. Imagine we have a PEP with the sample table below:</p> sample_name file_path genome sample1 /path/to/sample1.bam hg38 sample2 /path/to/sample2.bam hg19 sample3 /path/to/sample3.bam mm10 <pre><code>import subprocess\nimport requests\n\n# Get the PEP from PEPhub\npep_url = \"https://pephub-api.databio.org/api/v1/projects/{github username}/{pep name}/samples?tag=default\"\npep = requests.get(pep_url).json()\n\n# Iterate over the samples in the PEP\nfor sample in pep['items']:\n    genome = sample['genome']\n    file_path = sample['file_path']\n\n    subprocess.run(f\"gatk BaseRecalibrator -I {file_path} -R /path/to/{genome}.fa\")\n</code></pre> <p>This is a simple example, but you can see how you can use the PEP to manage your samples and easily integrate them into your workflows. You can use the PEPhub API to get the PEP and then iterate over the samples to run your commands.</p>"},{"location":"pephub/user/pops/","title":"How to organize groups of PEPs","text":""},{"location":"pephub/user/pops/#how-can-i-organize-peps-into-groups","title":"How can I organize PEPs into groups?","text":"<p>We call a group of PEPs a POP -- it stands for PEP of PEPs. Since a PEP is fundamentally a table, and we make a table where each row is a link to another table. So, that's a table of tables -- or a PEP of PEPs, which we call a POP for short.</p> <p>A POP is simply a group of PEPs represented as a PEP. Users can think of it as a group, collection, or folder of projects.  It groups PEPs together to represent similar projects, analyses, or related information, making it easier to manage and navigate related proposals.</p> <p>Each POP can contain other PEPs or POPs (since a POP is a PEP), allowing for a recursive structure.  This recursive nature enables complex hierarchies and relationships to be efficiently organized within the PEP framework.</p>"},{"location":"pephub/user/pops/#how-to-use-pops","title":"How to use POPs","text":"<p>A POP is actually just a PEP, that has a special flag set. You can create a new POP in the typical Add PEP interface; just select the POP tab.</p> <p></p> <p>If you already have a PEP and you want to designate it as a POP, just make sure it has at least these 3 columns: <code>namespace</code>, <code>name</code>, and <code>tag</code>, which correspond to the registry path for the PEP. You can add whatever other columns you want. Then, just toggle the POP toggle button in the settings interface for the PEP:</p> <p></p>"},{"location":"pephub/user/pops/#views","title":"Views","text":"<p>There are two ways to view a POP. First, since a POP is a PEP, you can just use the regular table view, the same as you would any PEP.  Second, you can use a special \"POP\" view, which just shows you a linked list of the PEPs contained in the pop. Toggle between the two views using the More context menu, which provides a View as POP or View as PEP options for any PEP that has the POP toggle checked.</p> <p></p>"},{"location":"pephub/user/semantic-search/","title":"PEPhub's semantic search","text":"<p>PEPhub's main search box (accessible from the home page) provides a powerful semantic search.</p> <p>When a user provides a natural-language search query, PEPhub transforms the query using the same-sentence transformer in real time, then queries the Qdrant API to retrieve the most semantically similar PEP vectors. Qdrant identifies similar PEPs by calculating nearest neighbors in vector space. PEPhub then returns the results to the client with their associated description and registry path. PEPhub\u2019s search engine uses a semantic approach, which provides several advantages: first, the system returns results with similar meaning whether or not they include the terms of the original query. Second, it is tolerant of misspellings and is not limited to any ontology or taxonomy. Finally, because each PEP is represented as a vector, we can use high-speed nearest-neighbor algorithms to identify relevant PEPs, making the search very fast. This method scales to millions of PEPs, and the speed is limited only by network speeds. Users may also tune results with limits, offsets, and relevance score cutoffs.</p>"},{"location":"pephub/user/validation/","title":"How to validate sample metadata","text":"<p>PEPhub validates sample metadata with eido. Schemas can be added and edited on PEPhub directly.</p> <p>Schemas are particularly useful before running pipelines, as validation provides essential information about PEP compatibility with specific pipelines and highlights any errors in the PEP structure.</p> <p>There are two ways to use the interfaced to validate PEPs: From the main PEP interface, or from the universal validator.</p>"},{"location":"pephub/user/validation/#validating-a-pep-from-the-main-pep-interface","title":"Validating a PEP from the main PEP interface","text":"<p>If you're editing a PEP, it's convenient to be able to validate it from the same interface. First, assign a schema to the PEP, and then validation will happen automatically, whenever you save the project.</p>"},{"location":"pephub/user/validation/#assign-a-schema-to-a-pep","title":"Assign a schema to a PEP","text":"<p>From the main table view, use the Edit menu to access the properties for a PEP:</p> <p></p> <p>In this interface, you can select a schema for this PEP.</p>"},{"location":"pephub/user/validation/#validating","title":"Validating","text":"<p>Once a schema is assigned you'll see the validation results:</p> <p></p> <p>If you click on this notice, you'll see more detailed information about what in the table is causing the validation to fail. This will allow you to validate metadata in real time, as you work on the table.</p>"},{"location":"pephub/user/validation/#using-the-universal-validator","title":"Using the universal validator","text":"<p>Alternatively, for a more flexible approach, you can use the Universal Validator. This provides a 2-step interface where you first provide a PEP, either by selecting one from PEPhub or by uploading it, and then a schema, which can be either selected from PEPhub, uploaded, or pasted.</p>"},{"location":"pephub/user/version-control/","title":"How to version control metadata with PEPhub","text":"<p>PEPhub table versions happen through two features: 1) history; and 2) tags.</p>"},{"location":"pephub/user/version-control/#history","title":"History","text":"<p>PEPhub automatically records a history of your files whenever changes are made. Any time you click \"save\", an entry is added to your history. You can view the history of table edits by selecting the <code>History</code> option from the <code>More</code> menu.</p> <p></p> <p>Selecting this option will bring up the History Interface, which will provide buttons allowing you to view or delete entries from your history table. If you choose the <code>View</code> button for an entry, it will show you the PEP at that point in history. It also opens a new interface that will allow you to click <code>Restore</code> to overwrite your current PEP with the historical version you are currently viewing, or you can <code>Download</code> the table as it was at that point in history.</p> <p></p> <p>In PEPhub, old versions are kept automatically, and they are referenced by date. PEPhub does not automatically assign version numbers or other identifiers; the only way to identify the old versions is by timestamp.</p>"},{"location":"pephub/user/version-control/#history-retention-policy","title":"History retention policy","text":"<p>Old versions of sample tables are kept for 30 days. Once a history entry is more than 30 days old, it will be automatically purged. If you want to keep an old version for longer, then you will need to manually tag the version, thereby forking it into a new repository.</p>"},{"location":"pephub/user/version-control/#tags","title":"Tags","text":"<p>The other versioning feature offered by PEPhub is to use tags. PEPhub tags are unique identifiers of repositories. Every repository has a tag. By default, the tag is simply default. The registry path of each PEP takes the form of:</p> <pre><code>{namespace}/{repository}:{tag}\n</code></pre> <p>For example, <code>nsheff/my_new_pep:v1</code> would be the <code>my_new_pep</code> repository in my user namespace (<code>nsheff</code>), and <code>v1</code> is the tag. You can use tags to version your own PEPs. When you're ready to declare a version, just fork the current PEP into a new PEP and name the version tag accordingly.</p>"},{"location":"pephub/user/views/","title":"How to use PEPhub views","text":""},{"location":"pephub/user/views/#what-are-views","title":"What are views?","text":"<p>Large tables (e.g. &gt;5,000 rows) can be unwieldy with PEPhup. It can be hard to find the elements you're looking for. To address this, PEPhub provides the Views feature. Views provide a way to look at a subset of a large table (basically, a filtered table).</p>"},{"location":"pephub/user/views/#how-to-create-a-view","title":"How to create a view","text":"<p>To create a new view, click the Down Arrow to access the filter menu, and set up a filter. This will change the table to display a subset of the rows.</p> <p></p> <p>Then, you can use the View Settings menu (gear icon next to the view selector) to open the Views interface.</p> <p></p> <p>This will allow you to save the view. You can then select it any time from the views menu.</p>"},{"location":"pephub/user/views/#read-only-limitation","title":"Read-only limitation","text":"<p>Views are currently read-only; you will not be able to make edits to the table while viewing a subset. We hope to remove this restriction in the future.</p>"},{"location":"pephub/user/pephubclient/","title":"PEPhubClient","text":""},{"location":"pephub/user/pephubclient/#pephubclient","title":"PEPHubClient","text":"<p><code>PEPHubClient</code> is a CLI and Python API for PEPhub. Key features are:</p> <ul> <li>Download: Users can download public PEPs via CLI or Python API.</li> <li>Authorization: Users can log in to PEPhub using PEPHubClient via CLI, providing download access to private projects.</li> <li>Upload: Authenticated users can also upload PEPs to PEPhub.</li> </ul> <p>PEPHubClient uses PEPhub's device authorization protocol. To upload projects or to download private projects, user must be authorized through PEPhub.</p>"},{"location":"pephub/user/pephubclient/#installation","title":"Installation","text":"<p>To install PEPHubClient from PyPI, use the following command:</p> <pre><code>pip install pephubclient\n</code></pre> <p>To install <code>pephubclient</code> from the GitHub repository, use the following command:</p> <pre><code>pip install git+https://github.com/pepkit/pephubclient.git\n</code></pre>"},{"location":"pephub/user/pephubclient/#how-to-specify-url-for-pephub-instance","title":"How to specify URL for PEPhub instance","text":"<p>If you want to use your own PEPhub instance, you can specify it by setting the <code>PEPHUB_BASE_URL</code> environment variable. e.g. </p> <pre><code>export PEPHUB_BASE_URL=http://localhost:8000/\n</code></pre>"},{"location":"pephub/user/pephubclient/#authentication","title":"Authentication","text":"<p>To login, use the <code>login</code> command:</p> <pre><code>phc login\n</code></pre> <p>To logout, use <code>logout</code>:</p> <pre><code>phc logout\n</code></pre>"},{"location":"pephub/user/pephubclient/#example","title":"Example","text":"<pre><code>$ phc --help\n\n Usage: pephubclient [OPTIONS] COMMAND [ARGS]...                                                                   \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version             -v                                                                                        \u2502\n\u2502 --install-completion            Install completion for the current shell.                                       \u2502\n\u2502 --show-completion               Show completion for the current shell, to copy it or customize the              \u2502\n\u2502                                 installation.                                                                   \u2502\n\u2502 --help                          Show this message and exit.                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 login               Login to PEPhub                                                                             \u2502\n\u2502 logout              Logout                                                                                      \u2502\n\u2502 pull                Download and save project locally.                                                          \u2502\n\u2502 push                Upload/update project in PEPhub                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>$ phc pull --help\n\n Usage: pephubclient pull [OPTIONS] PROJECT_REGISTRY_PATH                                                          \n\n Download and save project locally.                                                                                \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    project_registry_path      TEXT  [default: None] [required]                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --force    --no-force      Overwrite project if it exists. [default: no-force]                                  \u2502\n\u2502 --help                     Show this message and exit.                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>$ phc push --help\n\n Usage: pephubclient push [OPTIONS] CFG                                                                            \n\n Upload/update project in PEPhub                                                                                   \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    cfg      TEXT  Project config file (YAML) or sample table (CSV/TSV)with one row per sample to constitute   \u2502\n\u2502                     project                                                                                     \u2502\n\u2502                     [default: None]                                                                             \u2502\n\u2502                     [required]                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --namespace                        TEXT  Project namespace [default: None] [required]                        \u2502\n\u2502 *  --name                             TEXT  Project name [default: None] [required]                             \u2502\n\u2502    --tag                              TEXT  Project tag [default: None]                                         \u2502\n\u2502    --force         --no-force               Force push to the database. Use it to update, or upload project.    \u2502\n\u2502                                             [default: no-force]                                                 \u2502\n\u2502    --is-private    --no-is-private          Upload project as private. [default: no-is-private]                 \u2502\n\u2502    --help                                   Show this message and exit.                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"pephub/user/pephubclient/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"pephub/user/pephubclient/changelog/#044-2024-08-21","title":"[0.4.4] - 2024-08-21","text":""},{"location":"pephub/user/pephubclient/changelog/#fixed","title":"Fixed","text":"<ul> <li>Project annotation model</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#043-2024-07-19","title":"[0.4.3] - 2024-07-19","text":""},{"location":"pephub/user/pephubclient/changelog/#updated","title":"Updated","text":"<ul> <li>Updated models for new PEPhub API</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#042-2024-04-16","title":"[0.4.2] - 2024-04-16","text":""},{"location":"pephub/user/pephubclient/changelog/#updated_1","title":"Updated","text":"<ul> <li>View creation, by adding description and no_fail flag</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#041-2024-03-07","title":"[0.4.1] - 2024-03-07","text":""},{"location":"pephub/user/pephubclient/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Expired token error handling  (#17)</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#040-2024-02-12","title":"[0.4.0] - 2024-02-12","text":""},{"location":"pephub/user/pephubclient/changelog/#added","title":"Added","text":"<ul> <li>a parameter that points to where peps should be saved (#32)</li> <li>pep zipping option to <code>save_pep</code> function (#34)</li> <li>API for samples (#29)</li> <li>API for projects (#28)</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#updated_2","title":"Updated","text":"<ul> <li>Transferred <code>save_pep</code> function to helpers</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#030-2024-01-17","title":"[0.3.0] - 2024-01-17","text":""},{"location":"pephub/user/pephubclient/changelog/#added_1","title":"Added","text":"<ul> <li>customization of the base PEPhub URL (#22)</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#updated_3","title":"Updated","text":"<ul> <li>Updated PEPhub API URL</li> <li>Increased the required pydantic version to &gt;2.5.0</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#022-2024-01-17","title":"[0.2.2] - 2024-01-17","text":""},{"location":"pephub/user/pephubclient/changelog/#added_2","title":"Added","text":"<ul> <li>customization of the base pephub URL. #22</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#updated_4","title":"Updated","text":"<ul> <li>PEPhub API URL</li> <li>Increased the required pydantic version to &gt;2.5.0</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#021-2023-11-01","title":"[0.2.1] - 2023-11-01","text":""},{"location":"pephub/user/pephubclient/changelog/#added_3","title":"Added","text":"<ul> <li>is_registry_path checker function</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#020-2023-10-02","title":"[0.2.0] - 2023-10-02","text":""},{"location":"pephub/user/pephubclient/changelog/#added_4","title":"Added","text":"<ul> <li>Project search functionality</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#011-2023-07-29","title":"[0.1.1] - 2023-07-29","text":""},{"location":"pephub/user/pephubclient/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Incorrect base url</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#011-2023-07-29_1","title":"[0.1.1] - 2023-07-29","text":""},{"location":"pephub/user/pephubclient/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>New raw PEP structure was broken. (#20)</li> </ul>"},{"location":"pephub/user/pephubclient/changelog/#010-2023-04-16","title":"[0.1.0] - 2023-04-16","text":""},{"location":"pephub/user/pephubclient/changelog/#added_5","title":"Added","text":"<ul> <li>First release</li> </ul>"},{"location":"pephub/user/pephubclient/cli/","title":"PEPHubClient CLI (phc)","text":"<p>Installing PEPHubClient provides a CLI through the <code>phc</code> command. It provides a set of commands to interact with PEPhub.</p> <pre><code>$ phc --help\n\n Usage: pephubclient [OPTIONS] COMMAND [ARGS]...                                                                   \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version             -v                                                                                        \u2502\n\u2502 --install-completion            Install completion for the current shell.                                       \u2502\n\u2502 --show-completion               Show completion for the current shell, to copy it or customize the              \u2502\n\u2502                                 installation.                                                                   \u2502\n\u2502 --help                          Show this message and exit.                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 login               Login to PEPhub                                                                             \u2502\n\u2502 logout              Logout                                                                                      \u2502\n\u2502 pull                Download and save project locally.                                                          \u2502\n\u2502 push                Upload/update project in PEPhub                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>$ phc pull --help\n\n Usage: pephubclient pull [OPTIONS] PROJECT_REGISTRY_PATH                                                          \n\n Download and save project locally.                                                                                \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    project_registry_path      TEXT  [default: None] [required]                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --force    --no-force      Overwrite project if it exists. [default: no-force]                                  \u2502\n\u2502 --help                     Show this message and exit.                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>$ phc push --help\n\n Usage: pephubclient push [OPTIONS] CFG                                                                            \n\n Upload/update project in PEPhub                                                                                   \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    cfg      TEXT  Project config file (YAML) or sample table (CSV/TSV)with one row per sample to constitute   \u2502\n\u2502                     project                                                                                     \u2502\n\u2502                     [default: None]                                                                             \u2502\n\u2502                     [required]                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --namespace                        TEXT  Project namespace [default: None] [required]                        \u2502\n\u2502 *  --name                             TEXT  Project name [default: None] [required]                             \u2502\n\u2502    --tag                              TEXT  Project tag [default: None]                                         \u2502\n\u2502    --force         --no-force               Force push to the database. Use it to update, or upload project.    \u2502\n\u2502                                             [default: no-force]                                                 \u2502\n\u2502    --is-private    --no-is-private          Upload project as private. [default: no-is-private]                 \u2502\n\u2502    --help                                   Show this message and exit.                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"pephub/user/pephubclient/phc_samples_usage/","title":"Python API samples","text":""},{"location":"pephub/user/pephubclient/phc_samples_usage/#module-pephubclientmodulessample","title":"module <code>pephubclient.modules.sample</code>","text":""},{"location":"pephub/user/pephubclient/phc_samples_usage/#global-variables","title":"Global Variables","text":"<ul> <li>PEPHUB_SAMPLE_URL</li> </ul>"},{"location":"pephub/user/pephubclient/phc_samples_usage/#class-pephubsample","title":"class <code>PEPHubSample</code>","text":"<p>Class for managing samples in PEPhub and provides methods for  getting, creating, updating and removing samples. This class is not related to peppy.Sample class. </p>"},{"location":"pephub/user/pephubclient/phc_samples_usage/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(jwt_data: str = None)\n</code></pre> <ul> <li>:param jwt_data: jwt token for authorization </li> </ul>"},{"location":"pephub/user/pephubclient/phc_samples_usage/#method-create","title":"method <code>create</code>","text":"<pre><code>create(\n    namespace: str,\n    name: str,\n    tag: str,\n    sample_name: str,\n    sample_dict: dict,\n    overwrite: bool = False\n) \u2192 None\n</code></pre> <p>Create sample in project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param sample_dict: sample dict </li> <li>:param sample_name: sample name </li> <li>:param overwrite: overwrite sample if it exists :return: None </li> </ul>"},{"location":"pephub/user/pephubclient/phc_samples_usage/#method-get","title":"method <code>get</code>","text":"<pre><code>get(namespace: str, name: str, tag: str, sample_name: str = None) \u2192 dict\n</code></pre> <p>Get sample from project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param sample_name: sample name :return: Sample object </li> </ul>"},{"location":"pephub/user/pephubclient/phc_samples_usage/#method-remove","title":"method <code>remove</code>","text":"<pre><code>remove(namespace: str, name: str, tag: str, sample_name: str)\n</code></pre> <p>Remove sample from project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param sample_name: sample name :return: None </li> </ul>"},{"location":"pephub/user/pephubclient/phc_samples_usage/#method-update","title":"method <code>update</code>","text":"<pre><code>update(namespace: str, name: str, tag: str, sample_name: str, sample_dict: dict)\n</code></pre> <p>Update sample in project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param sample_name: sample name </li> <li>:param sample_dict: sample dict, that contain elements to update, or :return: None </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"pephub/user/pephubclient/phc_schemas/","title":"Python API schemas","text":""},{"location":"pephub/user/pephubclient/phc_schemas/#module-schemapy","title":"module <code>schema.py</code>","text":""},{"location":"pephub/user/pephubclient/phc_schemas/#global-variables","title":"Global Variables","text":"<ul> <li>PEPHUB_SCHEMA_VERSION_URL</li> <li>PEPHUB_SCHEMA_VERSIONS_URL</li> <li>PEPHUB_SCHEMA_NEW_SCHEMA_URL</li> <li>PEPHUB_SCHEMA_NEW_VERSION_URL</li> <li>PEPHUB_SCHEMA_RECORD_URL</li> <li>LATEST_VERSION</li> </ul>"},{"location":"pephub/user/pephubclient/phc_schemas/#class-pephubschema","title":"class <code>PEPHubSchema</code>","text":"<p>Class for managing schemas in PEPhub and provides methods for  getting, creating, updating and removing schemas records and schema versions. </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-__init__","title":"function <code>__init__</code>","text":"<pre><code>__init__(jwt_data: str = None)\n</code></pre> <ul> <li>:param jwt_data: jwt token for authorization </li> </ul>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-add_version","title":"function <code>add_version</code>","text":"<pre><code>add_version(\n    namespace: str,\n    schema_name: str,\n    schema_value: dict,\n    version: str = '1.0.0',\n    contributors: str = None,\n    release_notes: str = None,\n    tags: Optional[str, List[str], dict] = None\n) \u2192 None\n</code></pre> <p>Add new version to the schema registry </p> <ul> <li>:param namespace: Namespace of the schema </li> <li>:param schema_name: Name of the schema record </li> <li>:param schema_value: Schema value itself in dict format </li> <li>:param version: First version of the schema </li> <li>:param contributors: Schema contributors of current version </li> <li>:param release_notes: Release notes for current version </li> <li>:param tags: Tags of the current version. Can be str, list[str], or dict </li> </ul> <p>:raise: ResponseError if status not 202. :return: None </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-create_schema","title":"function <code>create_schema</code>","text":"<pre><code>create_schema(\n    namespace: str,\n    schema_name: str,\n    schema_value: dict,\n    version: str = '1.0.0',\n    description: str = None,\n    maintainers: str = None,\n    contributors: str = None,\n    release_notes: str = None,\n    tags: Optional[str, List[str], dict] = None,\n    lifecycle_stage: str = None,\n    private: bool = False\n) \u2192 None\n</code></pre> <p>Create a new schema record + version in the database </p> <ul> <li>:param namespace: Namespace of the schema </li> <li>:param schema_name: Name of the schema record </li> <li>:param schema_value: Schema value itself in dict format </li> <li>:param version: First version of the schema </li> <li>:param description: Schema description </li> <li>:param maintainers: Schema maintainers </li> <li>:param contributors: Schema contributors of current version </li> <li>:param release_notes: Release notes for current version </li> <li>:param tags: Tags of the current version. Can be str, list[str], or dict </li> <li>:param lifecycle_stage: Stage of the schema record </li> <li>:param private: Weather project should be public or private. Default: False (public) </li> </ul> <p>:raise: ResponseError if status not 202. :return: None </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-delete_schema","title":"function <code>delete_schema</code>","text":"<pre><code>delete_schema(namespace: str, schema_name: str) \u2192 None\n</code></pre> <p>Delete schema from the database </p> <ul> <li>:param namespace: Namespace of the schema </li> <li>:param schema_name: Name of the schema version </li> </ul>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-delete_version","title":"function <code>delete_version</code>","text":"<pre><code>delete_version(namespace: str, schema_name: str, version: str) \u2192 None\n</code></pre> <p>Delete schema Version </p> <ul> <li>:param namespace: Namespace of the schema </li> <li>:param schema_name: Name of the schema </li> <li>:param version: Schema version </li> </ul> <p>:raise: ResponseError if status not 202. :return: None </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-get","title":"function <code>get</code>","text":"<pre><code>get(namespace: str, schema_name: str, version: str = 'latest') \u2192 dict\n</code></pre> <p>Get schema value for specific schema version. </p> <ul> <li>:param: namespace: namespace of schema </li> <li>:param: schema_name: name of schema </li> <li>:param: version: version of schema </li> </ul> <p>:return: Schema object as dictionary </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-get_versions","title":"function <code>get_versions</code>","text":"<pre><code>get_versions(namespace: str, schema_name: str) \u2192 SchemaVersionResult\n</code></pre> <p>Get list of versions </p> <ul> <li>:param namespace: Namespace of the schema record </li> <li>:param schema_name: Name of the schema record </li> </ul> <p>:return: {  pagination: PaginationResult  results: List[SchemaVersionAnnotation] } </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-update_record","title":"function <code>update_record</code>","text":"<pre><code>update_record(\n    namespace: str,\n    schema_name: str,\n    update_fields: Union[dict, UpdateSchemaRecordFields]\n) \u2192 None\n</code></pre> <p>Update schema registry data </p> <ul> <li>:param namespace: Namespace of the schema </li> <li>:param schema_name: Name of the schema version </li> <li>:param update_fields: dict or pydantic model UpdateSchemaRecordFields:  {  maintainers: str,  lifecycle_stage: str,  private: bool,  name: str,  description: str,  } </li> </ul> <p>:raise: ResponseError if status not 202. :return: None </p>"},{"location":"pephub/user/pephubclient/phc_schemas/#function-update_version","title":"function <code>update_version</code>","text":"<pre><code>update_version(\n    namespace: str,\n    schema_name: str,\n    version: str,\n    update_fields: Union[dict, UpdateSchemaVersionFields]\n) \u2192 None\n</code></pre> <p>Update released version of the schema. </p> <ul> <li> <p>:param namespace: Namespace of the schema </p> </li> <li> <p>:param schema_name: Name of the schema version </p> </li> <li>:param version: Schema version </li> <li>:param update_fields: dict or pydantic model UpdateSchemaVersionFields:  {  contributors: str,  schema_value: str,  release_notes: str,  } </li> </ul> <p>:raise: ResponseError if status not 202. :return: None </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"pephub/user/pephubclient/phc_usage/","title":"Python API","text":""},{"location":"pephub/user/pephubclient/phc_usage/#module-pephubclientpephubclient","title":"module <code>pephubclient.pephubclient</code>","text":""},{"location":"pephub/user/pephubclient/phc_usage/#class-pephubclient","title":"class <code>PEPHubClient</code>","text":""},{"location":"pephub/user/pephubclient/phc_usage/#property-sample","title":"property sample","text":"<p>view represents the PHCSample class which contains all samples API</p>"},{"location":"pephub/user/pephubclient/phc_usage/#property-view","title":"property view","text":"<p>view represents the PHCView class which contains all views API</p>"},{"location":"pephub/user/pephubclient/phc_usage/#method-find_project","title":"method <code>find_project</code>","text":"<pre><code>find_project(\n    namespace: str,\n    query_string: str = '',\n    limit: int = 100,\n    offset: int = 0,\n    filter_by: Literal['submission_date', 'last_update_date'] = None,\n    start_date: str = None,\n    end_date: str = None\n) \u2192 SearchReturnModel\n</code></pre> <p>Find project in specific namespace and return list of PEP annotation </p> <ul> <li>:param namespace: Namespace where to search for projects</li> <li>:param query_string: Search query</li> <li>:param limit: Return limit</li> <li>:param offset: Return offset</li> <li>:param filter_by: Use filter date. Option: [submission_date, last_update_date]</li> <li>:param start_date: filter beginning date</li> <li>:param end_date: filter end date (if none today's date is used) :return: </li> </ul>"},{"location":"pephub/user/pephubclient/phc_usage/#method-load_project","title":"method <code>load_project</code>","text":"<pre><code>load_project(\n    project_registry_path: str,\n    query_param: Optional[dict] = None\n) \u2192 Project\n</code></pre> <p>Load peppy project from PEPhub in peppy.Project object </p> <ul> <li> <p>:param project_registry_path: registry path of the project </p> </li> <li> <p>:param query_param: query parameters used in get request :return Project: peppy project. </p> </li> </ul>"},{"location":"pephub/user/pephubclient/phc_usage/#method-load_raw_pep","title":"method <code>load_raw_pep</code>","text":"<pre><code>load_raw_pep(registry_path: str, query_param: Optional[dict] = None) \u2192 dict\n</code></pre> <p>Request PEPhub and return the requested project as peppy.Project object. </p> <ul> <li>:param registry_path: Project namespace, eg. \"geo/GSE124224:tag\" </li> <li>:param query_param: Optional variables to be passed to PEPhub :return: Raw project in dict. </li> </ul>"},{"location":"pephub/user/pephubclient/phc_usage/#method-login","title":"method <code>login</code>","text":"<pre><code>login() \u2192 NoReturn\n</code></pre> <p>Log in to PEPhub </p>"},{"location":"pephub/user/pephubclient/phc_usage/#method-logout","title":"method <code>logout</code>","text":"<pre><code>logout() \u2192 NoReturn\n</code></pre> <p>Log out from PEPhub </p>"},{"location":"pephub/user/pephubclient/phc_usage/#method-pull","title":"method <code>pull</code>","text":"<pre><code>pull(\n    project_registry_path: str,\n    force: Optional[bool] = False,\n    zip: Optional[bool] = False,\n    output: Optional[str] = None\n) \u2192 None\n</code></pre> <p>Download project locally </p> <ul> <li>:param str project_registry_path: Project registry path in PEPhub (e.g. databio/base:default) </li> <li>:param bool force: if project exists, overwrite it. </li> <li>:param bool zip: if True, save project as zip file </li> <li>:param str output: path where project will be saved :return: None </li> </ul>"},{"location":"pephub/user/pephubclient/phc_usage/#method-push","title":"method <code>push</code>","text":"<pre><code>push(\n    cfg: str,\n    namespace: str,\n    name: Optional[str] = None,\n    tag: Optional[str] = None,\n    is_private: Optional[bool] = False,\n    force: Optional[bool] = False\n) \u2192 None\n</code></pre> <p>Push (upload/update) project to Pephub using config/csv path </p> <ul> <li>:param str cfg: Project config file (YAML) or sample table (CSV/TSV)  with one row per sample to constitute project </li> <li>:param str namespace: namespace </li> <li>:param str name: project name </li> <li>:param str tag: project tag </li> <li>:param bool is_private: Specifies whether project should be private [Default= False] </li> <li>:param bool force: Force push to the database. Use it to update, or upload project. [Default= False] :return: None </li> </ul>"},{"location":"pephub/user/pephubclient/phc_usage/#method-upload","title":"method <code>upload</code>","text":"<pre><code>upload(\n    project: Project,\n    namespace: str,\n    name: str = None,\n    tag: str = None,\n    is_private: bool = False,\n    force: bool = True\n) \u2192 None\n</code></pre> <p>Upload peppy project to the PEPhub. </p> <ul> <li>:param peppy.Project project: Project object that has to be uploaded to the DB </li> <li>:param namespace: namespace </li> <li>:param name: project name </li> <li>:param tag: project tag </li> <li>:param force: Force push to the database. Use it to update, or upload project. </li> <li>:param is_private: Make project private </li> <li>:param force: overwrite project if it exists :return: None </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"pephub/user/pephubclient/phc_views_usage/","title":"Python API views","text":""},{"location":"pephub/user/pephubclient/phc_views_usage/#module-pephubclientmodulesview","title":"module <code>pephubclient.modules.view</code>","text":""},{"location":"pephub/user/pephubclient/phc_views_usage/#global-variables","title":"Global Variables","text":"<ul> <li>PEPHUB_VIEW_URL</li> <li>PEPHUB_VIEW_SAMPLE_URL</li> </ul>"},{"location":"pephub/user/pephubclient/phc_views_usage/#class-pephubview","title":"class <code>PEPHubView</code>","text":"<p>Class for managing views in PEPhub and provides methods for  getting, creating, updating and removing views. </p> <p>This class aims to warp the Views API for easier maintenance and better user experience. </p>"},{"location":"pephub/user/pephubclient/phc_views_usage/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(jwt_data: str = None)\n</code></pre> <ul> <li>:param jwt_data: jwt token for authorization </li> </ul>"},{"location":"pephub/user/pephubclient/phc_views_usage/#method-add_sample","title":"method <code>add_sample</code>","text":"<pre><code>add_sample(\n    namespace: str,\n    name: str,\n    tag: str,\n    view_name: str,\n    sample_name: str\n)\n</code></pre> <p>Add sample to view in project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param view_name: name of the view </li> <li>:param sample_name: name of the sample </li> </ul>"},{"location":"pephub/user/pephubclient/phc_views_usage/#method-create","title":"method <code>create</code>","text":"<pre><code>create(\n    namespace: str,\n    name: str,\n    tag: str,\n    view_name: str,\n    description: str = None,\n    sample_list: list = None,\n    no_fail: bool = False\n)\n</code></pre> <p>Create view in project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param description: description of the view </li> <li>:param view_name: name of the view </li> <li>:param sample_list: list of sample names </li> <li>:param no_fail: whether to raise an error if view was not added to the project </li> </ul>"},{"location":"pephub/user/pephubclient/phc_views_usage/#method-delete","title":"method <code>delete</code>","text":"<pre><code>delete(namespace: str, name: str, tag: str, view_name: str) \u2192 None\n</code></pre> <p>Delete view from project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param view_name: name of the view :return: None </li> </ul>"},{"location":"pephub/user/pephubclient/phc_views_usage/#method-get","title":"method <code>get</code>","text":"<pre><code>get(\n    namespace: str,\n    name: str,\n    tag: str,\n    view_name: str,\n    raw: bool = False\n) \u2192 Union[Project, dict]\n</code></pre> <p>Get view from project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param view_name: name of the view </li> <li>:param raw: if True, return raw response :return: peppy.Project object or dictionary of the project (view) </li> </ul>"},{"location":"pephub/user/pephubclient/phc_views_usage/#method-remove_sample","title":"method <code>remove_sample</code>","text":"<pre><code>remove_sample(\n    namespace: str,\n    name: str,\n    tag: str,\n    view_name: str,\n    sample_name: str\n)\n</code></pre> <p>Remove sample from view in project in PEPhub. </p> <ul> <li>:param namespace: namespace of project </li> <li>:param name: name of project </li> <li>:param tag: tag of project </li> <li>:param view_name: name of the view </li> <li>:param sample_name: name of the sample :return: None </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"pephub/user/pephubclient/tutorial/","title":"PEPHub Client","text":"<p>User guide for the PEPHub client.</p> <p>PEPhubclient is available as Python API and CLI. This document will guide you through the usage of the PEPHub client.</p>"},{"location":"pephub/user/pephubclient/tutorial/#installation","title":"Installation","text":"<p>To install PEPHubClient from PyPI, use the following command:</p> <pre><code>pip install pephubclient\n</code></pre> <p>To install <code>pephubclient</code> from the GitHub repository, use the following command:</p> <pre><code>pip install git+https://github.com/pepkit/pephubclient.git\n</code></pre>"},{"location":"pephub/user/pephubclient/tutorial/#authentication","title":"Authentication","text":"<p>To login, use the <code>login</code> command:</p> <pre><code>phc login\n</code></pre>"},{"location":"pephub/user/pephubclient/tutorial/#using-the-pephubclient-cli","title":"Using the PEPHubClient CLI","text":"<p>PEPHubClient provides a CLI for interacting with PEPhub. You can find the list of available commands here: PEPHubClient CLI</p>"},{"location":"pephub/user/pephubclient/tutorial/#using-the-pephubclient-python-api","title":"Using the PEPHubClient Python API","text":"<pre><code>from pephubclient import PEPHubClient\n\n# initiate pephubclient object\nphc = PEPHubClient()\n\n# load pep as peppy.Project object\nexample_pep = phc.load_project(\"databio/example:default\")\nprint(example_pep)\n\n# Printed: \n## Project\n## 6 samples: 4-1_11102016, 3-1_11102016, 2-2_11102016, 2-1_11102016, 8-3_11152016, 8-1_11152016\n## Sections: pep_version, sample_table, name, description\n\n# To upload a project:\nphc.upload(example_pep, namespace=\"databio\", name=\"example\", force=True)\n</code></pre> <p>Full documentation for the Python API can be found here:</p> <ul> <li>PEPHubClient Python API</li> <li>PEPHubClient Python API - Samples</li> <li>PEPHubClient Python API - Views</li> </ul> <p>If you have any questions or need help, please contact us at databio.org</p>"},{"location":"peppy/","title":"Peppy","text":""},{"location":"peppy/#introduction","title":"Introduction","text":"<p><code>peppy</code> is a Python package that provides an API for handling standardized project and sample metadata. If you define your project in Portable Encapsulated Project (PEP) format, you can use the <code>peppy</code> package to instantiate an in-memory representation of your project and sample metadata. You can then use <code>peppy</code> for interactive analysis, or to develop Python tools so you don't have to handle sample processing. <code>peppy</code> is useful to tool developers and data analysts who want a standard way of representing sample-intensive research project metadata.</p>"},{"location":"peppy/#what-is-a-pep","title":"What is a PEP?","text":"<p>A PEP is a collection of metadata files conforming to a standardized structure. These files are written using the simple YAML and TSV/CSV formats, and they can be read by a variety of tools in the pep toolkit, including <code>peppy</code>.  If you don't already understand why the PEP concept is useful to you, start by reading the PEP specification, where you can also find example projects.</p>"},{"location":"peppy/#why-use-peppy","title":"Why use <code>peppy</code>?","text":"<p><code>peppy</code> provides an API with which to interact from Python with PEP metadata.  This is often useful on its own, but the big wins include:</p> <ul> <li>Portability between computing environments</li> <li>Reusability among different tools and project stages</li> <li>Durability with respect to data movement</li> </ul>"},{"location":"peppy/#who-should-use-peppy","title":"Who should use <code>peppy</code>?","text":"<p>There are two main kinds of user that may have interest:</p> <ul> <li>A tool developer</li> <li>A data analyst</li> </ul> <p>If you neither of those describes you, you may be interested in <code>pepr</code> (R package), which provides an R interface to PEP objects, or looper (command-line application), which lets you run any command-line tool or pipeline on samples in a project.</p> <p>Developer</p> <p>As a tool developer, you should <code>import peppy</code> in your Python tool and read PEP projects as its input. </p> <p>This will simplify use of your tool, because users may already have PEP-formatted projects for other tools.</p> <p>Analyst</p> <p><code>peppy</code> provides an easy way to read project metadata into Python. You will have access to an API to access samples and their attributes, facilitating downstream analysis.</p>"},{"location":"peppy/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"peppy/changelog/#0500a1-2025-12-22","title":"[0.50.0a1] -- 2025-12-22","text":""},{"location":"peppy/changelog/#added","title":"Added","text":"<ul> <li>Merged <code>eido</code> (#492) and <code>pephubclient</code> into <code>peppy</code>.</li> <li>Command line interface based on <code>typer</code> for <code>eido</code> and <code>pephubclient</code>.</li> <li>Pytest for Windows.</li> </ul>"},{"location":"peppy/changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed incorrect path expanding from config file URL in Windows.</li> <li>Ambiguity (#498) and redundancy (#499) in error messages.</li> </ul>"},{"location":"peppy/changelog/#0407-2024-09-30","title":"[0.40.7] -- 2024-09-30","text":""},{"location":"peppy/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed incorrect sample table index in subtables</li> </ul>"},{"location":"peppy/changelog/#0406-2024-09-10","title":"[0.40.6] -- 2024-09-10","text":""},{"location":"peppy/changelog/#changed","title":"Changed","text":"<ul> <li>#493</li> </ul>"},{"location":"peppy/changelog/#0405-2024-07-25","title":"[0.40.5] -- 2024-07-25","text":""},{"location":"peppy/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed bug in initialization of peppy object from csv</li> </ul>"},{"location":"peppy/changelog/#0404-2024-07-17","title":"[0.40.4] -- 2024-07-17","text":""},{"location":"peppy/changelog/#changed_1","title":"Changed","text":"<ul> <li>minor change, PR #490</li> </ul>"},{"location":"peppy/changelog/#0403-2024-07-17","title":"[0.40.3] -- 2024-07-17","text":""},{"location":"peppy/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>#393</li> <li>#480</li> <li>#369</li> <li>#399</li> <li>#476</li> <li>#471</li> </ul>"},{"location":"peppy/changelog/#0402-2024-05-28","title":"[0.40.2] -- 2024-05-28","text":""},{"location":"peppy/changelog/#added_1","title":"Added","text":"<ul> <li>added <code>sample_name</code> property to samples object.</li> </ul>"},{"location":"peppy/changelog/#0401-2024-01-11","title":"[0.40.1] -- 2024-01-11","text":""},{"location":"peppy/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Initializing Project with <code>NaN</code> value instead of <code>None</code> in <code>from_pandas</code> method</li> </ul>"},{"location":"peppy/changelog/#0400-2023-12-18","title":"[0.40.0] -- 2023-12-18","text":"<p>This version introduced backwards-incompatible changes.</p>"},{"location":"peppy/changelog/#changed_2","title":"Changed","text":"<ul> <li>Replaced <code>attmap</code> with <code>MutableMapping</code> (which removes some attributes)</li> <li>Replaced OrderedDict with dict</li> <li>Deprecated support for Python versions &lt;= 3.7</li> </ul> <p>Due to the changes mentioned above, a few functionalities may be disabled. For example, the <code>name</code> and <code>description</code> project properties can no longer be accessed with <code>getitem</code>; use the <code>getattr</code> syntax instead</p>"},{"location":"peppy/changelog/#added_2","title":"Added","text":"<ul> <li>Constructor methods: <code>Project.from_dict</code>, <code>Project.from_pandas</code>, <code>Project.from_sample_yaml</code>, <code>Project.from_pep_config</code></li> </ul>"},{"location":"peppy/changelog/#0357-2023-07-19","title":"[0.35.7] -- 2023-07-19","text":""},{"location":"peppy/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>incorrect setting of sample and subsample indexes using from_dict function (#452)</li> <li>clarified debug messages</li> </ul>"},{"location":"peppy/changelog/#0356-2023-06-27","title":"[0.35.6] -- 2023-06-27","text":""},{"location":"peppy/changelog/#added_3","title":"Added","text":"<ul> <li><code>orient</code> argument to <code>to_dict</code> method</li> </ul>"},{"location":"peppy/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>The name of the raw subsample object to match the actual name (list). Commit: #442</li> </ul>"},{"location":"peppy/changelog/#changed_3","title":"Changed","text":"<ul> <li>Reduced the number of items returned in the to_dict(extended=True) method to 3, with the name and description now stored in the config key.</li> </ul>"},{"location":"peppy/changelog/#0355-2023-03-27","title":"[0.35.5] -- 2023-03-27","text":""},{"location":"peppy/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>A bug with custom sample ids</li> <li>Improved performance for large tables dramatically</li> </ul>"},{"location":"peppy/changelog/#0354-2023-01-17","title":"[0.35.4] -- 2023-01-17","text":""},{"location":"peppy/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Fixed disabling rich progress on small datasets bug</li> <li>Disabled progressbar if object variable <code>progressbar</code> is set False</li> </ul>"},{"location":"peppy/changelog/#0353-2022-11-16","title":"[0.35.3] -- 2022-11-16","text":""},{"location":"peppy/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Returning <code>NaN</code> value in initialization project from pandas df</li> </ul>"},{"location":"peppy/changelog/#0352-2022-09-13","title":"[0.35.2] -- 2022-09-13","text":""},{"location":"peppy/changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Returning <code>NaN</code> value within <code>to_dict</code> method was fixed and method now returns <code>None</code> instead</li> </ul>"},{"location":"peppy/changelog/#0351-2022-09-07","title":"[0.35.1] -- 2022-09-07","text":""},{"location":"peppy/changelog/#changed_4","title":"Changed","text":"<ul> <li>Organization of test files. Separated unittests from smoketests.</li> </ul>"},{"location":"peppy/changelog/#fixed_11","title":"Fixed","text":"<ul> <li>The root cause of <code>np.nan</code> values showing up in Pandas dataframes. Replaced the values with None right after reading the database, which made it possible to remove all custom <code>np.nan</code> to <code>None</code> converters used later in the code.</li> <li>Typing in some methods.</li> <li>Code redundancy in fixtures in conftest.</li> </ul>"},{"location":"peppy/changelog/#added_4","title":"Added","text":"<ul> <li>New test cases with test data</li> </ul>"},{"location":"peppy/changelog/#0350-2022-08-25","title":"[0.35.0] -- 2022-08-25","text":""},{"location":"peppy/changelog/#changed_5","title":"Changed","text":"<ul> <li>Optimized converting Projects to and from dict. Now, <code>to_dict(extended=True)</code> returns only essential properties to save space and time.</li> <li>Small refactors.</li> </ul>"},{"location":"peppy/changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Initialization of <code>peppy.Project</code> from <code>pandas.DataFrame</code>. Now <code>from_pandas()</code> can receive sample table, subsample table and config file</li> <li>Multiple bugs introduced during initialization of the project with custom index column names</li> </ul>"},{"location":"peppy/changelog/#added_5","title":"Added","text":"<ul> <li>New test cases and test data</li> </ul>"},{"location":"peppy/changelog/#0340-2022-08-17","title":"[0.34.0] -- 2022-08-17","text":""},{"location":"peppy/changelog/#changed_6","title":"Changed","text":"<ul> <li>Way of initialization project from dictionary. Now it's possible as follows: <code>Project().from_dict()</code></li> </ul>"},{"location":"peppy/changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Fix error that was raised when duplicated sample in <code>sample_table</code> had different read types (single-end mixed with paired-end).</li> </ul>"},{"location":"peppy/changelog/#added_6","title":"Added","text":"<ul> <li>Feature of initializing <code>peppy.Project</code> from <code>pandas.DataFrame</code></li> </ul>"},{"location":"peppy/changelog/#0330-2022-07-25","title":"[0.33.0] -- 2022-07-25","text":""},{"location":"peppy/changelog/#changed_7","title":"Changed","text":"<ul> <li><code>pep_version</code> is no longer a required parameter to create a <code>peppy.Project</code> instance from a configuration file.</li> </ul>"},{"location":"peppy/changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Performance issues during sample parsing. Two list comprehensions were combined to speed up this functionality.</li> <li><code>KeyError</code> is thrown when attempting to access the <code>pep_version</code> of a <code>peppy.Project</code> instance instantiated from a sample table (<code>csv</code>)</li> </ul>"},{"location":"peppy/changelog/#added_7","title":"Added","text":"<ul> <li>Implementation of <code>__eq__</code> for the <code>peppy.Project</code> class such that two instances of the class can be compared using python's equality operators (<code>==</code>, <code>!=</code>).</li> <li>New <code>from_dict</code> function that lets a user instantiate a new <code>peppy.Project</code> object using an in-memory representation of a PEP (a <code>dict</code>). This supports database storage of PEPs.</li> <li>New <code>extended</code> flag for the <code>to_dict</code> method on <code>peppy.Project</code> objects. This creates a richer dictionary representation of PEPs.</li> <li>Better sample parsing</li> </ul>"},{"location":"peppy/changelog/#0320-2022-05-03","title":"[0.32.0] -- 2022-05-03","text":""},{"location":"peppy/changelog/#changed_8","title":"Changed","text":"<ul> <li>Unify exceptions related to remote YAML file reading in <code>read_yaml</code> function. Now always a <code>RemoteYAMLError</code> is thrown.</li> <li><code>Project</code> dict representation</li> </ul>"},{"location":"peppy/changelog/#added_8","title":"Added","text":"<ul> <li>Support for PEP <code>2.1.0</code>, whichi includes support for no YAML configuration file component (CSV only), automatic sample merging if there are any duplicates in sample table index column, and new project attributes: <code>sample_table_index</code> and <code>subsample_table_index</code>.</li> </ul>"},{"location":"peppy/changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Project string representation; Issue 368</li> </ul>"},{"location":"peppy/changelog/#0312-2021-11-04","title":"[0.31.2] -- 2021-11-04","text":""},{"location":"peppy/changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Bug with setuptools 58</li> </ul>"},{"location":"peppy/changelog/#0311-2021-04-15","title":"[0.31.1] -- 2021-04-15","text":""},{"location":"peppy/changelog/#added_9","title":"Added","text":"<ul> <li>Support for remote URL config files</li> </ul>"},{"location":"peppy/changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Error when accessing <code>Project.subsample_table</code> property when no subsample tables were defined</li> </ul>"},{"location":"peppy/changelog/#0310-2020-10-07","title":"[0.31.0] - 2020-10-07","text":""},{"location":"peppy/changelog/#added_10","title":"Added","text":"<ul> <li><code>to_dict</code> method in <code>Sample</code> class that can include or exclude <code>Project</code> reference</li> </ul>"},{"location":"peppy/changelog/#0303-2020-09-22","title":"[0.30.3] - 2020-09-22","text":""},{"location":"peppy/changelog/#changed_9","title":"Changed","text":"<ul> <li>If there's just one <code>subsample_table</code> specified, <code>Project.subsample_table</code> property will return an object of <code>pandas.DataFrame</code> class rather than a <code>list</code> of ones</li> </ul>"},{"location":"peppy/changelog/#fixed_18","title":"Fixed","text":"<ul> <li><code>TypeError</code> when <code>subsample_table</code> is set to <code>null</code></li> </ul>"},{"location":"peppy/changelog/#0302-2020-08-06","title":"[0.30.2] - 2020-08-06","text":""},{"location":"peppy/changelog/#added_11","title":"Added","text":"<ul> <li>Support for multiple subsample tables</li> <li>License file to the package source distribution</li> </ul>"},{"location":"peppy/changelog/#0301-2020-05-26","title":"[0.30.1] - 2020-05-26","text":""},{"location":"peppy/changelog/#changed_10","title":"Changed","text":"<ul> <li>Package authors list</li> </ul>"},{"location":"peppy/changelog/#0300-2020-05-26","title":"[0.30.0] - 2020-05-26","text":"<p>This version introduced backwards-incompatible changes.</p>"},{"location":"peppy/changelog/#added_12","title":"Added","text":"<ul> <li>attribute duplication functionality</li> <li>config importing functionality</li> <li>attribute removal functionality</li> <li>possibility to define multi-attribute rules in attribute implication</li> </ul>"},{"location":"peppy/changelog/#changed_11","title":"Changed","text":"<ul> <li>Project configuration file to follow PEP2.0.0 specification. Browse the specification for changes related to config format</li> <li>Do not require <code>sample_name</code> attribute in the sample table</li> </ul>"},{"location":"peppy/changelog/#0223-2019-12-13","title":"[0.22.3] - 2019-12-13","text":""},{"location":"peppy/changelog/#changed_12","title":"Changed","text":"<ul> <li>Remove <code>is_command_callable</code> from <code>utils</code> module; instead, refer to <code>ubiquerg</code>.</li> <li>It's now exceptional (rather than just a warning) for a sample table file to be missing a valid name column.</li> </ul>"},{"location":"peppy/changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Empty columns in subsample tables are treated just as empty columns in sample tables (respective attributes are not included rather than populated with <code>nan</code>)</li> </ul>"},{"location":"peppy/changelog/#0222-2019-06-20","title":"[0.22.2] - 2019-06-20","text":""},{"location":"peppy/changelog/#changed_13","title":"Changed","text":"<ul> <li>Remove <code>ngstk</code> requirement.</li> </ul>"},{"location":"peppy/changelog/#0221-2019-06-19","title":"[0.22.1] - 2019-06-19","text":""},{"location":"peppy/changelog/#changed_14","title":"Changed","text":"<ul> <li>Prohibit storing reference to full <code>Project</code> object on a <code>Sample</code>.</li> </ul>"},{"location":"peppy/changelog/#0220-2019-06-06","title":"[0.22.0] -- (2019-06-06)","text":""},{"location":"peppy/changelog/#changed_15","title":"Changed","text":"<ul> <li>Deprecate <code>Project</code> <code>constants</code> in favor of <code>constant_attributes.</code></li> <li>Improved <code>Project</code> text representation for interactive/terminal display (<code>__repr__</code>): Issue 296</li> </ul>"},{"location":"peppy/changelog/#fixed_20","title":"Fixed","text":"<ul> <li>Properly use <code>constant_attributes</code> if present from subproject. Issue 292</li> <li>Fixed a bug with subproject activation paths</li> <li>Revert deprecation of <code>sample_name</code> to <code>name</code>; so <code>sample_name</code> is again approved.</li> </ul>"},{"location":"peppy/changelog/#0210-2019-05-02","title":"[0.21.0] -- (2019-05-02)","text":""},{"location":"peppy/changelog/#added_13","title":"Added","text":"<ul> <li>Support for Snakemake projects (particularly <code>SnakeProject</code>)</li> <li>Hook for <code>get_arg_string</code> on <code>Project</code> to omit some pipeline options/arguments from the returned argument string</li> <li><code>sample_table</code> and <code>subsample_table</code> functions, providing a functional syntax for requesting the respective attribute values from a <code>Project</code></li> <li>Hook on <code>merge_sample</code> for specifying name of subannotation column that stores name for each sample</li> </ul>"},{"location":"peppy/changelog/#changed_16","title":"Changed","text":"<ul> <li>Improved messaging: \"Unmatched regex-like\", \"Missing and/or empty attribute(s)\"</li> <li>On <code>Project</code>, <code>sheet</code> is deprecated in favor of <code>sample_table</code>.</li> <li>On <code>Project</code>, <code>sample_subannotation</code> is deprecated in favor of <code>subsample_table</code>.</li> <li>On <code>Sample</code>, reference to <code>sample_name</code> is deprecated in favor of simply <code>name</code>.</li> </ul>"},{"location":"peppy/changelog/#0200-2019-04-17","title":"[0.20.0] -- (2019-04-17)","text":""},{"location":"peppy/changelog/#added_14","title":"Added","text":"<ul> <li><code>subsample_table</code> on a <code>Project</code> gives the table of sample subannotation / \"units\" if applicable.</li> </ul>"},{"location":"peppy/changelog/#changed_17","title":"Changed","text":"<ul> <li>Add <code>attribute</code> parameter to <code>fetch_samples</code> function to enable more general applicability.   Additionally, the attribute value matching is more strict now -- requires perfect match.</li> <li>Remove Python 3.4 support.</li> <li>Use <code>attmap</code> for implementation of attribute-style access into a key-value collection.</li> <li>Deprecate <code>sample_annotation</code> and <code>sample_subannotation</code> in favor of <code>sample_table</code> and <code>subsample_table</code>, respectively.</li> </ul>"},{"location":"peppy/changelog/#0190-2019-01-16","title":"[0.19.0] -- (2019-01-16)","text":""},{"location":"peppy/changelog/#new","title":"New","text":"<ul> <li>Added <code>activate_subproject</code> method to <code>Project</code>.</li> </ul>"},{"location":"peppy/changelog/#changed_18","title":"Changed","text":"<ul> <li><code>Project</code> construction no longer requires sample annotations sheet.</li> <li>Specification of assembly/ies in project config outside of <code>implied_attributes</code> is deprecated.</li> <li><code>implied_columns</code> and <code>derived_columns</code> are deprecated in favor of <code>implied_attributes</code> and <code>derived_attributes</code>.</li> </ul>"},{"location":"peppy/changelog/#0182-2018-07-23","title":"[0.18.2] -- (2018-07-23)","text":""},{"location":"peppy/changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Made requirements more lenient to allow for newer versions of required packages.</li> </ul>"},{"location":"peppy/changelog/#0181-2018-06-29","title":"[0.18.1] -- (2018-06-29)","text":""},{"location":"peppy/changelog/#fixed_22","title":"Fixed","text":"<ul> <li>Fixed a bug that would cause sample attributes to lose order.</li> <li>Fixed a bug that caused an install error with newer <code>numexpr</code> versions.</li> </ul>"},{"location":"peppy/changelog/#new_1","title":"New","text":"<ul> <li>Project names are now inferred with the <code>infer_name</code> function, which uses a priority lookup to infer the project name: First, the <code>name</code> attribute in the <code>yaml</code> file; otherwise, the containing folder unless it is <code>metadata</code>, in which case, it's the parent of that folder.</li> <li>Add <code>get_sample</code> and <code>get_samples</code> functions to <code>Project</code> objects.</li> <li>Add <code>get_subsamples</code>and <code>get_subsample</code> functions to both <code>Project</code> and <code>Sample</code> objects.</li> <li>Subsamples are now objects that can be retrieved individually by name, with the <code>subsample_name</code> as the index column header.</li> </ul>"},{"location":"peppy/changelog/#0172-2018-04-03","title":"[0.17.2] -- (2018-04-03)","text":""},{"location":"peppy/changelog/#fixed_23","title":"Fixed","text":"<ul> <li>Ensure data source path relativity is with respect to project config file's folder.</li> </ul>"},{"location":"peppy/changelog/#0171-2017-12-21","title":"[0.17.1] -- (2017-12-21)","text":""},{"location":"peppy/changelog/#changed_19","title":"Changed","text":"<ul> <li>Version bump for first pypi release</li> <li>Fixed bug with packaging for pypi release</li> </ul>"},{"location":"peppy/changelog/#090-2017-12-21","title":"[0.9.0] -- (2017-12-21)","text":""},{"location":"peppy/changelog/#new_2","title":"New","text":"<ul> <li>Separation completed, <code>peppy</code> package is now standalone</li> <li><code>looper</code> can now rely on <code>peppy</code></li> </ul>"},{"location":"peppy/changelog/#changed_20","title":"Changed","text":"<ul> <li><code>merge_table</code> renamed to <code>sample_subannotation</code></li> <li>setup changed for compatibility with PyPI</li> </ul>"},{"location":"peppy/changelog/#081-2017-11-16","title":"[0.8.1] -- (2017-11-16)","text":""},{"location":"peppy/changelog/#new_3","title":"New","text":"<ul> <li>Separated from looper into its own python package (originally called <code>pep</code>)</li> </ul>"},{"location":"peppy/changelog/#072-2017-11-16","title":"[0.7.2] -- (2017-11-16)","text":""},{"location":"peppy/changelog/#fixed_24","title":"Fixed","text":"<ul> <li>Correctly count successful command submissions when not using <code>--dry-run</code>.</li> </ul>"},{"location":"peppy/changelog/#071-2017-11-15","title":"[0.7.1] -- (2017-11-15)","text":""},{"location":"peppy/changelog/#fixed_25","title":"Fixed","text":"<ul> <li>No longer falsely display that there's a submission failure.</li> <li>Allow non-string values to be unquoted in the <code>pipeline_args</code> section.</li> </ul>"},{"location":"peppy/changelog/#070-2017-11-15","title":"[0.7.0] -- (2017-11-15)","text":""},{"location":"peppy/changelog/#new_4","title":"New","text":"<ul> <li>Add <code>--lump</code> and <code>--lumpn</code> options</li> <li>Catch submission errors from cluster resource managers</li> <li>Implied columns can now be derived</li> <li>Now protocols can be specified on the command-line <code>--include-protocols</code></li> <li>Add rudimentary figure summaries</li> <li>Allow wildcard protocol_mapping for catch-all pipeline assignment</li> <li>New sample_subtypes section in pipeline_interface</li> </ul>"},{"location":"peppy/changelog/#changed_21","title":"Changed","text":"<ul> <li>Sample child classes are now defined explicitly in the pipeline interface. Previously, they were guessed based on presence of a class extending Sample in a pipeline script.</li> <li>Changed 'library' key sample attribute to 'protocol'</li> <li>Improve user messages</li> <li>Simplifies command-line help display</li> </ul>"},{"location":"peppy/changelog/#060-2017-07-21","title":"[0.6.0] -- (2017-07-21)","text":""},{"location":"peppy/changelog/#new_5","title":"New","text":"<ul> <li>Add support for implied_column section of the project config file</li> <li>Add support for Python 3</li> <li>Merges pipeline interface and protocol mappings. This means we now allow direct pointers to <code>pipeline_interface.yaml</code> files, increasing flexibility, so this relaxes the specified folder structure that was previously used for <code>pipelines_dir</code> (with <code>config</code> subfolder).</li> <li>Allow URLs as paths to sample sheets.</li> <li>Allow tsv format for sample sheets.</li> <li>Checks that the path to a pipeline actually exists before writing the submission script.</li> </ul>"},{"location":"peppy/changelog/#changed_22","title":"Changed","text":"<ul> <li>Changed LOOPERENV environment variable to PEPENV, generalizing it to generic models</li> <li>Changed name of <code>pipelines_dir</code> to <code>pipeline_interfaces</code> (but maintained backwards compatibility for now).</li> <li>Changed name of <code>run</code> column to <code>toggle</code>, since <code>run</code> can also refer to a sequencing run.</li> <li>Relaxes many constraints (like resources sections, pipelines_dir columns), making project configuration files useful outside looper. This moves us closer to dividing models from looper, and improves flexibility.</li> <li>Various small bug fixes and dev improvements.</li> <li>Require <code>setuptools</code> for installation, and <code>pandas 0.20.2</code>. If <code>numexpr</code> is installed, version <code>2.6.2</code> is required.</li> <li>Allows tilde in <code>pipeline_interfaces</code></li> </ul>"},{"location":"peppy/changelog/#050-2017-03-01","title":"[0.5.0] -- (2017-03-01)","text":""},{"location":"peppy/changelog/#new_6","title":"New","text":"<ul> <li>Add new looper version tracking, with <code>--version</code> and <code>-V</code> options and printing version at runtime</li> <li>Add support for asterisks in file paths</li> <li>Add support for multiple pipeline directories in priority order</li> <li>Revamp of messages make more intuitive output</li> <li>Colorize output</li> <li>Complete rehaul of logging and test infrastructure, using logging and pytest packages</li> </ul>"},{"location":"peppy/changelog/#changed_23","title":"Changed","text":"<ul> <li>Removes pipelines_dir requirement for models, making it useful outside looper</li> <li>Small bug fixes related to <code>all_input_files</code> and <code>required_input_files</code> attributes</li> <li>More robust installation and more explicit requirement of Python 2.7</li> </ul>"},{"location":"peppy/changelog/#040-2017-01-12","title":"[0.4.0] -- (2017-01-12)","text":""},{"location":"peppy/changelog/#new_7","title":"New","text":"<ul> <li>New command-line interface (CLI) based on sub-commands</li> <li>New subcommand (<code>looper summarize</code>) replacing the <code>summarizePipelineStats.R</code> script</li> <li>New subcommand (<code>looper check</code>) replacing the <code>flagCheck.sh</code> script</li> <li>New command (<code>looper destroy</code>) to remove all output of a project</li> <li>New command (<code>looper clean</code>) to remove intermediate files of a project flagged for deletion</li> <li>Support for portable and pipeline-independent allocation of computing resources with Looperenv.</li> </ul>"},{"location":"peppy/changelog/#changed_24","title":"Changed","text":"<ul> <li>Removed requirement to have <code>pipelines</code> repository installed in order to extend base Sample objects</li> <li>Maintenance of sample attributes as provided by user by means of reading them in as strings (to be improved further</li> <li>Improved serialization of Sample objects</li> </ul>"},{"location":"peppy/contributing/","title":"Contributing","text":"<p>Pull requests are welcome.</p> <p>After adding tests in <code>tests</code> for a new feature or a bug fix, please run the test suite. To do so, the only additional dependencies (beyond those needed for the package itself) can be installed with:</p> <pre><code>pip install -r requirements/requirements-dev.txt\n</code></pre> <p>Once those are installed, the tests can be run with <code>pytest</code>. Alternatively, <code>python setup.py test</code> can be used.</p>"},{"location":"peppy/hello-world/","title":"Installation and Hello, World!","text":""},{"location":"peppy/hello-world/#installation","title":"Installation","text":"<p>With <code>pip</code> you can install the latest release from PyPI:</p> <pre><code>pip install --user peppy\n</code></pre> <p>Update <code>peppy</code> with <code>pip</code>:</p> <pre><code>pip install --user --upgrade peppy\n</code></pre> <p>Releases and development versions may also be installed from the GitHub releases:</p> <pre><code>pip install --user https://github.com/pepkit/peppy/zipball/master\n</code></pre>"},{"location":"peppy/hello-world/#hello-world","title":"Hello world!","text":"<p>Now, to test <code>peppy</code>, let's grab an clone an example project that follows PEP format. We've produced a bunch of example PEPs in the <code>example_peps</code> repository. Let's clone it:</p> <pre><code>git clone https://github.com/pepkit/example_peps.git\n</code></pre> <p>Then, from within the <code>example_peps</code> folder, enter the following commands in a Python session:</p> <pre><code>import peppy\n\nproject = peppy.Project(\"example_basic/project_config.yaml\") # instantiate in-memory Project representation\nsamples = project.samples # grab the list of Sample objects defined in this Project\n\n# Find the input file for the first sample in the project\nsamples[0][\"file\"]\n</code></pre> <p>That's it! You've got <code>peppy</code> running on an example project. Now you can play around with project metadata from within python. There are lots of other ways to initialize a project, which we will in the next section.</p>"},{"location":"peppy/initialize/","title":"How to initiate peppy using different methods","text":"<p>The primary use case of <code>peppy</code> is to create a <code>peppy.Project</code> object, which will give you an API for interacting with your project and sample metadata. There are multiple ways to instantiate a <code>peppy.Project</code>.  The most common is to use a configuration file; however, you can also use a <code>CSV</code> file (sample sheet), or a sample <code>YAML</code> file (sample sheet), or use Python objects directly, such as a <code>pandas</code> DataFrame, or a Python <code>dict</code>.</p> peppy can read from and produce various metadata formats"},{"location":"peppy/initialize/#1-from-pep-configuration-file","title":"1. From PEP configuration file","text":"<pre><code>import peppy\nproject = peppy.Project.from_pep_config(\"path/to/project/config.yaml\")\n</code></pre>"},{"location":"peppy/initialize/#2-from-csv-file-sample-sheet","title":"2. FROM <code>CSV</code> file (sample sheet)","text":"<pre><code>import peppy\nproject = peppy.Project.from_pep_config(\"path/to/project/sample_sheet.csv\")\n</code></pre> <p>You can also instantiate directly from a URL to a CSV file:</p> <pre><code>import peppy\nproject = peppy.Project(\"https://raw.githubusercontent.com/pepkit/example_peps/master/example_basic/sample_table.csv\")\n</code></pre>"},{"location":"peppy/initialize/#3-from-yaml-sample-sheet","title":"3. From <code>YAML</code> sample sheet","text":"<pre><code>import peppy\n\nproject = peppy.Project.from_sample_yaml(\"path/to/project/sample_sheet.yaml\")\n</code></pre>"},{"location":"peppy/initialize/#4-from-a-pandas-dataframe","title":"4. From a <code>pandas</code> DataFrame","text":"<pre><code>import pandas as pd\nimport peppy\ndf = pd.read_csv(\"path/to/project/sample_sheet.csv\")\nproject = peppy.Project.from_pandas(df)\n</code></pre>"},{"location":"peppy/initialize/#5-from-a-peppy-generated-dict","title":"5. From a <code>peppy</code>-generated <code>dict</code>","text":"<p>Store a <code>peppy.Project</code> object as a dict using <code>prj.to_dict()</code>. Then, load it with <code>Project.from_dict()</code>:</p> <pre><code>import peppy\n\nproject = peppy.Project(\"https://raw.githubusercontent.com/pepkit/example_peps/master/example_basic/sample_table.csv\")\nproject_dict = project.to_dict(extended=True)\nproject_copy = peppy.Project.from_dict(project_dict)\n\n# now you can check if this project is the same as the original project\nprint(project_copy == project)\n</code></pre> <p>Or, you could generate an equivalent dictionary in some other way:</p> <pre><code>import peppy\nproject = peppy.Project.from_dict(\n    {'_config': {'description': None,\n                 'name': 'example_basic',\n                 'pep_version': '2.0.0',\n                 'sample_table': 'sample_table.csv',},\n    '_sample_dict': [{'organism': 'pig', 'sample_name': 'pig_0h', 'time': '0'},\n                     {'organism': 'pig', 'sample_name': 'pig_1h', 'time': '1'},\n                     {'organism': 'frog', 'sample_name': 'frog_0h', 'time': '0'},\n                     {'organism': 'frog', 'sample_name': 'frog_1h', 'time': '1'}],\n    '_subsample_list': [[{'read1': 'frog1a_data.txt',\n                       'read2': 'frog1a_data2.txt',\n                       'sample_name': 'frog_0h'},\n                      {'read1': 'frog1b_data.txt',\n                       'read2': 'frog1b_data2.txt',\n                       'sample_name': 'pig_0h'},\n                      {'read1': 'frog1c_data.txt',\n                       'read2': 'frog1b_data2.txt',\n                       'sample_name': 'pig_0h'}]]})\n</code></pre>"},{"location":"peppy/models/","title":"Project models","text":"<p><code>peppy</code> models projects and samples as Python objects.</p> <pre><code>import peppy\n\nmy_project = peppy.Project(\"path/to/project_config.yaml\")\nmy_samples = my_project.samples\n</code></pre> <p>Once you have your project and samples in your Python session, the possibilities are endless. For example, one way we use these objects is for post-pipeline processing. After we use looper to run each sample through its pipeline, we can load the project and it sample objects into an analysis session, where we do comparisons across samples.</p> <p>Exploration:</p> <p>To interact with the various <code>models</code> and become acquainted with their features and behavior, there is a lightweight module that provides small working versions of a couple of the core objects. Specifically, from within the <code>tests</code> directory, the Python code in the <code>tests.interactive</code> module can be copied and pasted into an interpreter. This provides a <code>Project</code> instance called <code>proj</code> and a <code>PipelineInterface</code> instance called <code>pi</code>. Additionally, this provides logging information in great detail, affording visibility into some what's happening as the <code>models</code> are created and used.</p>"},{"location":"peppy/models/#extending-sample-objects","title":"Extending sample objects","text":"<p>By default we use generic models (see Peppy API docs for more) that can be used in many contexts via Python import, or by object serialization and deserialization via YAML.</p> <p>Since these models provide useful methods to store, update, and read attributes in the objects created from them (most notably a sample - <code>Sample</code> object), a frequent use case is during the run of a pipeline. A pipeline can create a more custom <code>Sample</code> model, adding or altering properties and methods.</p>"},{"location":"peppy/models/#use-case","title":"Use case","text":"<p>You have several samples, of different experiment types, each yielding different varieties of data and files. For each sample of a given experiment type that uses a particular pipeline, the set of file path types that are relevant for the initial pipeline processing or for downstream analysis is known. For instance, a peak file with a certain genomic location will likely be relevant for a ChIP-seq sample, while a transcript abundance/quantification file will probably be used when working with a RNA-seq sample. This common situation, in which one or more file types are specific to a pipeline and analysis both benefits from and is amenable to a bespoke <code>Sample</code> type.</p> <p>Rather than working with a base <code>Sample</code> instance and repeatedly specifying paths to relevant files, those locations can be provided just once, stored in an instance of the custom <code>Sample</code> type, and later used or modified as needed by referencing a named attribute on the object. This approach can dramatically reduce the number of times that a full filepath must be precisely typed, improving pipeline readability and accuracy.</p>"},{"location":"peppy/models/#mechanics","title":"Mechanics","text":"<p>It's the specification of both an experiment or data type (\"library\" or \"protocol\") and a pipeline with which to process that input type that <code>looper</code> uses to determine which type of <code>Sample</code> object(s) to create for pipeline processing and analysis (i.e., which <code>Sample</code> extension to use). There's a pair of symmetric reasons for this--the relationship between input type and pipeline can be one-to-many, in both directions. That is, it's possible for a single pipeline to process more than one input type, and a single input type may be processed by more than one pipeline.</p> <p>There are a few different <code>Sample</code> extension scenarios. Most basic is the one in which an extension, or subtype, is neither defined nor needed--the pipeline author does not provide one, and users do not request one. Almost equally effortless on the user side is the case in which a pipeline author intends for a single subtype to be used with her pipeline. In this situation, the pipeline author simply implements the subtype within the pipeline module, and nothing further is required--of the pipeline author or of a user! The <code>Sample</code> subtype will be found within the pipeline module, and the inference will be made that it's intended to be used as the fundamental representation of a sample within that pipeline.</p> <p>If a pipeline author extends the base<code>Sample</code> type in the pipeline module, it's likely that the pipeline's proper functionality depends on the use of that subtype. In some cases, though, it may be desirable to use the base <code>Sample</code> type even if the pipeline author has provided a more customized version with the pipeline. To favor the base <code>Sample</code> over the tailored one created by a pipeline author, the user may simply set <code>sample_subtypes</code> to <code>null</code> in an altered version of the pipeline interface, either for all types of inpute to that pipeline, or just a subset.</p> <pre><code># atacseq.py\n\nimport os\nfrom peppy import Sample\n\nclass ATACseqSample(Sample):\n    \"\"\"\n    Class to model ATAC-seq samples based on the generic Sample class.\n\n    :param pandas.Series series: data defining the Sample\n    \"\"\"\n\n    def __init__(self, series):\n        if not isinstance(series, pd.Series):\n            raise TypeError(\"Provided object is not a pandas Series.\")\n        super(ATACseqSample, self).__init__(series)\n        self.make_sample_dirs()\n\n    def set_file_paths(self, project=None):\n        \"\"\"Sets the paths of all files for this sample.\"\"\"\n        # Inherit paths from Sample by running Sample's set_file_paths()\n        super(ATACseqSample, self).set_file_paths(project)\n\n        self.fastqc = os.path.join(self.paths.sample_root, self.name + \".fastqc.zip\")\n        self.trimlog = os.path.join(self.paths.sample_root, self.name + \".trimlog.txt\")\n        self.fastq = os.path.join(self.paths.sample_root, self.name + \".fastq\")\n        self.trimmed = os.path.join(self.paths.sample_root, self.name + \".trimmed.fastq\")\n        self.mapped = os.path.join(self.paths.sample_root, self.name + \".bowtie2.bam\")\n        self.peaks = os.path.join(self.paths.sample_root, self.name + \"_peaks.bed\")\n</code></pre> <p>To leverage the power of a <code>Sample</code> subtype, the relevant model is the <code>PipelineInterface</code>. For each pipeline defined in the <code>pipelines</code> section of <code>pipeline_interface.yaml</code>, there's accommodation for a <code>sample_subtypes</code> subsection to communicate this information. The value for each such key may be either a single string or a collection of key-value pairs. If it's a single string, the value is the name of the class that's to be used as the template for each <code>Sample</code> object created for processing by that pipeline. If instead it's a collection of key-value pairs, the keys should be names of input data types (as in the <code>protocol_mapping</code>), and each value is the name of the class that should be used for each sample object of the corresponding keyfor that pipeline. This underscores that it's the combination of a pipeline and input type that determines the subtype.</p> <pre><code># Content of pipeline_interface.yaml\n\nprotocol_mapping:\n    ATAC: atacseq.py\n\npipelines:\n    atacseq.py:\n        ...\n        ...\n        sample_subtypes: ATACseqSample\n        ...\n        ...\n    ...\n    ...\n</code></pre> <p>If a pipeline author provides more than one subtype, the <code>sample_subtypes</code> section is needed to select from among them once it's time to create <code>Sample</code> objects. If multiple options are available, and the <code>sample_subtypes</code> section fails to clarify the decision, the base/generic type will be used. The responsibility for supplying the <code>sample_subtypes</code> section, as is true for the rest of the pipeline interface, therefore rests primarily with the pipeline developer. It is possible for an end user to modify these settings, though.</p> <p>Since the mechanism for subtype detection is <code>inspect</code>-ion of each of the pipeline module's classes and retention of those which satisfy a subclass status check against <code>Sample</code>, it's possible for pipeline authors to implement a class hierarchy with multi-hop inheritance relationships. For example, consider the addition of the following class to the previous example of a pipeline module <code>atacseq.py</code>:</p> <pre><code>class DNaseSample(ATACseqSample):\n    ...\n</code></pre> <p>In this case there are now two <code>Sample</code> subtypes available, and more generally, there will necessarily be multiple subtypes available in any pipeline module that uses a subtype scheme with multiple, serial inheritance steps. In such cases, the pipeline interface should include an unambiguous <code>sample_subtypes</code> section.</p> <pre><code># Content of pipeline_interface.yaml\n\nprotocol_mapping:\n    ATAC: atacseq.py\n    DNase: atacseq.py\n\npipelines:\n    atacseq.py:\n        ...\n        ...\n        sample_subtypes:\n            ATAC: ATACseqSample\n            DNase: DNaseSample\n        ...\n        ...\n    ...\n    ...\n</code></pre>"},{"location":"peppy/support/","title":"Support","text":"<p>Please use the issue tracker at GitHub to file bug reports or feature requests on the project's issues page.</p>"},{"location":"peppy/validating/","title":"How to validate a PEP","text":"<p>Starting with version <code>0.30.0</code>, peppy now includes a powerful validation framework. We provide a schema for the basic PEP specification, so you can validate that a PEP fills that spec. Then, you can also write an extended schema to validate a pep for a specific analysis. All of the PEP validation functionality is handled by a separate package called <code>eido</code>. You can read more in the eido documentation, including:</p> <ul> <li>How to validate a PEP against the generic PEP format</li> <li>How to validate a PEP against a custom schema</li> <li>How to write your own custom schema</li> </ul> <p>See the eido documentation for further detail.</p>"},{"location":"peppy/code/feature1_append/","title":"Learn append sample modifier in <code>peppy</code>","text":"<p>This vignette will show you how and why to use the append functionality of the <code>pepr</code> package. </p> <ul> <li>basic information about this concept on the specification website.</li> </ul>"},{"location":"peppy/code/feature1_append/#problemgoal","title":"Problem/Goal","text":"<p>The example below demonstrates how to use the constant attributes to define the samples attributes in the <code>read_type</code> column of the <code>sample_table.csv</code> file. This functionality is extremely useful when there are many samples that are characterized by identical values of certain attribute (here: value <code>SINGLE</code> in <code>read_type</code> attribute). Please consider the example below for reference:</p> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_append/\"\nsample_table_ori = examples_dir + \"sample_table_pre.csv\"\n%cat $sample_table_ori | column -t -s, | cat\n</code></pre> <pre><code>sample_name  organism  time  read_type\n\npig_0h       pig       0     SINGLE\n\npig_1h       pig       1     SINGLE\n\nfrog_0h      frog      0     SINGLE\n\nfrog_1h      frog      1     SINGLE\n</code></pre>"},{"location":"peppy/code/feature1_append/#solution","title":"Solution","text":"<p>As the name suggests the attributes in the specified attributes (here: <code>read_type</code>) can be defined as constant ones. The way how this process is carried out is indicated explicitly in the <code>project_config.yaml</code> file (presented below). The name of the column is determined in the <code>sample_modifiers.append</code> key-value pair. Note that definition of more than one constant attribute is possible.</p> <pre><code>project_config_file = examples_dir + \"project_config.yaml\"\n%cat $project_config_file\n</code></pre> <pre><code>pep_version: \"2.0.0\"\nsample_table: sample_table.csv\n\nsample_modifiers:\n  append:\n    read_type: SINGLE\n</code></pre> <p>Let's introduce a few modifications to the original <code>sample_table.csv</code> file to use the <code>sample_modifiers.append</code> section of the config. Simply skip the attributes that are set constant and let the <code>pepr</code> do the work for you.</p> <pre><code>sample_table = examples_dir + \"sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  organism  time\n\npig_0h       pig       0\n\npig_1h       pig       1\n\nfrog_0h      frog      0\n\nfrog_1h      frog      1\n</code></pre>"},{"location":"peppy/code/feature1_append/#code","title":"Code","text":"<p>Import <code>peppy</code> and read in the project metadata by specifying the path to the <code>project_config.yaml</code>:</p> <pre><code>from peppy import Project\np = Project(project_config_file)\n</code></pre> <p>And inspect it:</p> <pre><code>print(p)\np.sample_table\n</code></pre> <pre><code>Project 'example_append' (/Users/mstolarczyk/Uczelnia/UVA/code/peppy/tests/data/example_peps-cfg2/example_append/project_config.yaml)\n4 samples: pig_0h, pig_1h, frog_0h, frog_1h\nSections: pep_version, sample_table, sample_modifiers\n</code></pre> organism read_type sample_name time sample_name pig_0h pig SINGLE pig_0h 0 pig_1h pig SINGLE pig_1h 1 frog_0h frog SINGLE frog_0h 0 frog_1h frog SINGLE frog_1h 1 <p>As you can see, the resulting samples are annotated the same way as if they were read from the original annotations file with attributes in the last column manually determined.</p>"},{"location":"peppy/code/feature2_imply/","title":"Learn implied sample modifier in <code>peppy</code>","text":"<p>This vignette will show you how and why to use the implied attributes functionality of the <code>peppy</code> package. </p> <ul> <li> <p>basic information about the PEP concept on the project website</p> </li> <li> <p>broader theoretical description in the implied attributes documentation section.</p> </li> </ul>"},{"location":"peppy/code/feature2_imply/#problemgoal","title":"Problem/Goal","text":"<p>The example below demonstrates how and why to use implied attributes functionality to save your time and effort in case multiple sample attributes need to be defined for many samples and they follow certain patterns. Please consider the example below for reference:</p> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_imply/\"\nsample_table_ori = examples_dir + \"sample_table_pre.csv\"\n%cat $sample_table_ori | column -t -s, | cat\n</code></pre> <pre><code>sample_name  organism  time  file_path                        genome  genome_size\nfrog_0h      frog      0     data/lab/project/frog_0h.fastq\nfrog_1h      frog      1     data/lab/project/frog_1h.fastq\nhuman_1h     human     1     data/lab/project/human_1h.fastq  hg38    hs\nhuman_0h     human     0     data/lab/project/human_0h.fastq  hg38    hs\nmouse_1h     mouse     1     data/lab/project/mouse_1h.fastq  mm10    mm\nmouse_0h     mouse     0     data/lab/project/mouse_1h.fastq  mm10    mm\n</code></pre>"},{"location":"peppy/code/feature2_imply/#solution","title":"Solution","text":"<p>Noticeably, the samples with attributes <code>human</code> and <code>mouse</code> (in the <code>organism</code> column) follow two distinct patterns here. They have additional attributes in attributes <code>genome</code> and <code>genome_size</code> in the <code>sample_table.csv</code> file. Consequently you can use implied attributes to add those attributes to the sample annotations (set global, species-level attributes at the project level instead of duplicating that information for every sample that belongs to a species). The way how this process is carried out is indicated explicitly in the <code>project_config.yaml</code> file (presented below).</p> <pre><code>project_config_file = examples_dir + \"project_config.yaml\"\n%cat $project_config_file\n</code></pre> <pre><code>pep_version: '2.0.0'\nsample_table: sample_table.csv\nlooper:\n  output_dir: $HOME/hello_looper_results\n\nsample_modifiers:\n  imply:\n    - if:\n        organism: human\n      then:\n        genome: hg38\n        macs_genome_size: hs\n    - if:\n        organism: mouse\n      then:\n        genome: mm10\n        macs_genome_size: mm\n</code></pre> <p>Consequently, you can design <code>sample_modifiers.imply</code> - a multi-level key-value section in the <code>project_config.yaml</code> file. Note that the keys must match the column names and attributes in the <code>sample_annotations.csv</code> file. </p> <p>Let's introduce a few modifications to the original <code>sample_table.csv</code> file to use the <code>sample_modifiers.imply</code> section of the config. Simply skip the attributes that will be implied and let the <code>peppy</code> do the work for you.</p> <pre><code>sample_table = examples_dir + \"sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  organism  time  file_path\nfrog_0h      frog      0     data/lab/project/frog_0h.fastq\nfrog_1h      frog      1     data/lab/project/frog_1h.fastq\nhuman_1h     human     1     data/lab/project/human_1h.fastq\nhuman_0h     human     0     data/lab/project/human_0h.fastq\nmouse_1h     mouse     1     data/lab/project/mouse_1h.fastq\nmouse_0h     mouse     0     data/lab/project/mouse_1h.fastq\n</code></pre>"},{"location":"peppy/code/feature2_imply/#code","title":"Code","text":"<p>Load <code>peppy</code> and read in the project metadata by specifying the path to the <code>project_config.yaml</code>:</p> <pre><code>from peppy import Project\np = Project(project_config_file)\n</code></pre> <p>And inspect it:</p> <pre><code>print(p)\np.sample_table\n</code></pre> <pre><code>Project 'example_imply' (/Users/mstolarczyk/Uczelnia/UVA/code/peppy/tests/data/example_peps-cfg2/example_imply/project_config.yaml)\n6 samples: frog_0h, frog_1h, human_1h, human_0h, mouse_1h, mouse_0h\nSections: pep_version, sample_table, looper, sample_modifiers\n</code></pre> file_path organism sample_name time genome macs_genome_size sample_name frog_0h data/lab/project/frog_0h.fastq frog frog_0h 0 NaN NaN frog_1h data/lab/project/frog_1h.fastq frog frog_1h 1 NaN NaN human_1h data/lab/project/human_1h.fastq human human_1h 1 hg38 hs human_0h data/lab/project/human_0h.fastq human human_0h 0 hg38 hs mouse_1h data/lab/project/mouse_1h.fastq mouse mouse_1h 1 mm10 mm mouse_0h data/lab/project/mouse_1h.fastq mouse mouse_0h 0 mm10 mm <p>As you can see, the resulting samples are annotated the same way as if they were read from the original annotations file with attributes in the two last columns manually determined.</p>"},{"location":"peppy/code/feature3_derived/","title":"Learn derived attributes in <code>peppy</code>","text":"<p>This vignette will show you how and why to use the derived attributes functionality of the <code>peppy</code> package. </p> <ul> <li> <p>basic information about the PEP concept on the project website.</p> </li> <li> <p>broader theoretical description in the derived attributes documentation section.</p> </li> </ul>"},{"location":"peppy/code/feature3_derived/#problemgoal","title":"Problem/Goal","text":"<p>The example below demonstrates how to use the derived attributes to flexibly define the samples attributes the <code>file_path</code> column of the <code>sample_table.csv</code> file to match the file names in  your project. Please consider the example below for reference:</p> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_derive/\"\nsample_table_pre = examples_dir + \"sample_table_pre.csv\"\n%cat $sample_table_pre | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol  organism  time  file_path\npig_0h       RRBS      pig       0     data/lab/project/pig_0h.fastq\npig_1h       RRBS      pig       1     data/lab/project/pig_1h.fastq\nfrog_0h      RRBS      frog      0     data/lab/project/frog_0h.fastq\nfrog_1h      RRBS      frog      1     data/lab/project/frog_1h.fastq\n</code></pre>"},{"location":"peppy/code/feature3_derived/#solution","title":"Solution","text":"<p>As the name suggests the attributes in the specified attributes (here: <code>file_path</code>) can be derived from other ones. The way how this process is carried out is indicated explicitly in the <code>project_config.yaml</code> file (presented below). The name of the column is determined in the <code>sample_modifiers.derive.attributes</code> key-value pair, whereas the pattern for the attributes construction - in the <code>sample_modifiers.derive.sources</code> one. Note that the second level key (here: <code>source</code>) has to exactly match the attributes in the <code>file_path</code> column of the modified <code>sample_annotation.csv</code> (presented below).</p> <pre><code>project_config = examples_dir + \"project_config.yaml\"\n%cat $project_config | column -t -s, | cat\n</code></pre> <pre><code>pep_version: \"2.0.0\"\nsample_table: sample_table.csv\noutput_dir: \"$HOME/hello_looper_results\"\nsample_modifiers:\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: $HOME/data/lab/project/{organism}_{time}h.fastq\n      source2: /path/from/collaborator/weirdNamingScheme_{external_id}.fastq\n</code></pre> <p>Let's introduce a few modifications to the original <code>sample_annotation.csv</code> file to map the appropriate data sources from the <code>project_config.yaml</code> with attributes in the derived column - <code>[file_path]</code>:</p> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_derive/\"\nsample_table = examples_dir + \"sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol  organism  time  file_path\n\npig_0h       RRBS      pig       0     source1\n\npig_1h       RRBS      pig       1     source1\n\nfrog_0h      RRBS      frog      0     source1\n\nfrog_1h      RRBS      frog      1     source1\n</code></pre>"},{"location":"peppy/code/feature3_derived/#code","title":"Code","text":"<p>Import <code>peppy</code> and read in the project metadata by specifying the path to the <code>project_config.yaml</code>:</p> <pre><code>from peppy import Project\np = Project(project_config)\np.sample_table\n</code></pre> file_path organism protocol sample_name time sample_name pig_0h /Users/mstolarczyk/data/lab/project/pig_0h.fastq pig RRBS pig_0h 0 pig_1h /Users/mstolarczyk/data/lab/project/pig_1h.fastq pig RRBS pig_1h 1 frog_0h /Users/mstolarczyk/data/lab/project/frog_0h.fastq frog RRBS frog_0h 0 frog_1h /Users/mstolarczyk/data/lab/project/frog_1h.fastq frog RRBS frog_1h 1 <p>As you can see, the resulting samples are annotated the same way as if they were read from the original, unwieldy, annotations file.</p>"},{"location":"peppy/code/feature4_subsample_table/","title":"Learn sample subannotations in <code>peppy</code>","text":"<p>This vignette will show you how and why to use the subsample table functionality of the <code>peppy</code> package.</p> <ul> <li> <p>basic information about the PEP concept visit the project website.</p> </li> <li> <p>broader theoretical description in the subsample table documentation section.</p> </li> </ul>"},{"location":"peppy/code/feature4_subsample_table/#problemgoal","title":"Problem/Goal","text":"<p>This series of examples below demonstrates how and why to use sample subannoatation functionality in multiple cases to provide multiple input files of the same type for a single sample.</p>"},{"location":"peppy/code/feature4_subsample_table/#solutions","title":"Solutions","text":""},{"location":"peppy/code/feature4_subsample_table/#example-1-basic-sample-subannotation-table","title":"Example 1: basic sample subannotation table","text":"<p>This example demonstrates how the sample subannotation functionality is used. In this example, 2 samples have multiple input files that need merging (<code>frog_1</code> and <code>frog_2</code>), while 1 sample (<code>frog_3</code>) does not. Therefore, <code>frog_3</code> specifies its file in the <code>sample_table.csv</code> file, while the others leave that field blank and instead specify several files in the <code>subsample_table.csv</code> file.</p> <p>This example is made up of these components:</p> <ul> <li>Project config file:</li> </ul> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_subtable1/\"\nproject_config = examples_dir + \"project_config.yaml\"\n%cat $project_config\n</code></pre> <pre><code>pep_version: \"2.0.0\"\nsample_table: sample_table.csv\nsubsample_table: subsample_table.csv\noutput_dir: $HOME/example_results\n</code></pre> <ul> <li>Sample table:</li> </ul> <pre><code>sample_table = examples_dir + \"sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol       file\nfrog_1       anySampleType  multi\nfrog_2       anySampleType  multi\nfrog_3       anySampleType  multi\n</code></pre> <ul> <li>Subsample table:</li> </ul> <pre><code>subsample_table = examples_dir + \"subsample_table.csv\"\n%cat $subsample_table | column -t -s, | cat\n</code></pre> <pre><code>column: line too long\nsample_name  subsample_name  file\nfrog_1       sub_a           data/frog1a_data.txt\nfrog_1       sub_b           data/frog1b_data.txt\nfrog_1       sub_c           data/frog1c_data.txt\nfrog_2       sub_a           data/frog2a_data.txt\n</code></pre> <p>Let's load the project config, create the Project object and see if multiple files are present </p> <pre><code>from peppy import Project\np = Project(project_config)\nsamples = p.sample_table\nsamples\n</code></pre> file protocol sample_name subsample_name sample_name frog_1 [data/frog1a_data.txt, data/frog1b_data.txt, d... anySampleType frog_1 [sub_a, sub_b, sub_c] frog_2 [data/frog2a_data.txt, data/frog2b_data.txt] anySampleType frog_2 [sub_a, sub_b] frog_3 multi anySampleType frog_3 NaN"},{"location":"peppy/code/feature4_subsample_table/#example-2-subannotations-and-derived-attributes","title":"Example 2: subannotations and derived attributes","text":"<p>This example uses a <code>subsample_table.csv</code> file and a derived attributes to point to files. This is a rather complex example. Notice we must include the <code>file_id</code> column in the <code>sample_table.csv</code> file, and leave it blank; this is then populated by just some of the samples (<code>frog_1</code> and <code>frog_2</code>) in the <code>subsample_table.csv</code>, but is left empty for the samples that are not merged.</p> <p>This example is made up of these components:</p> <ul> <li>Project config file:</li> </ul> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_subtable2/\"\nproject_config = examples_dir + \"project_config.yaml\"\n%cat $project_config\n</code></pre> <pre><code>pep_version: \"2.0.0\"\nsample_table: sample_table.csv\nsubsample_table: subsample_table.csv\noutput_dir: $HOME/hello_looper_results\npipeline_interfaces: [../pipeline/pipeline_interface.yaml]\n\nsample_modifiers:\n  derive:\n    attributes: [file]\n    sources:\n      local_files: \"../data/{identifier}{file_id}_data.txt\"\n      local_files_unmerged: \"../data/{identifier}_data.txt\"\n</code></pre> <ul> <li>Sample table:</li> </ul> <pre><code>sample_table = examples_dir + \"sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>column: line too long\nsample_name  protocol       identifier  file\nfrog_1       anySampleType  frog1       local_files\nfrog_2       anySampleType  frog2       local_files\nfrog_3       anySampleType  frog3       local_files_unmerged\n</code></pre> <ul> <li>Subsample table:</li> </ul> <pre><code>subsample_table = examples_dir + \"subsample_table.csv\"\n%cat $subsample_table | column -t -s, | cat\n</code></pre> <pre><code>column: line too long\nsample_name  file_id  subsample_name\nfrog_1       a        a\nfrog_1       b        b\nfrog_1       c        c\nfrog_2       a        a\n</code></pre> <p>Let's load the project config, create the Project object and see if multiple files are present </p> <pre><code>p = Project(project_config)\nsamples = p.sample_table\nsamples\n</code></pre> file file_id identifier protocol sample_name subsample_name sample_name frog_1 [../data/frog1a_data.txt, ../data/frog1b_data.... [a, b, c] frog1 anySampleType frog_1 [a, b, c] frog_2 [../data/frog2a_data.txt, ../data/frog2b_data.... [a, b] frog2 anySampleType frog_2 [a, b] frog_3 ../data/frog3_data.txt NaN frog3 anySampleType frog_3 NaN frog_4 ../data/frog4_data.txt NaN frog4 anySampleType frog_4 NaN"},{"location":"peppy/code/feature4_subsample_table/#example-3-subannotations-and-expansion-characters","title":"Example 3: subannotations and expansion characters","text":"<p>This example gives the exact same results as Example 2, but in this case, uses a wildcard for <code>frog_2</code> instead of including it in the <code>subsample_table.csv</code> file. Since we can't use a wildcard and a subannotation for the same sample, this necessitates specifying a second data source class (<code>local_files_unmerged</code>) that uses an asterisk (<code>*</code>). The outcome is the same (<code>file</code> columns match).</p> <p>This example is made up of these components:</p> <ul> <li>Project config file:</li> </ul> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_subtable3/\"\n# need to cd to the example dir so that the glob works as expected\n%cd $examples_dir \nproject_config = \"project_config.yaml\"\n%cat $project_config\n</code></pre> <pre><code>/Users/mstolarczyk/Uczelnia/UVA/code/peppy/tests/data/example_peps-cfg2/example_subtable3\npep_version: \"2.0.0\"\nsample_table: sample_table.csv\nsubsample_table: subsample_table.csv\noutput_dir: $HOME/hello_looper_results\npipeline_interfaces: [../pipeline/pipeline_interface.yaml]\n\nsample_modifiers:\n  derive:\n    attributes: [file]\n    sources:\n      local_files: \"../data/{identifier}{file_id}_data.txt\"\n      local_files_unmerged: \"../data/{identifier}*_data.txt\"\n</code></pre> <ul> <li>Sample table:</li> </ul> <pre><code>%cat sample_table.csv | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol       identifier  file                  file_id\nfrog_1       anySampleType  frog1       local_files\nfrog_2       anySampleType  frog2       local_files_unmerged\nfrog_3       anySampleType  frog3       local_files_unmerged\nfrog_4       anySampleType  frog4       local_files_unmerged\n</code></pre> <ul> <li>Subsample table:</li> </ul> <pre><code>%cat subsample_table.csv | column -t -s, | cat\n</code></pre> <pre><code>sample_name  file_id\nfrog_1       a\nfrog_1       b\nfrog_1       c\n</code></pre> <p>Let's load the project config, create the Project object and see if multiple files are present </p> <pre><code>p = Project(project_config)\nsamples = p.sample_table\nsamples\n</code></pre> file file_id identifier protocol sample_name subsample_name sample_name frog_1 [../data/frog1a_data.txt, ../data/frog1b_data.... [a, b, c] frog1 anySampleType frog_1 [0, 1, 2] frog_2 [../data/frog2_data.txt, ../data/frog2a_data.t... NaN frog2 anySampleType frog_2 NaN frog_3 ../data/frog3_data.txt NaN frog3 anySampleType frog_3 NaN frog_4 ../data/frog4_data.txt NaN frog4 anySampleType frog_4 NaN"},{"location":"peppy/code/feature5_amend/","title":"Learn amendments in <code>peppy</code>","text":"<p>This vignette will show you how and why to use the amendments functionality of the <code>peppy</code> package. </p> <ul> <li> <p>basic information about the PEP concept on the project website.</p> </li> <li> <p>broader theoretical description in the amendments documentation section.</p> </li> </ul>"},{"location":"peppy/code/feature5_amend/#problemgoal","title":"Problem/Goal","text":"<p>The example below demonstrates how and why to use amendments project attribute to, e.g. define numerous similar projects in a single project config file. This functionality is extremely convenient when one has to define projects with small settings discreptancies, like different attributes in the annotation sheet. For example libraries <code>ABCD</code> and <code>EFGH</code> instead of the original <code>RRBS</code>.</p> <pre><code>examples_dir = \"../tests/data/example_peps-cfg2/example_amendments1/\"\nsample_table = examples_dir + \"sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol  organism  time  file_path\n\npig_0h       RRBS      pig       0     source1\n\npig_1h       RRBS      pig       1     source1\n\nfrog_0h      RRBS      frog      0     source1\n\nfrog_1h      RRBS      frog      1     source1\n</code></pre>"},{"location":"peppy/code/feature5_amend/#solution","title":"Solution","text":"<p>This can be achieved by using amendments section of <code>project_config.yaml</code> file (presented below). The attributes specified in the lowest levels of this section (here: <code>sample_table</code>) overwrite the original ones. Consequently, a completely new set of settings is determined with just this value changed. Moreover, multiple amendments can be defined in a single config file and activated at the same time. Based on the file presented below, two subprojects will be defined: <code>newLib</code> and <code>newLib2</code>.</p> <pre><code>project_config_file = examples_dir + \"project_config.yaml\"\n%cat $project_config_file\n</code></pre> <pre><code>pep_version: \"2.0.0\"\nsample_table: sample_table.csv\noutput_dir: $HOME/hello_looper_results\n\nsample_modifiers:\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: /data/lab/project/{organism}_{time}h.fastq\n      source2: /path/from/collaborator/weirdNamingScheme_{external_id}.fastq\nproject_modifiers:\n  amend:\n    newLib:\n      sample_table: sample_table_newLib.csv\n    newLib2:\n      sample_table: sample_table_newLib2.csv\n</code></pre> <p>Obviously, the amendments functionality can be combined with other <code>peppy</code> package options, e.g. imply and derive sample modifiers. The derive modifier is used in the example considered here (<code>derive</code> key in the <code>sample_modifiers</code> section of the config file).</p> <p>Files <code>sample_table_newLib.csv</code> and <code>sample_table_newLib2.csv</code> introduce different the <code>library</code> attributes. They are used in the subprojects <code>newLib</code> and <code>newLib2</code>, respectively</p> <pre><code>sample_table = examples_dir + \"sample_table_newLib.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol  organism  time  file_path\npig_0h       ABCD      pig       0     source1\npig_1h       ABCD      pig       1     source1\nfrog_0h      ABCD      frog      0     source1\nfrog_1h      ABCD      frog      1     source1\n</code></pre> <pre><code>sample_table = examples_dir + \"sample_table_newLib2.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol  organism  time  file_path\npig_0h       EFGH      pig       0     source1\npig_1h       EFGH      pig       1     source1\nfrog_0h      EFGH      frog      0     source1\nfrog_1h      EFGH      frog      1     source1\n</code></pre>"},{"location":"peppy/code/feature5_amend/#code","title":"Code","text":"<p>Import <code>peppy</code> and read in the project metadata by specifying the path to the <code>project_config.yaml</code></p> <pre><code>from peppy import Project\np = Project(project_config_file)\n</code></pre> <p>An appropriate message is displayed, which informs you what are the names of the amendments that you have defined in the <code>project_config.yaml</code> file. Nontheless, just the main project is \"active\".</p> <p>Let's inspect it:</p> <pre><code>p.sample_table\n</code></pre> file_path organism protocol sample_name time sample_name pig_0h /data/lab/project/pig_0h.fastq pig RRBS pig_0h 0 pig_1h /data/lab/project/pig_1h.fastq pig RRBS pig_1h 1 frog_0h /data/lab/project/frog_0h.fastq frog RRBS frog_0h 0 frog_1h /data/lab/project/frog_1h.fastq frog RRBS frog_1h 1 <p>The column <code>file_path</code> was derived and the <code>library</code> column holds the original attributes: <code>RRBS</code> for each sample.</p> <p>To \"activate\" any of the amendments just pass the names of the desired amendments to the <code>amendments</code> argument in the <code>Project</code> object constructor. </p> <p>In case you don't remember the subproject names run the <code>listAmendments()</code> metohods on the <code>Project</code> object, just like that:</p> <pre><code>p.list_amendments\n</code></pre> <pre><code>['newLib', 'newLib2']\n</code></pre> <pre><code>p_new_lib = Project(project_config_file, amendments = \"newLib\")\n</code></pre> <p>Let's inspect it:</p> <pre><code>p_new_lib.sample_table\n</code></pre> file_path organism protocol sample_name time sample_name pig_0h /data/lab/project/pig_0h.fastq pig ABCD pig_0h 0 pig_1h /data/lab/project/pig_1h.fastq pig ABCD pig_1h 1 frog_0h /data/lab/project/frog_0h.fastq frog ABCD frog_0h 0 frog_1h /data/lab/project/frog_1h.fastq frog ABCD frog_1h 1 <p>As you can see, the <code>library</code> column consists of new attributes (<code>ABCD</code>), which were defined in the <code>sample_table_newLib.csv</code> file.</p> <p>Amendments can be also activated interactively, after <code>Project</code> object has been crated. Let's activate the second amendment this way:</p> <pre><code>p_new_lib2 = p.activate_amendments(\"newLib2\")\np_new_lib2.sample_table\n</code></pre> file_path organism protocol sample_name time sample_name pig_0h /data/lab/project/pig_0h.fastq pig EFGH pig_0h 0 pig_1h /data/lab/project/pig_1h.fastq pig EFGH pig_1h 1 frog_0h /data/lab/project/frog_0h.fastq frog EFGH frog_0h 0 frog_1h /data/lab/project/frog_1h.fastq frog EFGH frog_1h 1"},{"location":"peppy/code/python-api/","title":"API","text":""},{"location":"peppy/code/python-api/#package-peppy-documentation","title":"Package <code>peppy</code> Documentation","text":"<p>Project configuration, particularly for logging.</p> <p>Project-scope constants may reside here, but more importantly, some setup here will provide a logging infrastructure for all of the project's modules. Individual modules and classes may provide separate configuration on a more local level, but this will at least provide a foundation.</p>"},{"location":"peppy/code/python-api/#class-project","title":"Class <code>Project</code>","text":"<p>A class to model a Project (collection of samples and metadata).</p>"},{"location":"peppy/code/python-api/#parameters","title":"Parameters:","text":"<ul> <li><code>cfg</code> (<code>str</code>):  Project config file (YAML) or sample table (CSV/TSV)with one row per sample to constitute project</li> <li><code>sample_table_index</code> (<code>str | Iterable[str]</code>):  name of the columns to setthe sample_table index to</li> <li><code>subsample_table_index</code> (<code>str | Iterable[str]</code>):  name of the columns to setthe subsample_table index to</li> <li><code>amendments</code> (<code>str | Iterable[str]</code>):  names of the amendments to activate</li> <li><code>amendments</code> (<code>Iterable[str]</code>):  amendments to use within configuration file</li> <li><code>defer_samples_creation</code> (<code>bool</code>):  whether the sample creation should be skipped</li> </ul>"},{"location":"peppy/code/python-api/#examples","title":"Examples:","text":"<pre><code>    from peppy import Project\n    prj = Project(cfg=\"ngs.yaml\")\n    samples = prj.samples\n</code></pre> <pre><code>def __init__(self, cfg: str=None, amendments: Union[str, Iterable[str]]=None, sample_table_index: Union[str, Iterable[str]]=None, subsample_table_index: Union[str, Iterable[str]]=None, defer_samples_creation: bool=False)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <pre><code>def activate_amendments(self, amendments)\n</code></pre> <p>Update settings based on amendment-specific values.</p> <p>This method will update Project attributes, adding new values associated with the amendments indicated, and in case of collision with an existing key/attribute the amendments' values will be favored.</p>"},{"location":"peppy/code/python-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>amendments</code> (<code>Iterable[str]</code>):  A string with amendmentnames to be activated</li> </ul>"},{"location":"peppy/code/python-api/#returns","title":"Returns:","text":"<ul> <li><code>peppy.Project</code>:  Updated Project instance</li> </ul>"},{"location":"peppy/code/python-api/#raises","title":"Raises:","text":"<ul> <li><code>TypeError</code>:  if argument to amendment parameter is null</li> <li><code>NotImplementedError</code>:  if this call is made on a project notcreated from a config file</li> </ul> <pre><code>def add_samples(self, samples)\n</code></pre> <p>Add list of Sample objects</p>"},{"location":"peppy/code/python-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>samples</code> (<code>peppy.Sample | Iterable[peppy.Sample]</code>):  samples to add</li> </ul> <pre><code>def amendments(self)\n</code></pre> <p>Return currently active list of amendments or None if none was activated</p>"},{"location":"peppy/code/python-api/#returns_1","title":"Returns:","text":"<ul> <li><code>Iterable[str]</code>:  a list of currently active amendment names</li> </ul> <pre><code>def attr_constants(self)\n</code></pre> <p>Update each Sample with constants declared by a Project. If Project does not declare constants, no update occurs.</p> <pre><code>def attr_derive(self, attrs=None)\n</code></pre> <p>Set derived attributes for all Samples tied to this Project instance</p> <pre><code>def attr_imply(self)\n</code></pre> <p>Infer value for additional field(s) from other field(s).</p> <p>Add columns/fields to the sample based on values in those already-set that the sample's project defines as indicative of implications for additional data elements for the sample.</p> <pre><code>def attr_merge(self)\n</code></pre> <p>Merge sample subannotations (from subsample table) with sample annotations (from sample_table)</p> <pre><code>def attr_remove(self)\n</code></pre> <p>Remove declared attributes from all samples that have them defined</p> <pre><code>def attr_synonyms(self)\n</code></pre> <p>Copy attribute values for all samples to a new one</p> <pre><code>def config(self)\n</code></pre> <p>Get the config mapping</p>"},{"location":"peppy/code/python-api/#returns_2","title":"Returns:","text":"<ul> <li><code>Mapping</code>:  config. May be formatted to comply with the mostrecent version specifications</li> </ul> <pre><code>def config_file(self)\n</code></pre> <p>Get the config file path</p>"},{"location":"peppy/code/python-api/#returns_3","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the config file</li> </ul> <pre><code>def copy(self)\n</code></pre> <p>Copy self to a new object.</p> <pre><code>def create_samples(self, modify: bool=False)\n</code></pre> <p>Populate Project with Sample objects</p> <pre><code>def deactivate_amendments(self)\n</code></pre> <p>Bring the original project settings back.</p>"},{"location":"peppy/code/python-api/#returns_4","title":"Returns:","text":"<ul> <li><code>peppy.Project</code>:  Updated Project instance</li> </ul>"},{"location":"peppy/code/python-api/#raises_1","title":"Raises:","text":"<ul> <li><code>NotImplementedError</code>:  if this call is made on a project notcreated from a config file</li> </ul> <pre><code>def description(self)\n</code></pre> <pre><code>def from_dict(cls, pep_dictionary: dict)\n</code></pre> <p>Init a peppy project instance from a dictionary representation of an already processed PEP.</p>"},{"location":"peppy/code/python-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>pep_dictionary</code> (<code>Dict[Any]</code>):  dict,_samples: list | dict, _subsamples: list[list | dict]}</li> </ul> <pre><code>def from_pandas(cls, samples_df: pandas.core.frame.DataFrame, sub_samples_df: List[pandas.core.frame.DataFrame]=None, config: dict=None)\n</code></pre> <p>Init a peppy project instance from a pandas Dataframe</p>"},{"location":"peppy/code/python-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>samples_df</code> (``):  in-memory pandas DataFrame object of samples</li> <li><code>sub_samples_df</code> (``):  in-memory list of pandas DataFrame objects of sub-samples</li> <li><code>config</code> (``):  dict of yaml file</li> </ul> <pre><code>def from_pep_config(cls, cfg: str=None, amendments: Union[str, Iterable[str]]=None, sample_table_index: Union[str, Iterable[str]]=None, subsample_table_index: Union[str, Iterable[str]]=None, defer_samples_creation: bool=False)\n</code></pre> <p>Init a peppy project instance from a yaml file</p>"},{"location":"peppy/code/python-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>cfg</code> (<code>str</code>):  Project config file (YAML) or sample table (CSV/TSV)with one row per sample to constitute project</li> <li><code>sample_table_index</code> (<code>str | Iterable[str]</code>):  name of the columns to setthe sample_table index to</li> <li><code>subsample_table_index</code> (<code>str | Iterable[str]</code>):  name of the columns to setthe subsample_table index to</li> <li><code>amendments</code> (<code>str | Iterable[str]</code>):  names of the amendments to activate</li> <li><code>amendments</code> (<code>Iterable[str]</code>):  amendments to use within configuration file</li> <li><code>defer_samples_creation</code> (<code>bool</code>):  whether the sample creation should be skipped</li> </ul> <pre><code>def from_sample_yaml(cls, yaml_file: str)\n</code></pre> <p>Init a peppy project instance from a yaml file</p>"},{"location":"peppy/code/python-api/#parameters_6","title":"Parameters:","text":"<ul> <li><code>yaml_file</code> (<code>str</code>):  path to yaml file</li> </ul> <pre><code>def get_description(self)\n</code></pre> <p>Infer project description from config file.</p> <p>The provided description has to be of class coercible to string</p>"},{"location":"peppy/code/python-api/#returns_5","title":"Returns:","text":"<ul> <li><code>str</code>:  inferred name for project.</li> </ul>"},{"location":"peppy/code/python-api/#raises_2","title":"Raises:","text":"<ul> <li><code>InvalidConfigFileException</code>:  if description is not of classcoercible to string</li> </ul> <pre><code>def get_sample(self, sample_name)\n</code></pre> <p>Get an individual sample object from the project.</p> <p>Will raise a ValueError if the sample is not found. In the case of multiple samples with the same name (which is not typically allowed), a warning is raised and the first sample is returned</p>"},{"location":"peppy/code/python-api/#parameters_7","title":"Parameters:","text":"<ul> <li><code>sample_name</code> (<code>str</code>):  The name of a sample to retrieve</li> </ul>"},{"location":"peppy/code/python-api/#returns_6","title":"Returns:","text":"<ul> <li><code>peppy.Sample</code>:  The requested Sample object</li> </ul>"},{"location":"peppy/code/python-api/#raises_3","title":"Raises:","text":"<ul> <li><code>ValueError</code>:  if there's no sample with the specified name defined</li> </ul> <pre><code>def get_samples(self, sample_names)\n</code></pre> <p>Returns a list of sample objects given a list of sample names</p>"},{"location":"peppy/code/python-api/#parameters_8","title":"Parameters:","text":"<ul> <li><code>sample_names</code> (<code>list</code>):  A list of sample names to retrieve</li> </ul>"},{"location":"peppy/code/python-api/#returns_7","title":"Returns:","text":"<ul> <li><code>list[peppy.Sample]</code>:  A list of Sample objects</li> </ul> <pre><code>def infer_name(self)\n</code></pre> <p>Infer project name from config file path.</p> <p>First assume the name is the folder in which the config file resides, unless that folder is named \"metadata\", in which case the project name is the parent of that folder.</p>"},{"location":"peppy/code/python-api/#returns_8","title":"Returns:","text":"<ul> <li><code>str</code>:  inferred name for project.</li> </ul>"},{"location":"peppy/code/python-api/#raises_4","title":"Raises:","text":"<ul> <li><code>InvalidConfigFileException</code>:  if the project lacks both a name anda configuration file (no basis, then, for inference)</li> <li><code>InvalidConfigFileException</code>:  if specified Project name is invalid</li> </ul> <pre><code>def is_sample_table_large(self)\n</code></pre> <pre><code>def list_amendments(self)\n</code></pre> <p>Return a list of available amendments or None if not declared</p>"},{"location":"peppy/code/python-api/#returns_9","title":"Returns:","text":"<ul> <li><code>Iterable[str]</code>:  a list of available amendment names</li> </ul> <pre><code>def load_samples(self)\n</code></pre> <p>Read the sample_table and subsample_tables into dataframes and store in the object root. The values sourced from the project config can be overwritten by the optional arguments.</p> <pre><code>def modify_samples(self)\n</code></pre> <p>Perform any sample modifications defined in the config.</p> <pre><code>def name(self)\n</code></pre> <pre><code>def parse_config_file(self, cfg_path: str=None, amendments: Iterable[str]=None)\n</code></pre> <p>Parse provided yaml config file and check required fields exist.</p>"},{"location":"peppy/code/python-api/#parameters_9","title":"Parameters:","text":"<ul> <li><code>cfg_path</code> (<code>str</code>):  path to the config file to read and parse</li> <li><code>amendments</code> (<code>Iterable[str]</code>):  Name of amendments to activate</li> </ul>"},{"location":"peppy/code/python-api/#raises_5","title":"Raises:","text":"<ul> <li><code>KeyError</code>:  if config file lacks required section(s)</li> </ul> <pre><code>def pep_version(self)\n</code></pre> <p>The declared PEP version string</p> <p>It is validated to make sure it is a valid PEP version string</p>"},{"location":"peppy/code/python-api/#returns_10","title":"Returns:","text":"<ul> <li><code>str</code>:  PEP version string</li> </ul>"},{"location":"peppy/code/python-api/#raises_6","title":"Raises:","text":"<ul> <li><code>InvalidConfigFileException</code>:  in case of invalid PEP version</li> </ul> <pre><code>def remove_samples(self, sample_names)\n</code></pre> <p>Remove Samples from Project</p>"},{"location":"peppy/code/python-api/#parameters_10","title":"Parameters:","text":"<ul> <li><code>sample_names</code> (<code>Iterable[str]</code>):  sample names to remove</li> </ul> <pre><code>def sample_name_colname(self)\n</code></pre> <p>Deprecated, please use <code>Project.sample_table_index</code> instead</p> <p>Name of the effective sample name containing column in the sample table. It is \"sample_name\" by default, but when it's missing it could be replaced by the selected sample table index, defined on the object instantiation stage.</p>"},{"location":"peppy/code/python-api/#returns_11","title":"Returns:","text":"<ul> <li><code>str</code>:  name of the column that consist of sample identifiers</li> </ul> <pre><code>def sample_table(self)\n</code></pre> <p>Get sample table. If any sample edits were performed, it will be re-generated</p>"},{"location":"peppy/code/python-api/#returns_12","title":"Returns:","text":"<ul> <li><code>pandas.DataFrame</code>:  a data frame with current samples attributes</li> </ul> <pre><code>def sample_table_index(self)\n</code></pre> <p>The effective sample table index.</p> <p>It is <code>sample_name</code> by default, but could be overwritten by the selected sample table index, defined on the object instantiation stage or in the project configuration file via <code>sample_table_index</code> field. That's the sample table index selection priority order: 1. Constructor specified 2. Config specified 3. Default: <code>sample_table</code></p>"},{"location":"peppy/code/python-api/#returns_13","title":"Returns:","text":"<ul> <li><code>str</code>:  name of the column that consist of sample identifiers</li> </ul> <pre><code>def samples(self)\n</code></pre> <p>Generic/base Sample instance for each of this Project's samples.</p>"},{"location":"peppy/code/python-api/#returns_14","title":"Returns:","text":"<ul> <li><code>Iterable[Sample]</code>:  Sample instance for eachof this Project's samples</li> </ul> <pre><code>def subsample_table(self)\n</code></pre> <p>Get subsample table</p>"},{"location":"peppy/code/python-api/#returns_15","title":"Returns:","text":"<ul> <li><code>pandas.DataFrame</code>:  a data frame with subsample attributes</li> </ul> <pre><code>def subsample_table_index(self)\n</code></pre> <p>The effective subsample table indexes.</p> <p>It is <code>[subasample_name, sample_name]</code> by default, but could be overwritten by the selected subsample table indexes, defined on the object instantiation stage or in the project configuration file via <code>subsample_table_index</code> field. That's the subsample table indexes selection priority order: 1. Constructor specified 2. Config specified 3. Default: <code>[subasample_name, sample_name]</code></p>"},{"location":"peppy/code/python-api/#returns_16","title":"Returns:","text":"<ul> <li><code>List[str]</code>:  names of the columns that consist of sample and subsample identifiers</li> </ul> <pre><code>def to_dict(self, extended: bool=False, orient: Literal['dict', 'list', 'series', 'split', 'tight', 'records', 'index']='dict') -&gt; dict\n</code></pre> <p>Convert the Project object to a dictionary.</p>"},{"location":"peppy/code/python-api/#parameters_11","title":"Parameters:","text":"<ul> <li><code>extended</code> (<code>bool</code>):  whether to produce complete project dict (used to reinit the project)</li> <li><code>orient</code> (<code>Literal</code>):  orientation of the returned df</li> </ul>"},{"location":"peppy/code/python-api/#returns_17","title":"Returns:","text":"<ul> <li><code>dict</code>:  a dictionary representation of the Project object</li> </ul>"},{"location":"peppy/code/python-api/#class-sample","title":"Class <code>Sample</code>","text":"<p>Class to model Samples based on a pandas Series.</p>"},{"location":"peppy/code/python-api/#parameters_12","title":"Parameters:","text":"<ul> <li><code>series</code> (<code>Mapping | pandas.core.series.Series</code>):  Sample's data.</li> </ul> <pre><code>def __init__(self, series, prj=None)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <pre><code>def attributes(self)\n</code></pre> <pre><code>def copy(self)\n</code></pre> <p>Copy self to a new object.</p> <pre><code>def derive_attribute(self, data_sources, attr_name)\n</code></pre> <p>Uses the template path provided in the project config section \"data_sources\" to piece together an actual path by substituting variables (encoded by \"{variable}\"\") with sample attributes.</p>"},{"location":"peppy/code/python-api/#parameters_13","title":"Parameters:","text":"<ul> <li><code>data_sources</code> (<code>Mapping</code>):  mapping from key name (as a value ina cell of a tabular data structure) to, e.g., filepath</li> <li><code>attr_name</code> (<code>str</code>):  Name of sample attribute(equivalently, sample sheet column) specifying a derived column.</li> </ul>"},{"location":"peppy/code/python-api/#returns_18","title":"Returns:","text":"<ul> <li><code>str</code>:  regex expansion of data source specified in configuration,with variable substitutions made</li> </ul>"},{"location":"peppy/code/python-api/#raises_7","title":"Raises:","text":"<ul> <li><code>ValueError</code>:  if argument to data_sources parameter is null/empty</li> </ul> <pre><code>def get_sheet_dict(self)\n</code></pre> <p>Create a K-V pairs for items originally passed in via the sample sheet. This is useful for summarizing; it provides a representation of the sample that excludes things like config files and derived entries.</p>"},{"location":"peppy/code/python-api/#returns_19","title":"Returns:","text":"<ul> <li><code>OrderedDict</code>:  mapping from name to value for data elementsoriginally provided via the sample sheet (i.e., the a map-like representation of the instance, excluding derived items)</li> </ul> <pre><code>def project(self)\n</code></pre> <p>Get the project mapping</p>"},{"location":"peppy/code/python-api/#returns_20","title":"Returns:","text":"<ul> <li><code>peppy.Project</code>:  project object the sample was created from</li> </ul> <pre><code>def to_dict(self, add_prj_ref=False)\n</code></pre> <p>Serializes itself as dict object.</p>"},{"location":"peppy/code/python-api/#parameters_14","title":"Parameters:","text":"<ul> <li><code>add_prj_ref</code> (<code>bool</code>):  whether the project reference bound do theSample object should be included in the YAML representation</li> </ul>"},{"location":"peppy/code/python-api/#returns_21","title":"Returns:","text":"<ul> <li><code>dict</code>:  dict representation of this Sample</li> </ul> <pre><code>def to_yaml(self, path, add_prj_ref=False)\n</code></pre> <p>Serializes itself in YAML format.</p>"},{"location":"peppy/code/python-api/#parameters_15","title":"Parameters:","text":"<ul> <li><code>path</code> (<code>str</code>):  A file path to write yaml to; provide this orthe subs_folder_path</li> <li><code>add_prj_ref</code> (<code>bool</code>):  whether the project reference bound do theSample object should be included in the YAML representation</li> </ul>"},{"location":"peppy/code/python-api/#class-peppyerror","title":"Class <code>PeppyError</code>","text":"<p>Base error type for peppy custom errors.</p> <pre><code>def __init__(self, msg)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Version Information: <code>peppy</code> v0.40.1, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"peppy/code/tutorial/","title":"Basic PEP example","text":"<p>This vignette will show you a simple example PEP-formatted project, and how to read it into python using the <code>peppy</code> package. This example comes from the example_peps repository in the example_basic folder.</p> <p>Start by importing <code>peppy</code>, and then let's take a look at the configuration file that defines our project:</p> <pre><code>import peppy\nexamples_dir = \"../tests/data/example_peps-cfg2/\"\n</code></pre> <pre><code>project_config_file = examples_dir + \"example_basic/project_config.yaml\"\n%cat $project_config_file\n</code></pre> <pre><code>pep_version: \"2.0.0\"\nsample_table: sample_table.csv\n</code></pre> <p>It's a basic YAML-formatted file with couple of sections: - <code>pep_version</code> (required; indicates with PEP spec version this config subscribes to) - <code>sample_table</code> (suggested; a pointer to sample annotation sheet. It is stored in the same folder as <code>project_config.yaml</code>)</p> <p>Now, let's now glance at that annotation file: </p> <pre><code>sample_table = examples_dir + \"example_basic/sample_table.csv\"\n%cat $sample_table | column -t -s, | cat\n</code></pre> <pre><code>sample_name  protocol       file\nfrog_1       anySampleType  data/frog1_data.txt\nfrog_2       anySampleType  data/frog2_data.txt\n</code></pre> <p>This <code>sample_table.csv</code> file is a basic CSV file, with rows corresponding to samples, and columns corresponding to sample attributes. Let's read this simple example project into Python using <code>peppy</code>:</p> <pre><code>project = peppy.Project(examples_dir + \"example_basic/project_config.yaml\")\n</code></pre> <p>Now, we have access to all the project metadata in easy-to-use form using python objects. We can browse the samples in the project like this:</p> <pre><code>project.samples[0].file\n</code></pre> <pre><code>'data/frog1_data.txt'\n</code></pre>"},{"location":"pipestat/","title":"Pipestat","text":""},{"location":"pipestat/#what-is-pipestat","title":"What is pipestat?","text":"<p>Pipestat standardizes reporting of pipeline results. It provides 1) a standard specification for how pipeline outputs should be stored; and 2) an implementation to easily write results to that format from within Python or from the command line.</p>"},{"location":"pipestat/#how-does-pipestat-work","title":"How does pipestat work?","text":"<p>A pipeline author defines all the outputs produced by a pipeline by writing a JSON-schema. The pipeline then uses pipestat to report pipeline outputs as the pipeline runs, either via the Python API or command line interface. The user configures results to be stored either in a YAML-formatted file, a PostgreSQL database or on PEPhub. The results are recorded according to the pipestat specification, in a standard, pipeline-agnostic way. This way, downstream software can use this specification to create universal tools for analyzing, monitoring, and visualizing pipeline results that will work with any pipeline or workflow.</p>"},{"location":"pipestat/#quick-start","title":"Quick start","text":"<p>Check out the quickstart guide. See API Usage and CLI Usage.</p>"},{"location":"pipestat/backends/","title":"Back-end types","text":"<p>The pipestat specification describes three backend types for storing results: a YAML-formatted file, a PostgreSQL database or reporting results to PEPhub. This flexibility makes pipestat useful for a wide variety of use cases. Some users just need a simple text file for smaller-scale needs, which is convenient and universal, requiring no database infrastructure. For larger-scale systems, a database back-end is necessary. The pipestat specification provides a layer that spans the three possibilities, so that reports can be made in the same way, regardless of which back-end is used in a particular use case.</p> <p>By using the <code>pipestat</code> package to write results, the pipeline author need not be concerned with database connections or dealing with racefree file writing, as these tasks are already implemented. The user who runs the pipeline will simply configure the pipestat backend as required.</p> <p>Both backends organize the results in a hierarchy which is always structured this way:</p> <p></p>"},{"location":"pipestat/backends/#file","title":"File","text":"<p>The changes reported using the <code>report</code> method of <code>PipestatManger</code> will be securely written to the file. Currently only YAML format is supported. </p> <p>Example:</p> <pre><code>psm = PipestatManager(results_file_path=\"result_file.yaml\", schema_path=schema_file)\n</code></pre> <p>For the YAML file backend, each file represents a namespace. The file always begins with a single top-level key which indicates the namespace. Second-level keys correspond to the record identifiers; third-level keys correspond to result identifiers, which point to the reported values. The values can then be any of the allowed pipestat data types, which include both basic and advanced data types.</p> <pre><code>default_pipeline_name:\n  project: {}\n  sample:\n    sample_1:\n      meta:\n        pipestat_modified_time: '2025-10-01 12:48:58'\n        pipestat_created_time: '2025-10-01 12:48:58'\n      number_of_things: '12'\n</code></pre>"},{"location":"pipestat/backends/#postgresql-database","title":"PostgreSQL database","text":"<p>This option gives the user the possibility to use a fully fledged database to back <code>PipestatManager</code>. </p> <p>Example:</p> <p><pre><code>psm = PipestatManager(config_file=\"config_file.yaml\", schema_path=schema_file)\n</code></pre> where the config file has the following (example) values:</p> <pre><code>schema_path: sample_output_schema.yaml\ndatabase:\n  dialect: postgresql\n  driver: psycopg\n  name: pipestat-test\n  user: postgres\n  password: pipestat-password\n  host: 127.0.0.1\n  port: 5432\n</code></pre> <p>For the PostgreSQL backend, the name of the database is configurable and defined in the config file in <code>database.name</code>. The database is structured like this:</p> <ul> <li>The namespace corresponds to the name of the table.</li> <li>The record identifier is indicated in the unique <code>record_identifier</code> column in that table.</li> <li>Each result is specified as a column in the table, with the column name corresponding to the result identifier</li> <li>The values in the cells for a record and result identifier correspond to the actual data values reported for the given result.</li> </ul> <p></p>"},{"location":"pipestat/backends/#pep-on-pephub","title":"PEP on PEPhub","text":"<p>This option gives the user the possibility to use PEPhub as a backend for results. </p> <pre><code>psm = PipestatManager(pephub_path=pephubpath, schema_path=\"sample_output_schema.yaml\")\n</code></pre> <p>All three backends can be configured using the config file. However, the PostgreSQL backend must use a config file.</p>"},{"location":"pipestat/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"pipestat/changelog/#0122-2025-09-25","title":"[0.12.2] - 2025-09-25","text":""},{"location":"pipestat/changelog/#fixed","title":"Fixed","text":"<ul> <li>Fix setting with copy warning for pephub backend #206</li> <li>Fix #212</li> </ul>"},{"location":"pipestat/changelog/#0121-2025-02-10","title":"[0.12.1] - 2025-02-10","text":""},{"location":"pipestat/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>fix checking for records during summarize failing because aggregate_results file had not yet been resolved #215</li> </ul>"},{"location":"pipestat/changelog/#0120-2025-01-16","title":"[0.12.0] - 2025-01-16","text":""},{"location":"pipestat/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>portable report now has proper file extension in messaging.</li> <li>add exception to pipestat summarize if there are no results to report #210</li> <li>fix spaces in html files #211</li> <li>add output_dir parameter to psm.table</li> </ul>"},{"location":"pipestat/changelog/#0110-2024-10-02","title":"[0.11.0] - 2024-10-02","text":""},{"location":"pipestat/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>for output schema, make samples an array type and nest under items #204</li> <li>pipeline_name not setting correctly #207</li> <li>bug with objects populating html report</li> </ul>"},{"location":"pipestat/changelog/#0100-2024-07-18","title":"[0.10.0] - 2024-07-18","text":""},{"location":"pipestat/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>allow for bool or boolean in schema #189</li> <li>fix pipestat summarize bugs re: inaccurate paths #190</li> <li>add support for sqlite dbbackend #192</li> </ul>"},{"location":"pipestat/changelog/#added","title":"Added","text":"<ul> <li>pephub backend #125</li> </ul>"},{"location":"pipestat/changelog/#092-2024-06-24","title":"[0.9.2] - 2024-06-24","text":""},{"location":"pipestat/changelog/#changed","title":"Changed","text":"<ul> <li>User can override pipeline name via parameter or config file, otherwise look at output_schema, then fall back on default as last resort.</li> <li>Allow pipestat to proceed without creating a results file backend IF using \"{record_identifier}\" in the file path, helps address Looper #471</li> <li>Reduce overall verbosity when creating backends</li> </ul>"},{"location":"pipestat/changelog/#091-2024-04-24","title":"[0.9.1] - 2024-04-24","text":""},{"location":"pipestat/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Pipestat summarize html report columns now show stats only  #148.</li> <li>When creating HTML reports from both sample and project level results (multi results), only sample-level results show in the main index table #150. </li> <li>Add more complex schema during dependency check to mitigate false test failures regarding different output schemas #181. </li> </ul>"},{"location":"pipestat/changelog/#090-2024-04-19","title":"[0.9.0] - 2024-04-19","text":""},{"location":"pipestat/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Bug with rm_record for filebackend</li> <li>Bug when using record_identifier via env variable and the CLI</li> </ul>"},{"location":"pipestat/changelog/#added_1","title":"Added","text":"<ul> <li>Added results history and history retrieval for both file and db backends via <code>retrieve_history</code> #177.</li> <li>Added <code>remove_record</code> to Pipestat manager object (it is no longer only on backend classes)</li> <li>Added <code>meta</code> key to each record for the file backend</li> <li>db backend will now create an additional sql history table</li> <li>Reporting history is toggleable</li> </ul>"},{"location":"pipestat/changelog/#changed_1","title":"Changed","text":"<ul> <li>Removing the last result no longer removes the entire record.</li> <li><code>pipestat_created_time, and</code>pipestat_modified_time<code>now live under the</code>meta` key.</li> <li><code>history</code> lives under the <code>meta</code> key for the filebackend.</li> </ul>"},{"location":"pipestat/changelog/#082-2024-02-22","title":"[0.8.2] - 2024-02-22","text":""},{"location":"pipestat/changelog/#changed_2","title":"Changed","text":"<ul> <li>Changed yacman requirement and using FutureYamlConfigManager.</li> </ul>"},{"location":"pipestat/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Issue with retrieving similar record_identifiers, #159</li> </ul>"},{"location":"pipestat/changelog/#081-2024-02-07","title":"[0.8.1] - 2024-02-07","text":""},{"location":"pipestat/changelog/#changed_3","title":"Changed","text":"<ul> <li>Readme to reflect docker db configuration for testing. #145</li> <li>Added dependency warning when attempting to run pytest suite without optional dependencies. #146</li> <li>Remove most docs in favor of new docs location: https://pep.databio.org/pipestat/</li> </ul>"},{"location":"pipestat/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Ensure log files are gathered for portable reports, #149</li> <li>Fix minor html report title bugs #151</li> </ul>"},{"location":"pipestat/changelog/#080-2024-01-25","title":"[0.8.0] - 2024-01-25","text":""},{"location":"pipestat/changelog/#added_2","title":"Added","text":"<ul> <li>added <code>portable</code> flag to pipestat summarize to create a shareable version of the html report.</li> <li>added setting <code>index: True</code> within output schema to index specific results for DB backend.</li> </ul>"},{"location":"pipestat/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>pipestat summarize: objects drop down now only shows sample-level</li> </ul>"},{"location":"pipestat/changelog/#070-2024-01-17","title":"[0.7.0] - 2024-01-17","text":""},{"location":"pipestat/changelog/#added_3","title":"Added","text":"<ul> <li><code>__iter__</code> now takes limit and cursor arguments to create an iterator from <code>select_records</code></li> </ul>"},{"location":"pipestat/changelog/#changed_4","title":"Changed","text":"<ul> <li>updated pydantic requirement to be &gt;= 2.5.3</li> </ul>"},{"location":"pipestat/changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Get summary files: Objects YAML button now works.</li> </ul>"},{"location":"pipestat/changelog/#060-2023-12-22","title":"[0.6.0] - 2023-12-22","text":""},{"location":"pipestat/changelog/#added_4","title":"Added","text":"<ul> <li><code>select_records</code>, which allows for a single API for selecting attributes (result_identifiers) given filter_conditions and/or columns</li> <li><code>retrieve_one</code>, and <code>retrieve_many</code> which allows for selecting one or multiple records given record_identifier</li> <li>pipestat reader submodule to read DB results via FastAPI endpoints: <code>pipestat serve --config \"config.yaml\"</code></li> <li>ability to create <code>SamplePipestatManager</code> and <code>ProjectSamplePipestatManager</code> which are sample/project specific PipestatManager objects.</li> <li>PipestatBoss wrapper class which holds <code>SamplePipestatManager</code> and <code>ProjectSamplePipestatManager</code> classes.</li> <li><code>to_dict</code> methods to parsed schema object.</li> <li><code>select_distinct</code> function which retrieves unique results for a list of attributes.</li> <li><code>pipestat link</code> which creates a directory of symlinks for reported results.</li> <li><code>list_recent_results</code> which allows for retrieving records filtered via a start and end time.</li> <li>reporting and retrieving results via item access,e.g. <code>psm[\"sample1\", \"name_of_something\"] = \"name_of_something_string\"</code> or <code>result = psm[\"sample1\"]</code></li> <li>pipestat now supports one results file per sample by using <code>{record_identifier}</code> in the results_file_path.</li> </ul>"},{"location":"pipestat/changelog/#fixed_11","title":"Fixed","text":"<ul> <li>path expansion when creating database url.</li> <li>added jinja2 requirement.</li> <li><code>pipeline_name</code> column not populating in postgres db backend.</li> <li>pipestat will now create subdirectories during results_file creation.</li> <li>fixed bugs and polished both report generation and stats table generation per https://github.com/pepkit/pipestat/pull/131 </li> </ul>"},{"location":"pipestat/changelog/#changed_5","title":"Changed","text":"<ul> <li>Removed <code>retrieve</code>, <code>get_one_record</code>, <code>get_records function</code>.</li> <li>Removed <code>get_orm</code> and replace with <code>get_model</code>.</li> <li>Removed <code>get_table_name</code> function.</li> <li>Refactor:</li> <li><code>sample_name</code> -&gt; <code>record_identifier</code>.</li> <li><code>pipeline_type</code> has been removed from most functions.</li> <li>added optional dependencies for the database backend and pipestat reader, e.g. <code>pip install pipestat[dbbackend]</code>.</li> </ul>"},{"location":"pipestat/changelog/#052-2023-11-30","title":"[0.5.2] - 2023-11-30","text":""},{"location":"pipestat/changelog/#fixed_12","title":"Fixed","text":"<ul> <li>add jinja2 to requirements doc.</li> </ul>"},{"location":"pipestat/changelog/#051-2023-08-14","title":"[0.5.1] - 2023-08-14","text":""},{"location":"pipestat/changelog/#fixed_13","title":"Fixed","text":"<ul> <li>fix schema_path issue when building html reports and obtaining schema from config file.</li> </ul>"},{"location":"pipestat/changelog/#050-2023-08-08","title":"[0.5.0] - 2023-08-08","text":""},{"location":"pipestat/changelog/#added_5","title":"Added","text":"<ul> <li>Add summarize function to generate static html results report.</li> </ul>"},{"location":"pipestat/changelog/#041-2023-07-26","title":"[0.4.1] - 2023-07-26","text":""},{"location":"pipestat/changelog/#fix","title":"Fix","text":"<ul> <li>ensure Pipestat uses proper versions of Pydantic, Yacman.</li> </ul>"},{"location":"pipestat/changelog/#040-2023-06-29","title":"[0.4.0] - 2023-06-29","text":""},{"location":"pipestat/changelog/#changed_6","title":"Changed","text":"<ul> <li>Remove attmap dependency</li> <li>Migrate to SQLModel for Object\u2013relational mapping (ORM)</li> <li>Renamed <code>list_existing_results</code> to <code>list_results</code> and allow for returning a subset of results.</li> <li>Refactor: </li> <li><code>namespace</code> -&gt; <code>project_name</code>, </li> <li><code>pipeline_id</code> -&gt; <code>pipeline_name</code>, </li> <li><code>record_identifier</code> -&gt; <code>sample_name</code></li> </ul>"},{"location":"pipestat/changelog/#added_6","title":"Added","text":"<ul> <li>Add 'init -g' for creating generic configuration file.</li> <li>Add ability to pass function to format reported results.</li> </ul>"},{"location":"pipestat/changelog/#031-2022-08-18","title":"[0.3.1] - 2022-08-18","text":""},{"location":"pipestat/changelog/#fix_1","title":"Fix","text":"<ul> <li>database connection error</li> </ul>"},{"location":"pipestat/changelog/#030-2021-10-30","title":"[0.3.0] - 2021-10-30","text":""},{"location":"pipestat/changelog/#added_7","title":"Added","text":"<ul> <li><code>select_distinct</code> for select distinct values from given table and column(s)</li> </ul>"},{"location":"pipestat/changelog/#020-2021-10-25","title":"[0.2.0] - 2021-10-25","text":""},{"location":"pipestat/changelog/#added_8","title":"Added","text":"<ul> <li>optional parameter for specify returning columns with <code>select_txt</code></li> </ul>"},{"location":"pipestat/changelog/#010-2021-06-24","title":"[0.1.0] - 2021-06-24","text":"<p>This update introduces some backwards-incompatible changes due to database interface redesign</p>"},{"location":"pipestat/changelog/#changed_7","title":"Changed","text":"<ul> <li>database interface type from a driver to an Object\u2013relational mapping (ORM) approach</li> </ul>"},{"location":"pipestat/changelog/#added_9","title":"Added","text":"<ul> <li>results highlighting support</li> <li>database column parametrizing from the results schema</li> <li>static typing</li> <li>possibility to initialize the <code>PipestatManager</code> object (or use the <code>pipestat status</code> CLI) with no results schema defined for pipeline status management even when backed by a database; Issue #1</li> </ul>"},{"location":"pipestat/changelog/#004-2021-04-02","title":"[0.0.4] - 2021-04-02","text":""},{"location":"pipestat/changelog/#added_10","title":"Added","text":"<ul> <li>config validation</li> <li>typing in code</li> </ul>"},{"location":"pipestat/changelog/#003-2021-03-12","title":"[0.0.3] - 2021-03-12","text":""},{"location":"pipestat/changelog/#added_11","title":"Added","text":"<ul> <li>possibility to initialize the <code>PipestatManager</code> object (or use the <code>pipestat status</code> CLI) with no results schema defined for pipeline status management; Issue #1</li> </ul>"},{"location":"pipestat/changelog/#002-2021-02-22","title":"[0.0.2] - 2021-02-22","text":""},{"location":"pipestat/changelog/#added_12","title":"Added","text":"<ul> <li>initial package release</li> </ul>"},{"location":"pipestat/config/","title":"Pipestat configuration file specification","text":"<p>All the relevant pieces of information required to initialize the <code>PipestatManager</code> object can be provided to the object constructor or as a command line argument in the form of a YAML-formatted pipestat configuration file.</p> <pre><code>record_identifier: &lt;unique record identifier for either sample or project-level reporting&gt;\nschema_path: &lt;path to the schema&gt;\nresults_file_path: &lt;path to results file&gt; # either \"results_file_path\"\ndatabase: # or DB login credentials\n  name: &lt;database name&gt;\n  user: &lt;user name&gt;\n  password: &lt;user password&gt;\n  host: &lt;database address&gt;\n  port: &lt;database port&gt;\n  dialect: &lt;database type&gt;\n  driver: &lt;python database driver&gt;\n</code></pre> <p>If both <code>results_file_path</code> and DB login credentials are provided, the YAML results file is given priority.</p> <p>Any of the settings specified in the configuration file, apart from the database login credentials, can be overwritten with the respectively named arguments in the <code>PipestatManager</code> object constructor. Therefore, the configuration file is required only if the intended pipestat back-end is a database or if using pipestat in tandem with Looper.</p>"},{"location":"pipestat/config/#example","title":"Example","text":"<p>For the PostgreSQL instance has been started in a container, with the following command:</p> <pre><code>docker run -d\n    --name pipestat-postgres \\\n    -p 5432:5432 \\\n    -e POSTGRES_PASSWORD=b4fd34f^Fshdwede \\\n    -e POSTGRES_USER=john \\\n    -e POSTGRES_DB=pipestat-test \\\n    -v postgres-data:/var/lib/postgresql/data postgres\n</code></pre> <p>The configuration file should look like this:</p> <pre><code>schema_path: /path/to/schema.yaml\ndatabase:\n  name: pipestat-test\n  user: john\n  password: b4fd34f^Fshdwede\n  host: 127.0.0.1\n  port: 5432\n  dialect: postgresql\n  driver: psycopg\n</code></pre>"},{"location":"pipestat/configuration/","title":"Pipestat configuration","text":"<p>Pipestat requires a few pieces of information to run:</p> <ul> <li>a pipeline_name to write into, usually contained within the schema file (see below).</li> <li>a path to the schema file that describes results that can be reported, e.g. <code>\"your/path/sample_output_schema.yaml\"</code>: <pre><code>title: An example Pipestat output schema\ndescription: A pipeline that uses pipestat to report sample and project level results.\ntype: object\nproperties:\n  pipeline_name: \"default_pipeline_name\"\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        result_name:\n          type: string\n          description: \"ResultName\"\n</code></pre></li> <li>backend info: either path to a YAML-formatted file or pipestat config with PostgreSQL database login credentials. </li> <li>Note that the config file can also contain a path to the yaml-formatted results file: <pre><code>schema_path: sample_output_schema.yaml\n#The config can contain either a results_file_path (file backend) or a database connection (database backend)\n# results_file_path takes priority and will create a PipestatManager with a file backend\nresults_file_path: results_file.yaml \ndatabase:\n  dialect: postgresql\n  driver: psycopg\n  name: pipestat-test\n  user: postgres\n  password: pipestat-password\n  host: 127.0.0.1\n  port: 5432\n</code></pre></li> </ul> <p>Beginning with v0.10.0, there is also support for reporting results directly to PEPHub. Simply give PipestatManager a path to a PEPHub registry path:</p> <pre><code>psm = PipestatManager(pephub_path=\"databio/pipestat_demo:default\", schema_path=my_schema_file_path)\n</code></pre> <p>You can also place this in the configuration file:</p> <pre><code>pephub_path: \"databio/pipestat_demo:default\"\nschema_path: sample_output_schema.yaml\n</code></pre> <p>Apart from that, there are many other optional configuration points that have defaults. Please refer to the environment variables reference to learn about the the optional configuration options and their meaning.</p>"},{"location":"pipestat/configuration/#configuration-sources","title":"Configuration sources","text":"<p>Pipestat configuration can come from 3 sources, with the following priority:</p> <ol> <li><code>PipestatManager</code> constructor</li> <li>Pipestat configuration file</li> <li>Environment variables</li> </ol>"},{"location":"pipestat/contributing/","title":"Contributing","text":"<p>We welcome contributions from the community.</p>"},{"location":"pipestat/contributing/#suggestions-and-feedback","title":"Suggestions and feedback","text":"<p>Please open an issue on the GitHub issue tracker with suggestions, bug reports, or other feedback.</p>"},{"location":"pipestat/env-vars/","title":"Environment variables in pipestat","text":"<p>Both the command line interface (CLI) and Python API support a collection of environment variables, which can be used to configure pipestat actions.</p> <p>Here is a list of the supported environment variables:</p> Environment variable API argument Description PIPESTAT_RECORD_IDENTIFIER record_identifier record identifier to report for. This creates a weak bound to the record, which can be overridden in this object method calls PIPESTAT_CONFIG config path to the configuration file or a mapping with the config file content PIPESTAT_RESULTS_FILE results_file_path YAML file to report into, if file is used as the object back-end PIPESTAT_RESULTS_SCHEMA schema_path path to the output schema that formalizes the results structure PIPESTAT_STATUS_SCHEMA status_schema_path path to the status schema that formalizes the status flags structure PIPESTAT_TYPE pipeline_type sample or project-level PipestatManager"},{"location":"pipestat/install/","title":"Installing pipestat","text":""},{"location":"pipestat/install/#minimal-install-for-file-backend","title":"Minimal install for file backend","text":"<p>Install pipestat from PyPI with <code>pip</code>: </p> <pre><code>pip install pipestat\n</code></pre> <p>Confirm installation by calling <code>pipestat -h</code> on the command line. If the <code>pipestat</code> executable is not in your <code>$PATH</code>, append this to your <code>.bashrc</code> or <code>.profile</code> (or <code>.bash_profile</code> on macOS):</p> <pre><code>export PATH=~/.local/bin:$PATH\n</code></pre>"},{"location":"pipestat/install/#optional-dependencies-for-database-backend","title":"Optional dependencies for database backend","text":"<p>Pipestat can use either a file or a database as the backend for recording results. The default installation only provides file backend. To install dependencies required for the database backend:</p> <pre><code>pip install pipestat['dbbackend']\n</code></pre>"},{"location":"pipestat/install/#optional-dependencies-for-pipestat-reader","title":"Optional dependencies for pipestat reader","text":"<p>To install dependencies for the included <code>pipestatreader</code> submodule:</p> <pre><code>pip install pipestat['pipestatreader']\n</code></pre>"},{"location":"pipestat/multi/","title":"Multi Pipelines and Multi Result Files","text":""},{"location":"pipestat/multi/#enable-writing-to-a-single-results-file-from-multiple-pipelines","title":"Enable writing to a single results file from multiple pipelines","text":"<p>The pipestat API supports writing to one results.yaml file via multiple pipelines. This can be enabled during pipestat initialization by setting <code>multi_pipelines</code> to true:</p> <pre><code>psm = PipestatManager(results_file_path=result_file, schema_path=schema_file, multi_pipelines=True)\n</code></pre> <p>Example Results file <pre><code>test_pipeline_01:\n  project: {}\n  sample:\n    RECORD1:\n      number_of_things: 50000\n      pipestat_created_time: '2024-04-04 17:23:56'\n      pipestat_modified_time: '2024-04-04 18:28:59'\n      name_of_something: Another_Name\n\ntest_pipeline_02:\n  project:\n    RECORD2:\n      number_of_things: 42\n      pipestat_created_time: '2024-04-04 18:23:56'\n      pipestat_modified_time: '2024-04-04 19:28:59'\n  sample: {}\n</code></pre></p>"},{"location":"pipestat/multi/#enable-writing-each-record-to-its-own-results-file","title":"Enable writing each record to its own results file","text":"<p>Pipestat also supports writing a record's results to a separate file for every record. This can be achieved by placing the string <code>{record_identifier}</code> in the designated <code>results_file_path</code> field of the pipestat configuration file.</p> <pre><code>schema_path: sample_output_schema.yaml\nresults_file_path: \"${DATA}/processed/results_pipeline/{record_identifier}/stats.yaml\"\n</code></pre> <p>Here is an example from the PEPATAC pipeline which currently uses this functionality via a <code>looper</code> config file:</p> <pre><code>name: PEPATAC_tutorial\npep_config: tutorial_refgenie_project_config.yaml\n\noutput_dir: \"${TUTORIAL}/processed/\"\npipeline_interfaces:\n  sample: [\"${TUTORIAL}/tools/pepatac/sample_pipeline_interface.yaml\"]\n  project: [\"${TUTORIAL}/tools/pepatac/project_pipeline_interface.yaml\"]\n\npipestat:\n  results_file_path: \"${TUTORIAL}/processed/results_pipeline/{record_identifier}/stats.yaml\"\n</code></pre>"},{"location":"pipestat/pipestat-schema/","title":"How to write a pipestat schema","text":""},{"location":"pipestat/pipestat-schema/#introduction","title":"Introduction","text":"<p>Pipestat requires a schema, in which all the results that the pipeline can report are specified. </p> <p>It is written in JSON schema which defines specific data types:</p>"},{"location":"pipestat/pipestat-schema/#data-types","title":"Data types","text":"<p>Each result reported by a pipeline must have a specified data type. The supported basic types include:</p> <ul> <li>string</li> <li>number</li> <li>integer</li> <li>boolean</li> <li>null</li> </ul> <p>Pipestat also extends the json schema vocabulary by adding two additional types, which are common results of a pipeline: <code>image</code> and <code>file</code>. These types require reporting objects with the following attributes:</p> <ul> <li><code>file</code>:<ul> <li><code>path</code>: path to the reported file</li> <li><code>title</code>: human readable description of the file</li> </ul> </li> <li><code>image</code>:<ul> <li><code>path</code>: path to the reported image, usually PDF</li> <li><code>thumbnail</code>: path to the reported thumbnail, usually PNG or JPEG</li> <li><code>title</code>: human readable description of the image</li> </ul> </li> </ul>"},{"location":"pipestat/pipestat-schema/#complex-objects","title":"Complex objects","text":"<p>Pipestat also supports reporting more complex objects</p>"},{"location":"pipestat/pipestat-schema/#unsupported-data-types","title":"Unsupported data types","text":"<p><code>tuples</code> are currently unsupported for reporting and retrieving.</p>"},{"location":"pipestat/pipestat-schema/#a-simple-example","title":"A simple example","text":"<p>The pipestat output schema is a YAML-formatted file. The top level keys are the unique result identifiers. The associated values are jsonschema types. The <code>type</code> attribute is required. This is an example of a minimal component, specifying only an identifier, and its type:</p> <pre><code>result_identifier:\n  type: &lt;type&gt;\n</code></pre> <p>Here, <code>result_identifier</code> can be whatever name you want to use to identify this result. Here's a simple schema example that showcases most of the supported types:</p> <pre><code>title: Example Pipestat Output Schema\ndescription: A pipeline that uses pipestat to report sample level results.\ntype: object\nproperties:\n  pipeline_name: \"default_pipeline_name\"\n  samples:\n    type: array\n    properties: # result identifiers are properties of the samples object\n      number_of_things:\n        type: integer\n        description: \"Number of things\"\n      percentage_of_things:\n        type: number\n        description: \"Percentage of things\"\n      name_of_something:\n        type: string\n        description: \"Name of something\"\n      switch_value:\n        type: boolean\n        description: \"Is the switch on or off\"\n</code></pre> <p>The top level schema is of <code>type</code> <code>object</code>. It contains properties that define <code>samples</code>. Here, the <code>samples</code>'s properties are the results. So in the above example, the results that can be reported are: <code>number_of_things</code>,<code>percentage_of_things</code>,<code>name_of_something</code>, and <code>switch_value</code>.</p>"},{"location":"pipestat/pipestat-schema/#a-more-complex-example","title":"A more complex example","text":"<p>Here's a more complex schema example that showcases some of the more advanced jsonschema features:</p> <pre><code>title: An example Pipestat output schema\ndescription: A pipeline that uses pipestat to report sample and project level results.\ntype: object\nproperties:\n  pipeline_name: \"default_pipeline_name\"\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        number_of_things:\n          type: integer\n          description: \"Number of things\"\n        percentage_of_things:\n          type: number\n          description: \"Percentage of things\"\n        name_of_something:\n          type: string\n          description: \"Name of something\"\n        switch_value:\n          type: boolean\n          description: \"Is the switch on or off\"\n        md5sum:\n          type: string\n          description: \"MD5SUM of an object\"\n          highlight: true\n        collection_of_images:\n          description: \"This store collection of values or objects\"\n          type: array\n          items:\n            properties:\n                prop1:\n                  description: \"This is an example file\"\n                  $ref: \"#/$defs/file\"\n        output_file_in_object:\n          type: object\n          properties:\n            prop1:\n              description: \"This is an example file\"\n              $ref: \"#/$defs/file\"\n            prop2:\n              description: \"This is an example image\"\n              $ref: \"#/$defs/image\"\n          description: \"Object output\"\n        output_file_in_object_nested:\n          type: object\n          description: First Level\n          properties:\n            prop1:\n              type: object\n              description: Second Level\n              properties:\n                prop2:\n                  type: integer\n                  description: Third Level\n        output_file:\n          $ref: \"#/$defs/file\"\n          description: \"This a path to the output file\"\n        output_image:\n          $ref: \"#/$defs/image\"\n          description: \"This a path to the output image\"\n$defs:\n  image:\n    type: object\n    object_type: image\n    properties:\n      path:\n        type: string\n      thumbnail_path:\n        type: string\n      title:\n        type: string\n    required:\n      - path\n      - thumbnail_path\n      - title\n  file:\n    type: object\n    object_type: file\n    properties:\n      path:\n        type: string\n      title:\n        type: string\n    required:\n      - path\n      - title\n</code></pre> <p>In this example, we define reusable type definitions in <code>image</code> and <code>file</code>.</p> <p>For more details, see pipestat specification.</p>"},{"location":"pipestat/pipestat-specification/","title":"Pipestat specification","text":"Pipestat specification <ul> <li>Introduction</li> <li>Terminology</li> <li>Data types</li> <li>Pipestat output schema<ul> <li>Pipestat output schema format</li> <li>Results highlighting</li> </ul> </li> <li>Status schema</li> <li>Backends<ul> <li>YAML file</li> <li>PostgreSQL database</li> </ul> </li> </ul>"},{"location":"pipestat/pipestat-specification/#introduction","title":"Introduction","text":"<p>Pipelines, or workflows, are made from a set of commands that process input data and produce results. These results may take many forms, such as simple statistics, string variables, images, or processed data files. How do pipelines structure the results they produce? There is no standard structure for results, so usually, it's done differently for each pipeline. This restricts the portability of the outputs of pipelines, and makes it difficult to write software that can process results from a variety of different pipelines. As a result, each pipeline author usually writes dedicated report functions for each pipeline.</p> <p>Pipestat provides a formal specification for how a pipeline should structure its results. Therefore, any pipeline that follows the pipestat specification will record results in the same way. This makes it possible to build generic report software that can work with any pipestat-compatible pipeline, offloading the task of making pretty result reports to generic tools.</p> <p>This document outlines the specification for pipestat results. If your pipeline stores results like this, then downstream tools that read pipestat results will be able to build nice summaries of your pipeline runs automatically. To write results according to this specification, you can use the reference implementation (the <code>pipestat</code> python package), or you can simply write your results to this specification using whatever system you like.</p>"},{"location":"pipestat/pipestat-specification/#terminology","title":"Terminology","text":"<ul> <li>result: An element produced by a pipeline. Results have defined data types, described herein.</li> <li>result identifier. The name of a result, such as <code>aligned_read_count</code> or <code>duplication_rate</code>.</li> <li>value. The actual data for an output result for a given record.</li> <li>namespace: A way to group results that belong together. In the api, this is referenced via <code>pipeline_name</code>. This is typically an identifier for a particular pipeline, like <code>rnaseq-pipeline</code>. All results from this pipeline will share this namespace.</li> <li>record identifier. An identifier for a particular pipeline run, such as a sample name.</li> <li>pipestat specification: the way to structure a set of results stored from one or more pipeline runs.</li> <li>backend. The technology underlying the result storage, which can be either a simple file or a database.</li> </ul>"},{"location":"pipestat/pipestat-specification/#data-types","title":"Data types","text":"<p>Each result reported by a pipeline must have a specified data type. Pipestat is built on jsonschema types, so the jsonschema documentation outlines the basic available types. These types are:</p> <ul> <li>string</li> <li>number</li> <li>integer</li> <li>boolean</li> <li>null</li> </ul> <p>Importantly, pipestat extends the jsonschema vocabulary by adding two additional types, which are common results of a pipeline: <code>image</code> and <code>file</code>. These types require reporting objects with the following attributes:</p> <ul> <li><code>file</code>:<ul> <li><code>path</code>: path to the reported file</li> <li><code>title</code>: human readable description of the file</li> </ul> </li> <li><code>image</code>:<ul> <li><code>path</code>: path to the reported image, usually PDF</li> <li><code>thumbnail</code>: path to the reported thumbnail, usually PNG or JPEG</li> <li><code>title</code>: human readable description of the image</li> </ul> </li> </ul>"},{"location":"pipestat/pipestat-specification/#pipestat-output-schema","title":"Pipestat output schema","text":"<p>Each pipestat-compatible pipeline must define a pipestat output schema. The pipestat schema is where the pipeline author describes the results produced by the pipeline. The pipestat schema specifies:</p> <ol> <li>The result identifiers; that is, the immutable names of all the results reported by this pipeline.</li> <li>The data types associated with each result.</li> <li>Human-readable description of what each result represents.</li> </ol> <p>As a pipeline developer, your schema defines and describes all the important results to be recorded from your pipeline.</p> <p>Pipestat uses the schema as a base for creating a collection of self-contained result-specific jsonschema schemas that are used to validate the reported results prior to storing them.</p>"},{"location":"pipestat/pipestat-specification/#pipestat-output-schema-format","title":"Pipestat output schema format","text":"<p>The pipestat output schema is a YAML-formatted file. The top level keys are the unique result identifiers. The associated values are jsonschema types. The <code>type</code> attribute is required. This is an example of a minimal component, specifying only an identifier, and its type:</p> <pre><code>result_identifier:\n  type: &lt;type&gt;\n</code></pre> <p>Here, <code>result_identifier</code> can be whatever name you want to use to identify this result. Here's a simple schema example that showcases most of the supported types:</p> <pre><code>title: Example Pipestat Output Schema\ndescription: A pipeline that uses pipestat to report sample level results.\ntype: object\nproperties:\n  pipeline_name: \"default_pipeline_name\"\n  samples:\n    type: array\n    items:\n      type: object\n      properties: # result identifiers are properties of the samples object\n        number_of_things:\n          type: integer\n          description: \"Number of things\"\n        percentage_of_things:\n          type: number\n          description: \"Percentage of things\"\n        name_of_something:\n          type: string\n          description: \"Name of something\"\n        switch_value:\n          type: boolean\n          description: \"Is the switch on or off\"\n</code></pre> <p>Here's a more complex schema example that showcases some of the more advanced jsonschema features:</p> <pre><code>title: An example Pipestat output schema\ndescription: A pipeline that uses pipestat to report sample and project level results.\ntype: object\nproperties:\n  pipeline_name: \"default_pipeline_name\"\n  samples:\n    type: array\n    items:\n      type: object\n      properties:\n        number_of_things:\n          type: integer\n          description: \"Number of things\"\n        percentage_of_things:\n          type: number\n          description: \"Percentage of things\"\n        name_of_something:\n          type: string\n          description: \"Name of something\"\n        switch_value:\n          type: boolean\n          description: \"Is the switch on or off\"\n        md5sum:\n          type: string\n          description: \"MD5SUM of an object\"\n          highlight: true\n        collection_of_images:\n          description: \"This store collection of values or objects\"\n          type: array\n          items:\n            properties:\n                prop1:\n                  description: \"This is an example file\"\n                  $ref: \"#/$defs/file\"\n        output_file_in_object:\n          type: object\n          properties:\n            prop1:\n              description: \"This is an example file\"\n              $ref: \"#/$defs/file\"\n            prop2:\n              description: \"This is an example image\"\n              $ref: \"#/$defs/image\"\n          description: \"Object output\"\n        output_file_in_object_nested:\n          type: object\n          description: First Level\n          properties:\n            prop1:\n              type: object\n              description: Second Level\n              properties:\n                prop2:\n                  type: integer\n                  description: Third Level\n        output_file:\n          $ref: \"#/$defs/file\"\n          description: \"This a path to the output file\"\n        output_image:\n          $ref: \"#/$defs/image\"\n          description: \"This a path to the output image\"\n$defs:\n  image:\n    type: object\n    object_type: image\n    properties:\n      path:\n        type: string\n      thumbnail_path:\n        type: string\n      title:\n        type: string\n    required:\n      - path\n      - thumbnail_path\n      - title\n  file:\n    type: object\n    object_type: file\n    properties:\n      path:\n        type: string\n      title:\n        type: string\n    required:\n      - path\n      - title\n</code></pre>"},{"location":"pipestat/pipestat-specification/#results-highlighting","title":"Results highlighting","text":"<p>The pipestat specification allows to highlight results by adding <code>highlight: true</code> attribute under result identifier in the schema file. In the example below the <code>log_file</code> result will be highlighted.</p> <pre><code>number_of_things:\n  type: integer\n  description: \"Number of things\"\npercentage_of_things:\n  type: number\n  description: \"Percentage of things\"\nlog_file:\n  type: file\n  description: \"Path to the log file\"\n  highlight: true\n</code></pre> <p>The highlighted results can be later retrieved by pipestat clients via <code>PipestatManager.highlighted_results</code> property, which simply returns a list of result identifiers.</p>"},{"location":"pipestat/pipestat-specification/#status-schema","title":"Status schema","text":"<p>Apart from results reporting pipestat provides a robust pipeline status management system, which can be used to report pipeline status from within the pipeline and monitor pipeline's status in other software. Status schema file defines the possible pipeline status identifiers and provides other metadata, like <code>description</code> or <code>color</code> for display purposes.</p> <p>Here's an example of the pipestat status schema, which at the same time is the default status schema shipped with the pipestat Python package:</p> <pre><code>running:\n  description: \"the pipeline is running\"\n  color: [30, 144, 255] # dodgerblue\ncompleted:\n  description: \"the pipeline has completed\"\n  color: [50, 205, 50] # limegreen\nfailed:\n  description: \"the pipeline has failed\"\n  color: [220, 20, 60] # crimson\nwaiting:\n  description: \"the pipeline is waiting\"\n  color: [240, 230, 140] # khaki\npartial:\n  description: \"the pipeline stopped before completion point\"\n  color: [169, 169, 169] # darkgray\n</code></pre> <p>As depicted above the top-level attributes are the status identifiers. Within each section two attributes are required:</p> <ul> <li><code>description</code> (<code>str</code>) a freeform text exhaustively describing the status code.</li> <li><code>color</code> (<code>list[int]</code>) an array of integers of length 3 which specifies the desired color associated with the status in RGB color model.</li> </ul>"},{"location":"pipestat/pipestat-specification/#backends","title":"Backends","text":"<p>The pipestat specification describes two backend types for storing results: a YAML-formatted file or a PostgreSQL database. This flexibility makes pipestat useful for a wide variety of use cases. Some users just need a simple text file for smaller-scale needs, which is convenient and universal, requiring no database infrastructure. For larger-scale systems, a database back-end is necessary. The pipestat specification provides a layer that spans the two possibilities, so that reports can be made in the same way, regardless of which back-end is used in a particular use case.</p> <p>By using the <code>pipestat</code> package to write results, the pipeline author need not be concerned with database connections or dealing with racefree file writing, as these tasks are already implemented. The user who runs the pipeline will simply configure the pipestat backend as required.</p> <p>Both backends organize the results in a hierarchy which is always structured this way:</p> <p></p>"},{"location":"pipestat/pipestat-specification/#yaml-file","title":"YAML file","text":"<p>For the YAML file backend, each file represents a namespace. The file always begins with a single top-level key which indicates the namespace. Second-level keys correspond to the record identifiers; third-level keys correspond to result identifiers, which point to the reported values. The values can then be any of the allowed pipestat data types, which include both basic and advanced data types.</p> <pre><code>my_namespace:\n    record1:\n        my_result: 10\n        my_result1:\n            key: \"value1\"\n    record2:\n        my_result: 3\n        my_result1:\n            key: \"value2\"\n</code></pre> <p>A more concrete example would be:</p> <pre><code>rnaseq-pipe:\n    patient1:\n        duplicate_rate: 10\n        genomic_distribution:\n            promoter: 15\n            enhancer: 85\n    patient2:\n        duplicate_rate: 3\n        genomic_distribution:\n            promoter: 30\n            enhancer: 70\n</code></pre>"},{"location":"pipestat/pipestat-specification/#postgresql-database","title":"PostgreSQL database","text":"<p>For the PostgreSQL backend, the name of the database is configurable and defined in the config file in <code>database.name</code>. The database is structured like this:</p> <ul> <li>The namespace corresponds to the name of the table.</li> <li>The record identifier is indicated in the unique <code>record_identifier</code> column in that table.</li> <li>Each result is specified as a column in the table, with the column name corresponding to the result identifier</li> <li>The values in the cells for a record and result identifier correspond to the actual data values reported for the given result.</li> </ul> <p></p>"},{"location":"pipestat/report_statuses/","title":"Reporting record identifier/sample statuses","text":"<p>Ensure you set a directory to contain the status files if using a file backend:</p> <pre><code>psm = PipestatManager(results_file_path=\"results.yaml\",schema_path=\"output_schema.yaml\", flag_file_dir=\"./flags/\")\n</code></pre> <p>Now, when running your pipeline you can simply set the status based of the current record:</p> <pre><code>psm.set_status(record_identifier=\"sample1\", status_identifier=\"completed\")\n</code></pre> <p>All statuses are defined in the schemas/status_schema.yaml file:</p> <pre><code>running:\n  description: \"the pipeline is running\"\n  color: [30, 144, 255] # dodgerblue\ncompleted:\n  description: \"the pipeline has completed\"\n  color: [50, 205, 50] # limegreen\nfailed:\n  description: \"the pipeline has failed\"\n  color: [220, 20, 60] # crimson\nwaiting:\n  description: \"the pipeline is waiting\"\n  color: [240, 230, 140] # khaki\npartial:\n  description: \"the pipeline stopped before completion point\"\n  color: [169, 169, 169] # darkgray\n</code></pre>"},{"location":"pipestat/report_statuses/#coming-from-looper-make-sure-to-set-this-flag-directory-in-the-looper-config-file","title":"Coming from Looper? Make sure to set this flag directory in the looper config file:","text":"<pre><code>pep_config: ./metadata/pep_config.yaml # pephub registry path or local path\noutput_dir: ./results\npipeline_interfaces:\n  - pipeline/pipeline_interface.yaml\npipestat:\n  project_name: count_lines\n  results_file_path: results.yaml\n  flag_file_dir: results/flags\n</code></pre>"},{"location":"pipestat/results_records/","title":"Terminology","text":"<p>Key concepts when using pipestat:</p>"},{"location":"pipestat/results_records/#samplesrecords","title":"Samples/Records:","text":"<ul> <li>record identifier. An identifier for a particular pipeline run, such as a sample name. If you are using pipestat in tandem with looper, record_identifier = sample_name.</li> </ul>"},{"location":"pipestat/results_records/#results","title":"Results:","text":"<ul> <li>result identifier. The name of a result, such as <code>aligned_read_count</code> or <code>duplication_rate</code>.</li> <li>result: An element produced by a pipeline. Results have defined data types, described herein.</li> <li>value. The actual data for an output result for a given record.</li> </ul>"},{"location":"pipestat/results_records/#misc","title":"Misc:","text":"<ul> <li>namespace: A way to group results that belong together. In the api, this is referenced via <code>pipeline_name</code>. This is typically an identifier for a particular pipeline, like <code>rnaseq-pipeline</code>. All results from this pipeline will share this namespace.</li> <li>pipestat specification: the way to structure a set of results stored from one or more pipeline runs.</li> <li>backend. The technology underlying the result storage, which can be either a simple file or a database.</li> </ul>"},{"location":"pipestat/summarize/","title":"Sharing reported results","text":"<p>Pipestat currently has the ability to create HTML reports of reported pipeline results.</p> <pre><code>from pipestat import PipestatManager\n\npsm = PipestatManager(schema_path=\"sample_output_schema.yaml\", results_file_path=\"my_results.yaml\")\n\npsm.summarize(output_dir=\"/home/output_dir\")\n\n# You can also create a portable version to share via email etc\npsm.summarize(output_dir=\"/home/output_dir\",portable= True )\n</code></pre> <p>Similarly this can be accomplished via the CLI:</p> <pre><code>pipestat summarize --results-file my_results.yaml --schema output_schema.yaml --portable\n</code></pre>"},{"location":"pipestat/testing/","title":"Testing Configuration","text":""},{"location":"pipestat/testing/#optional-dependencies","title":"Optional Dependencies","text":"<p>Note: to run the pytest suite locally, you will need to install the related requirements:</p> <pre><code>cd pipestat\n\npip install -r requirements/requirements-test.txt\n</code></pre>"},{"location":"pipestat/testing/#database-backend-configuration-for-tests","title":"Database Backend Configuration for Tests","text":"<p>Many of the tests require a postgres database to be set up otherwise many of the tests will skip.</p> <p>We recommend using docker: <pre><code>docker run --rm -it --name pipestat_test_db \\\n    -e POSTGRES_USER=pipestatuser \\\n    -e POSTGRES_PASSWORD=shgfty^8922138$^! \\\n    -e POSTGRES_DB=pipestat-test \\\n    -p 127.0.0.1:5432:5432 \\\n    postgres\n</code></pre></p>"},{"location":"pipestat/usage/","title":"Usage reference","text":"<p>Pipestat offers a CLI that can be access via the <code>pipestat</code> command in the shell. It offers complete control over reporting, inspecting, etc, via a series of subcommands.</p> <p>Here you can see the command-line usage instructions for the main command and for each subcommand:</p>"},{"location":"pipestat/usage/#pipestat-help","title":"<code>pipestat --help</code>","text":"<pre><code>version: 0.12.1\nusage: pipestat [-h] [--version] [--silent] [--verbosity V] [--logdev] {report,inspect,remove,retrieve,status,init,summarize,link,serve,history} ...\n\npipestat - report pipeline results\n\npositional arguments:\n  {report,inspect,remove,retrieve,status,init,summarize,link,serve,history}\n    report              Report a result.\n    inspect             Inspect a database.\n    remove              Remove a result.\n    retrieve            Retrieve a result.\n    status              Manage pipeline status.\n    init                Initialize generic config file\n    summarize           Generates HTML Report\n    link                Create symlinks of reported files\n    serve               Initializes pipestatreader API\n    history             Retrieve history of reported results for one record identifier\n\noptions:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  --silent              Silence logging. Overrides verbosity.\n  --verbosity V         Set logging level (1-5 or logging module level name)\n  --logdev              Expand content of logging message format.\n\nPipestat standardizes reporting of pipeline results and pipeline status management. \nIt formalizes a way for pipeline developers and downstream tools developers to communicate \n-- results produced by a pipeline can easily andreliably become an input for downstream analyses. \nA PipestatManager object exposes an API for interacting with the results and pipeline status and \ncan be backed by either a YAML-formatted file or a database.\n</code></pre>"},{"location":"pipestat/usage/#pipestat-report-help","title":"<code>pipestat report --help</code>","text":"<pre><code>usage: pipestat report [-h] [-n N] [-f F] [-c C] [-a] [-s S] [--status-schema ST]\n                       [--flag-dir FD] [-p P] -i I [-r R] -v V [-o] [-t]\n\nReport a result.\n\noptions:\n  -h, --help                   show this help message and exit\n  -n N, --project-name N       Name of the pipeline to report result for. If not provided\n                               'PIPESTAT_PROJECT_NAME' env var will be used. Currently not\n                               set\n  -f F, --results-file F       Path to the YAML file where the results will be stored.\n                               This file will be used as pipestat backend and to restore\n                               the reported results across sessions\n  -c C, --config C             Path to the YAML configuration file. If not provided\n                               'PIPESTAT_CONFIG' env var will be used. Currently not set\n  -a, --database-only          Whether the reported data should not be stored in the\n                               memory, only in the database.\n  -s S, --schema S             Path to the schema that defines the results that can be\n                               reported. If not provided 'PIPESTAT_RESULTS_SCHEMA' env var\n                               will be used. Currently not set\n  --status-schema ST           Path to the status schema. Default will be used if not\n                               provided: /home/drc/GITHUB/pipestat/pipestat/venv/lib/pytho\n                               n3.10/site-packages/pipestat/schemas/status_schema.yaml\n  --flag-dir FD                Path to the flag directory in case YAML file is the\n                               pipestat backend.\n  -p P, --pipeline-type P      project or sample level pipeline type.\n  -i I, --result-identifier I  ID of the result to report; needs to be defined in the\n                               schema\n  -r R, --record-identifier R  ID of the record to report the result for. If not provided\n                               'PIPESTAT_RECORD_IDENTIFIER' env var will be used.\n                               Currently not set\n  -v V, --value V              Value of the result to report\n  -o, --overwrite              Whether the result should override existing ones in case of\n                               name clashes\n  -t, --skip-convert           Whether skip result type conversion into the required class\n                               in case it does not meet the schema requirements\n</code></pre>"},{"location":"pipestat/usage/#pipestat-inspect-help","title":"<code>pipestat inspect --help</code>","text":"<pre><code>usage: pipestat inspect [-h] [-n N] [-f F] [-c C] [-a] [-s S] [--status-schema ST]\n                        [--flag-dir FD] [-p P] [-d]\n\nInspect a database.\n\noptions:\n  -h, --help               show this help message and exit\n  -n N, --project-name N   Name of the pipeline to report result for. If not provided\n                           'PIPESTAT_PROJECT_NAME' env var will be used. Currently not set\n  -f F, --results-file F   Path to the YAML file where the results will be stored. This\n                           file will be used as pipestat backend and to restore the\n                           reported results across sessions\n  -c C, --config C         Path to the YAML configuration file. If not provided\n                           'PIPESTAT_CONFIG' env var will be used. Currently not set\n  -a, --database-only      Whether the reported data should not be stored in the memory,\n                           only in the database.\n  -s S, --schema S         Path to the schema that defines the results that can be\n                           reported. If not provided 'PIPESTAT_RESULTS_SCHEMA' env var\n                           will be used. Currently not set\n  --status-schema ST       Path to the status schema. Default will be used if not\n                           provided:\n                           /home/drc/GITHUB/pipestat/pipestat/venv/lib/python3.10/site-\n                           packages/pipestat/schemas/status_schema.yaml\n  --flag-dir FD            Path to the flag directory in case YAML file is the pipestat\n                           backend.\n  -p P, --pipeline-type P  project or sample level pipeline type.\n  -d, --data               Whether to display the data\n</code></pre>"},{"location":"pipestat/usage/#pipestat-remove-help","title":"<code>pipestat remove --help</code>","text":"<pre><code>usage: pipestat remove [-h] [-n N] [-f F] [-c C] [-a] [-s S] [--status-schema ST]\n                       [--flag-dir FD] [-p P] -i I [-r R]\n\nRemove a result.\n\noptions:\n  -h, --help                   show this help message and exit\n  -n N, --project-name N       Name of the pipeline to report result for. If not provided\n                               'PIPESTAT_PROJECT_NAME' env var will be used. Currently not\n                               set\n  -f F, --results-file F       Path to the YAML file where the results will be stored.\n                               This file will be used as pipestat backend and to restore\n                               the reported results across sessions\n  -c C, --config C             Path to the YAML configuration file. If not provided\n                               'PIPESTAT_CONFIG' env var will be used. Currently not set\n  -a, --database-only          Whether the reported data should not be stored in the\n                               memory, only in the database.\n  -s S, --schema S             Path to the schema that defines the results that can be\n                               reported. If not provided 'PIPESTAT_RESULTS_SCHEMA' env var\n                               will be used. Currently not set\n  --status-schema ST           Path to the status schema. Default will be used if not\n                               provided: /home/drc/GITHUB/pipestat/pipestat/venv/lib/pytho\n                               n3.10/site-packages/pipestat/schemas/status_schema.yaml\n  --flag-dir FD                Path to the flag directory in case YAML file is the\n                               pipestat backend.\n  -p P, --pipeline-type P      project or sample level pipeline type.\n  -i I, --result-identifier I  ID of the result to report; needs to be defined in the\n                               schema\n  -r R, --record-identifier R  ID of the record to report the result for. If not provided\n                               'PIPESTAT_RECORD_IDENTIFIER' env var will be used.\n                               Currently not set\n</code></pre>"},{"location":"pipestat/usage/#pipestat-retrieve-help","title":"<code>pipestat retrieve --help</code>","text":"<pre><code>usage: pipestat retrieve [-h] [-n N] [-f F] [-c C] [-a] [-s S] [--status-schema ST]\n                         [--flag-dir FD] [-p P] [-r R]\n\nRetrieve a result.\n\noptions:\n  -h, --help                   show this help message and exit\n  -n N, --project-name N       Name of the pipeline to report result for. If not provided\n                               'PIPESTAT_PROJECT_NAME' env var will be used. Currently not\n                               set\n  -f F, --results-file F       Path to the YAML file where the results will be stored.\n                               This file will be used as pipestat backend and to restore\n                               the reported results across sessions\n  -c C, --config C             Path to the YAML configuration file. If not provided\n                               'PIPESTAT_CONFIG' env var will be used. Currently not set\n  -a, --database-only          Whether the reported data should not be stored in the\n                               memory, only in the database.\n  -s S, --schema S             Path to the schema that defines the results that can be\n                               reported. If not provided 'PIPESTAT_RESULTS_SCHEMA' env var\n                               will be used. Currently not set\n  --status-schema ST           Path to the status schema. Default will be used if not\n                               provided: /home/drc/GITHUB/pipestat/pipestat/venv/lib/pytho\n                               n3.10/site-packages/pipestat/schemas/status_schema.yaml\n  --flag-dir FD                Path to the flag directory in case YAML file is the\n                               pipestat backend.\n  -p P, --pipeline-type P      project or sample level pipeline type.\n  -r R, --record-identifier R  ID of the record to report the result for. If not provided\n                               'PIPESTAT_RECORD_IDENTIFIER' env var will be used.\n                               Currently not set\n</code></pre>"},{"location":"pipestat/usage/#pipestat-status-help","title":"<code>pipestat status --help</code>","text":"<pre><code>usage: pipestat status [-h] {set,get} ...\n\nManage pipeline status.\n\npositional arguments:\n  {set,get}\n    set       Set status.\n    get       Get status.\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"pipestat/usage/#pipestat-status-get-help","title":"<code>pipestat status get --help</code>","text":"<pre><code>usage: pipestat status get [-h] [-n N] [-f F] [-c C] [-a] [-s S] [--status-schema ST]\n                           [--flag-dir FD] [-r R] [-p P]\n\nGet status.\n\noptions:\n  -h, --help                   show this help message and exit\n  -n N, --project-name N       Name of the pipeline to report result for. If not provided\n                               'PIPESTAT_PROJECT_NAME' env var will be used. Currently not\n                               set\n  -f F, --results-file F       Path to the YAML file where the results will be stored.\n                               This file will be used as pipestat backend and to restore\n                               the reported results across sessions\n  -c C, --config C             Path to the YAML configuration file. If not provided\n                               'PIPESTAT_CONFIG' env var will be used. Currently not set\n  -a, --database-only          Whether the reported data should not be stored in the\n                               memory, only in the database.\n  -s S, --schema S             Path to the schema that defines the results that can be\n                               reported. If not provided 'PIPESTAT_RESULTS_SCHEMA' env var\n                               will be used. Currently not set\n  --status-schema ST           Path to the status schema. Default will be used if not\n                               provided: /home/drc/GITHUB/pipestat/pipestat/venv/lib/pytho\n                               n3.10/site-packages/pipestat/schemas/status_schema.yaml\n  --flag-dir FD                Path to the flag directory in case YAML file is the\n                               pipestat backend.\n  -r R, --record-identifier R  ID of the record to report the result for. If not provided\n                               'PIPESTAT_RECORD_IDENTIFIER' env var will be used.\n                               Currently not set\n  -p P, --pipeline-type P      project or sample level pipeline type.\n</code></pre>"},{"location":"pipestat/usage/#pipestat-status-set-help","title":"<code>pipestat status set --help</code>","text":"<pre><code>usage: pipestat status set [-h] [-n N] [-f F] [-c C] [-a] [-s S] [--status-schema ST]\n                           [--flag-dir FD] [-r R] [-p P]\n                           status_identifier\n\nSet status.\n\npositional arguments:\n  status_identifier            Status identifier to set.\n\noptions:\n  -h, --help                   show this help message and exit\n  -n N, --project-name N       Name of the pipeline to report result for. If not provided\n                               'PIPESTAT_PROJECT_NAME' env var will be used. Currently not\n                               set\n  -f F, --results-file F       Path to the YAML file where the results will be stored.\n                               This file will be used as pipestat backend and to restore\n                               the reported results across sessions\n  -c C, --config C             Path to the YAML configuration file. If not provided\n                               'PIPESTAT_CONFIG' env var will be used. Currently not set\n  -a, --database-only          Whether the reported data should not be stored in the\n                               memory, only in the database.\n  -s S, --schema S             Path to the schema that defines the results that can be\n                               reported. If not provided 'PIPESTAT_RESULTS_SCHEMA' env var\n                               will be used. Currently not set\n  --status-schema ST           Path to the status schema. Default will be used if not\n                               provided: /home/drc/GITHUB/pipestat/pipestat/venv/lib/pytho\n                               n3.10/site-packages/pipestat/schemas/status_schema.yaml\n  --flag-dir FD                Path to the flag directory in case YAML file is the\n                               pipestat backend.\n  -r R, --record-identifier R  ID of the record to report the result for. If not provided\n                               'PIPESTAT_RECORD_IDENTIFIER' env var will be used.\n                               Currently not set\n  -p P, --pipeline-type P      project or sample level pipeline type.\n</code></pre>"},{"location":"pipestat/code/api-quickstart/","title":"Pipestat API Quickstart Guide","text":"<p>This example is for quickly reporting results to a results.yaml filebackend.</p> <pre><code>from pipestat import PipestatManager\n\n#File Backend requires a results.yaml file\nresult_file = \"../tests/data/results_docs_example.yaml\"\n\n#Every pipestat manager requires an output schema to know the format of results\nschema_file = \"../tests/data/sample_output_schema.yaml\"\n\n# With these two files, we can initialize a PipestatManager object and begin reporting results\n</code></pre> <pre><code>psm = PipestatManager(results_file_path=result_file, schema_path=schema_file)\n</code></pre> <pre><code>Initialize FileBackend\n</code></pre> <pre><code># Let's look at our output schema. Notice that the schema is only for reporting sample-level results\nprint(psm.schema)\n</code></pre> <pre><code>ParsedSchema (default_pipeline_name)\n Project-level properties:\n - None\n Sample-level properties:\n - number_of_things : {'type': 'integer', 'description': 'Number of things'}\n - percentage_of_things : {'type': 'number', 'description': 'Percentage of things'}\n - name_of_something : {'type': 'string', 'description': 'Name of something'}\n - switch_value : {'type': 'boolean', 'description': 'Is the switch on or off'}\n - output_file : {'description': 'This a path to the output file', 'type': 'object', 'object_type': 'file', 'properties': {'path': {'type': 'string'}, 'title': {'type': 'string'}}, 'required': ['path', 'title']}\n - output_image : {'description': 'This a path to the output image', 'type': 'object', 'object_type': 'image', 'properties': {'path': {'type': 'string'}, 'thumbnail_path': {'type': 'string'}, 'title': {'type': 'string'}}, 'required': ['path', 'thumbnail_path', 'title']}\n - md5sum : {'type': 'string', 'description': 'MD5SUM of an object', 'highlight': True}\n Status properties:\n - None\n</code></pre> <pre><code># Let's report a result. The result_identifier (e.g. percentage_of_things) must be in the output schema.\n# When reporting a result, a record_identifier must be provided either at the time of reporting \n# or upon PipestatManager creation.\n\npsm.report(record_identifier=\"my_sample_name_1\", values={\"percentage_of_things\": 100})\n</code></pre> <pre><code>[\"Reported records for 'my_sample_name_1' in 'default_pipeline_name' :\\n - percentage_of_things: 100\"]\n</code></pre> <pre><code># Pipestat reports the result as well as a created time and a modified time.\n# We can overwrite the modified time by reporting a new result. This is because force_overwrite defualts to True\npsm.report(record_identifier=\"my_sample_name_1\", values={\"percentage_of_things\": 50})\n</code></pre> <pre><code>These results exist for 'my_sample_name_1': percentage_of_things\nOverwriting existing results: percentage_of_things\n\n\n\n\n\n[\"Reported records for 'my_sample_name_1' in 'default_pipeline_name' :\\n - percentage_of_things: 50\"]\n</code></pre> <pre><code># If you set the flag to false and attempt to report results for a result that already exists...\npsm.report(record_identifier=\"my_sample_name_1\", values={\"percentage_of_things\": 50}, force_overwrite=False)\n</code></pre> <pre><code>These results exist for 'my_sample_name_1': percentage_of_things\n\n\n\n\n\nFalse\n</code></pre> <pre><code># Let's look at the reported data\n# Note that history recording is turned on by default and lives under meta -&gt; history keys\npsm.data\n</code></pre> <pre><code>default_pipeline_name:\n  project: {}\n  sample:\n    my_sample_name_1:\n      meta:\n        pipestat_modified_time: '2024-04-18 14:17:08'\n        pipestat_created_time: '2024-04-18 14:17:07'\n        history:\n          percentage_of_things:\n            '2024-04-18 14:17:08': 100\n      percentage_of_things: 50\n</code></pre> <pre><code># You can also retrieve a result:\nresult = psm.retrieve_one(record_identifier=\"my_sample_name_1\")\nprint(result)\n</code></pre> <pre><code>{'percentage_of_things': 50, 'record_identifier': 'my_sample_name_1'}\n</code></pre> <pre><code># Similarly you can retrieve historical results as well\nresult = psm.retrieve_history(record_identifier=\"my_sample_name_1\")\nprint(result)\n</code></pre> <pre><code>{'percentage_of_things': {'2024-04-18 14:17:08': 100}}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"pipestat/code/cli/","title":"Pipestat CLI","text":"<p>This tutorial demonstrates how to use the pipeline command-line interface (CLI). You should have already installed pipestat. Before following this tutorial please make sure you're familiar with more information-rich \"Pipestat Python API\" tutorial.  Also, for the following tutorial, you will need to point to a sample_output_schema.yaml in the schema path. An example file can be found here:</p> <p>https://github.com/pepkit/pipestat/blob/master/tests/data/sample_output_schema.yaml</p> <p>Report results from the command line by calling <code>pipestat</code> and passing in all relevant information:</p> <pre><code>rm ../tests/data/test_results_1.yaml\ntouch ../tests/data/test_results_1.yaml\npipestat report \\\n    --record-identifier sample_name \\\n    --result-identifier percentage_of_things \\\n    --value 12 \\\n    --results-file ../tests/data/test_results_1.yaml \\\n    --schema ../tests/data/sample_output_schema.yaml\n</code></pre> <pre><code>Reported records for 'sample_name' in 'pipeline_name' namespace:\n - percentage_of_things: 12\n</code></pre> <p>But this is obviously pretty cumbersome, since you have to pass lots of constant information to every call to report a result. So instead, you have an option to set up environment variables for a particular pipeline run:</p>"},{"location":"pipestat/code/cli/#prepare-environment","title":"Prepare environment","text":"<p>Pipestat environment variables avoid copious repetition of arguments in subsequent <code>pipestat</code> calls. Refer to the Environment variables reference for the complete list of supported environment variables. We will set a few for this tutorial:</p> <pre><code>export PIPESTAT_RESULTS_SCHEMA=../tests/data/sample_output_schema.yaml\nexport PIPESTAT_RECORD_IDENTIFIER=sample1\nexport PIPESTAT_RESULTS_FILE=`mktemp` # temporary file for results storage\n</code></pre> <p>Before we dive in, let's take a quick glance at the schema. This is the file that describes what sort of results are reported by this pipeline:</p> <pre><code>cat $PIPESTAT_RESULTS_SCHEMA\n</code></pre> <pre><code>number_of_things:\n  type: integer\n  description: \"Number of things\"\npercentage_of_things:\n  type: number\n  description: \"Percentage of things\"\nname_of_something:\n  type: string\n  description: \"Name of something\"\nswtich_value:\n  type: boolean\n  description: \"Is the switch on of off\"\ncollection_of_things:\n  type: array\n  description: \"This store collection of values\"\noutput_object:\n  type: object\n  description: \"Object output\"\noutput_file:\n  type: file\n  description: \"This a path to the output file\"\noutput_image:\n  type: image\n  description: \"This a path to the output image\"\nmd5sum:\n  type: string\n  description: \"MD5SUM of an object\"\n  highlight: true\n</code></pre>"},{"location":"pipestat/code/cli/#reporting","title":"Reporting","text":"<p>Naturally, the command line interface provides access to all the Python API functionalities of <code>pipestat</code>. So, for example, to report a result and back the object by a file use:</p> <pre><code>pipestat report -i number_of_things -v 100\n</code></pre> <pre><code>Reported records for 'sample1' in 'test' namespace:\n - number_of_things: 100\n</code></pre> <p>The result has been reported and the database file has been updated:</p> <pre><code>cat $PIPESTAT_RESULTS_FILE\n</code></pre> <pre><code>test:\n  sample1:\n    number_of_things: 100\n</code></pre> <p>Let's report another result:</p> <pre><code>pipestat report -i percentage_of_things -v 1.1\n</code></pre> <pre><code>Reported records for 'sample1' in 'test' namespace:\n - percentage_of_things: 1.1\n</code></pre> <pre><code>cat $PIPESTAT_RESULTS_FILE\n</code></pre> <pre><code>test:\n  sample1:\n    number_of_things: 100\n    percentage_of_things: 1.1\n</code></pre>"},{"location":"pipestat/code/cli/#inspection","title":"Inspection","text":"<p><code>pipestat inspect</code> command is a way to briefly look at the general <code>PipestatManager</code> state, like number of records, type of backend etc.</p> <pre><code>pipestat inspect\n</code></pre> <pre><code>PipestatManager (test)\nBackend: File \n - results: /var/folders/h8/8npwnh2s4rb8lr6hsy2ydrsh0000gp/T/tmp.hk8q23wT\n - status: /var/folders/h8/8npwnh2s4rb8lr6hsy2ydrsh0000gp/T)\nResults schema source: ../tests/data/sample_output_schema.yaml\nStatus schema source: /usr/local/lib/python3.9/site-packages/pipestat/schemas/status_schema.yaml\nRecords count: 1\nHighlighted results: md5sum\n</code></pre> <p>In order to display the contents of the results file or database table associated with the indicated namespace, add <code>--data</code> flag:</p> <pre><code>pipestat inspect --data\n</code></pre> <pre><code>PipestatManager (test)\nBackend: File \n - results: /var/folders/h8/8npwnh2s4rb8lr6hsy2ydrsh0000gp/T/tmp.hk8q23wT\n - status: /var/folders/h8/8npwnh2s4rb8lr6hsy2ydrsh0000gp/T)\nResults schema source: ../tests/data/sample_output_schema.yaml\nStatus schema source: /usr/local/lib/python3.9/site-packages/pipestat/schemas/status_schema.yaml\nRecords count: 1\nHighlighted results: md5sum\n\nData:\ntest:\n  sample1:\n    number_of_things: 100\n    percentage_of_things: 1.1\n</code></pre>"},{"location":"pipestat/code/cli/#retrieval","title":"Retrieval","text":"<p>Naturally, the reported results can be retrieved. Just call <code>pipestat retrieve</code> to do so:</p> <pre><code>pipestat retrieve -i percentage_of_things\n</code></pre> <pre><code>1.1\n</code></pre>"},{"location":"pipestat/code/cli/#history-retrieval","title":"History Retrieval","text":"<p>If you overwrite results, pipestat keeps a history by default. Use <code>pipestat history</code> to see previously reported results:</p> <p><code>{'percentage_of_things': {'2024-04-18 14:53:58': '1.1'}}</code></p>"},{"location":"pipestat/code/cli/#removal","title":"Removal","text":"<p>In order to remove a result call <code>pipestat remove</code>:</p> <pre><code>pipestat remove -i percentage_of_things\n</code></pre> <pre><code>Removed result 'percentage_of_things' for record 'sample1' from 'test' namespace\n</code></pre> <p>The results file and the state of the <code>PipestatManager</code> object reflect the removal:</p> <pre><code>cat $PIPESTAT_RESULTS_FILE\n</code></pre> <pre><code>test:\n  sample1:\n    number_of_things: 100\n</code></pre> <pre><code>pipestat inspect --data\n</code></pre> <pre><code>PipestatManager (test)\nBackend: File \n - results: /var/folders/h8/8npwnh2s4rb8lr6hsy2ydrsh0000gp/T/tmp.hk8q23wT\n - status: /var/folders/h8/8npwnh2s4rb8lr6hsy2ydrsh0000gp/T)\nResults schema source: ../tests/data/sample_output_schema.yaml\nStatus schema source: /usr/local/lib/python3.9/site-packages/pipestat/schemas/status_schema.yaml\nRecords count: 1\nHighlighted results: md5sum\n\nData:\ntest:\n  sample1:\n    number_of_things: 100\n</code></pre>"},{"location":"pipestat/code/cli/#status-management","title":"Status management","text":"<p>To manage pipeline status call <code>pipestat status &lt;subcommand&gt;</code>:</p> <ul> <li><code>set</code> to set pipeline statuses</li> <li><code>get</code> to retrieve pipeline statuses</li> </ul> <p>Starting with <code>pipestat 0.0.3</code> the <code>--schema</code> argument is not required for status management.</p> <pre><code>pipestat status set running\n</code></pre> <pre><code>pipestat status get\n</code></pre> <pre><code>running\n</code></pre> <p>Note that only statuses defined in the status schema are supported:</p> <pre><code>cat /usr/local/lib/python3.9/site-packages/pipestat/schemas/status_schema.yaml\n</code></pre> <pre><code>running:\n  description: \"the pipeline is running\"\n  color: [30, 144, 255] # dodgerblue\ncompleted:\n  description: \"the pipeline has completed\"\n  color: [50, 205, 50] # limegreen\nfailed:\n  description: \"the pipeline has failed\"\n  color: [220, 20, 60] # crimson\nwaiting:\n  description: \"the pipeline is waiting\"\n  color: [240, 230, 140] # khaki\npartial:\n  description: \"the pipeline stopped before completion point\"\n  color: [169, 169, 169] # darkgray\n</code></pre>"},{"location":"pipestat/code/cli/#html-report-generation","title":"HTML Report Generation","text":"<p>To generate a static html report, call <code>pipestat summarize --results-file PIPESTAT_RESULTS_FILE --schema PIPESTAT_RESULTS_SCHEMA</code></p> <pre><code>rm $PIPESTAT_RESULTS_FILE\n</code></pre>"},{"location":"pipestat/code/python-api/","title":"Python API","text":""},{"location":"pipestat/code/python-api/#package-pipestat-documentation","title":"Package <code>pipestat</code> Documentation","text":""},{"location":"pipestat/code/python-api/#class-pipestaterror","title":"Class <code>PipestatError</code>","text":"<p>Base exception type for this package</p>"},{"location":"pipestat/code/python-api/#class-samplepipestatmanager","title":"Class <code>SamplePipestatManager</code>","text":"<p>Pipestat standardizes reporting of pipeline results and pipeline status management. It formalizes a way for pipeline developers and downstream tools developers to communicate -- results produced by a pipeline can easily and reliably become an input for downstream analyses. A PipestatManager object exposes an API for interacting with the results and pipeline status and can be backed by either a YAML-formatted file or a database.</p> <pre><code>def __init__(self, **kwargs)\n</code></pre> <p>Initialize the PipestatManager object</p>"},{"location":"pipestat/code/python-api/#parameters","title":"Parameters:","text":"<ul> <li><code>record_identifier</code> (<code>str</code>):  record identifier to report for. Thiscreates a weak bound to the record, which can be overridden in this object method calls</li> <li><code>schema_path</code> (<code>str</code>):  path to the output schema that formalizesthe results structure</li> <li><code>results_file_path</code> (<code>str</code>):  YAML file to report into, if file isused as the object back-end</li> <li><code>database_only</code> (<code>bool</code>):  whether the reported data should not bestored in the memory, but only in the database</li> <li><code>config_file</code> (<code>str</code>):  path to the configuration file</li> <li><code>config_dict</code> (<code>dict</code>):   a mapping with the config file content</li> <li><code>flag_file_dir</code> (<code>str</code>):  path to directory containing flag files</li> <li><code>show_db_logs</code> (<code>bool</code>):  Defaults to False, toggles showing database logs</li> <li><code>pipeline_type</code> (<code>str</code>):  \"sample\" or \"project\"</li> <li><code>pipeline_name</code> (<code>str</code>):  name of the current pipeline, defaults to</li> <li><code>result_formatter</code> (<code>str</code>):  function for formatting result</li> <li><code>multi_pipelines</code> (<code>bool</code>):  allows for running multiple pipelines for one file backend</li> <li><code>output_dir</code> (<code>str</code>):  target directory for report generation via summarize and table generation via table.</li> </ul> <pre><code>def clear_status(self, *args, **kwargs)\n</code></pre> <pre><code>def config_path(self)\n</code></pre> <p>Config path. None if the config was not provided or if provided as a mapping of the config contents</p>"},{"location":"pipestat/code/python-api/#returns","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the provided config</li> </ul> <pre><code>def count_records(self, *args, **kwargs)\n</code></pre> <pre><code>def data(self)\n</code></pre> <p>Data object</p>"},{"location":"pipestat/code/python-api/#returns_1","title":"Returns:","text":"<ul> <li><code>yacman.YAMLConfigManager</code>:  the object that stores the reported data</li> </ul> <pre><code>def db_url(self)\n</code></pre> <p>Database URL, generated based on config credentials</p>"},{"location":"pipestat/code/python-api/#returns_2","title":"Returns:","text":"<ul> <li><code>str</code>:  database URL</li> </ul>"},{"location":"pipestat/code/python-api/#raises","title":"Raises:","text":"<ul> <li><code>PipestatDatabaseError</code>:  if the object is not backed by a database</li> </ul> <pre><code>def file(self)\n</code></pre> <p>File path that the object is reporting the results into</p>"},{"location":"pipestat/code/python-api/#returns_3","title":"Returns:","text":"<ul> <li><code>str</code>:  file path that the object is reporting the results into</li> </ul> <pre><code>def get_status(self, *args, **kwargs)\n</code></pre> <pre><code>def highlighted_results(self)\n</code></pre> <p>Highlighted results</p>"},{"location":"pipestat/code/python-api/#returns_4","title":"Returns:","text":"<ul> <li><code>List[str]</code>:  a collection of highlighted results</li> </ul> <pre><code>def initialize_dbbackend(*args, **kwargs)\n</code></pre> <pre><code>def link(self, *args, **kwargs)\n</code></pre> <pre><code>def list_recent_results(self, *args, **kwargs)\n</code></pre> <pre><code>def output_dir(self)\n</code></pre> <p>Output directory for report and stats generation</p>"},{"location":"pipestat/code/python-api/#returns_5","title":"Returns:","text":"<ul> <li><code>str</code>:  path to output_dir</li> </ul> <pre><code>def pipeline_name(self)\n</code></pre> <p>Pipeline name</p>"},{"location":"pipestat/code/python-api/#returns_6","title":"Returns:","text":"<ul> <li><code>str</code>:  Pipeline name</li> </ul> <pre><code>def pipeline_type(self)\n</code></pre> <p>Pipeline type: \"sample\" or \"project\"</p>"},{"location":"pipestat/code/python-api/#returns_7","title":"Returns:","text":"<ul> <li><code>str</code>:  pipeline type</li> </ul> <pre><code>def project_name(self)\n</code></pre> <p>Project name the object writes the results to</p>"},{"location":"pipestat/code/python-api/#returns_8","title":"Returns:","text":"<ul> <li><code>str</code>:  project name the object writes the results to</li> </ul> <pre><code>def record_count(self)\n</code></pre> <p>Number of records reported</p>"},{"location":"pipestat/code/python-api/#returns_9","title":"Returns:","text":"<ul> <li><code>int</code>:  number of records reported</li> </ul> <pre><code>def record_identifier(self)\n</code></pre> <p>Pipeline type: \"sample\" or \"project\"</p>"},{"location":"pipestat/code/python-api/#returns_10","title":"Returns:","text":"<ul> <li><code>str</code>:  pipeline type</li> </ul> <pre><code>def remove(self, *args, **kwargs)\n</code></pre> <pre><code>def remove_record(self, *args, **kwargs)\n</code></pre> <pre><code>def report(self, *args, **kwargs)\n</code></pre> <pre><code>def result_schemas(self)\n</code></pre> <p>Result schema mappings</p>"},{"location":"pipestat/code/python-api/#returns_11","title":"Returns:","text":"<ul> <li><code>dict</code>:  schemas that formalize the structure of each resultin a canonical jsonschema way</li> </ul> <pre><code>def retrieve_one(self, *args, **kwargs)\n</code></pre> <pre><code>def schema(self)\n</code></pre> <p>Schema mapping</p>"},{"location":"pipestat/code/python-api/#returns_12","title":"Returns:","text":"<ul> <li><code>ParsedSchema</code>:  schema object that formalizes the results structure</li> </ul> <pre><code>def schema_path(self)\n</code></pre> <p>Schema path</p>"},{"location":"pipestat/code/python-api/#returns_13","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the provided schema</li> </ul> <pre><code>def select_distinct(self, *args, **kwargs)\n</code></pre> <pre><code>def select_records(self, *args, **kwargs)\n</code></pre> <pre><code>def set_status(self, *args, **kwargs)\n</code></pre> <pre><code>def status_schema(self)\n</code></pre> <p>Status schema mapping</p>"},{"location":"pipestat/code/python-api/#returns_14","title":"Returns:","text":"<ul> <li><code>dict</code>:  schema that formalizes the pipeline status structure</li> </ul> <pre><code>def status_schema_source(self)\n</code></pre> <p>Status schema source</p>"},{"location":"pipestat/code/python-api/#returns_15","title":"Returns:","text":"<ul> <li><code>dict</code>:  source of the schema that formalizesthe pipeline status structure</li> </ul> <pre><code>def summarize(self, *args, **kwargs)\n</code></pre> <pre><code>def table(self, *args, **kwargs)\n</code></pre>"},{"location":"pipestat/code/python-api/#class-projectpipestatmanager","title":"Class <code>ProjectPipestatManager</code>","text":"<p>Pipestat standardizes reporting of pipeline results and pipeline status management. It formalizes a way for pipeline developers and downstream tools developers to communicate -- results produced by a pipeline can easily and reliably become an input for downstream analyses. A PipestatManager object exposes an API for interacting with the results and pipeline status and can be backed by either a YAML-formatted file or a database.</p> <pre><code>def __init__(self, **kwargs)\n</code></pre> <p>Initialize the PipestatManager object</p>"},{"location":"pipestat/code/python-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>record_identifier</code> (<code>str</code>):  record identifier to report for. Thiscreates a weak bound to the record, which can be overridden in this object method calls</li> <li><code>schema_path</code> (<code>str</code>):  path to the output schema that formalizesthe results structure</li> <li><code>results_file_path</code> (<code>str</code>):  YAML file to report into, if file isused as the object back-end</li> <li><code>database_only</code> (<code>bool</code>):  whether the reported data should not bestored in the memory, but only in the database</li> <li><code>config_file</code> (<code>str</code>):  path to the configuration file</li> <li><code>config_dict</code> (<code>dict</code>):   a mapping with the config file content</li> <li><code>flag_file_dir</code> (<code>str</code>):  path to directory containing flag files</li> <li><code>show_db_logs</code> (<code>bool</code>):  Defaults to False, toggles showing database logs</li> <li><code>pipeline_type</code> (<code>str</code>):  \"sample\" or \"project\"</li> <li><code>pipeline_name</code> (<code>str</code>):  name of the current pipeline, defaults to</li> <li><code>result_formatter</code> (<code>str</code>):  function for formatting result</li> <li><code>multi_pipelines</code> (<code>bool</code>):  allows for running multiple pipelines for one file backend</li> <li><code>output_dir</code> (<code>str</code>):  target directory for report generation via summarize and table generation via table.</li> </ul> <pre><code>def clear_status(self, *args, **kwargs)\n</code></pre> <pre><code>def config_path(self)\n</code></pre> <p>Config path. None if the config was not provided or if provided as a mapping of the config contents</p>"},{"location":"pipestat/code/python-api/#returns_16","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the provided config</li> </ul> <pre><code>def count_records(self, *args, **kwargs)\n</code></pre> <pre><code>def data(self)\n</code></pre> <p>Data object</p>"},{"location":"pipestat/code/python-api/#returns_17","title":"Returns:","text":"<ul> <li><code>yacman.YAMLConfigManager</code>:  the object that stores the reported data</li> </ul> <pre><code>def db_url(self)\n</code></pre> <p>Database URL, generated based on config credentials</p>"},{"location":"pipestat/code/python-api/#returns_18","title":"Returns:","text":"<ul> <li><code>str</code>:  database URL</li> </ul>"},{"location":"pipestat/code/python-api/#raises_1","title":"Raises:","text":"<ul> <li><code>PipestatDatabaseError</code>:  if the object is not backed by a database</li> </ul> <pre><code>def file(self)\n</code></pre> <p>File path that the object is reporting the results into</p>"},{"location":"pipestat/code/python-api/#returns_19","title":"Returns:","text":"<ul> <li><code>str</code>:  file path that the object is reporting the results into</li> </ul> <pre><code>def get_status(self, *args, **kwargs)\n</code></pre> <pre><code>def highlighted_results(self)\n</code></pre> <p>Highlighted results</p>"},{"location":"pipestat/code/python-api/#returns_20","title":"Returns:","text":"<ul> <li><code>List[str]</code>:  a collection of highlighted results</li> </ul> <pre><code>def initialize_dbbackend(*args, **kwargs)\n</code></pre> <pre><code>def link(self, *args, **kwargs)\n</code></pre> <pre><code>def list_recent_results(self, *args, **kwargs)\n</code></pre> <pre><code>def output_dir(self)\n</code></pre> <p>Output directory for report and stats generation</p>"},{"location":"pipestat/code/python-api/#returns_21","title":"Returns:","text":"<ul> <li><code>str</code>:  path to output_dir</li> </ul> <pre><code>def pipeline_name(self)\n</code></pre> <p>Pipeline name</p>"},{"location":"pipestat/code/python-api/#returns_22","title":"Returns:","text":"<ul> <li><code>str</code>:  Pipeline name</li> </ul> <pre><code>def pipeline_type(self)\n</code></pre> <p>Pipeline type: \"sample\" or \"project\"</p>"},{"location":"pipestat/code/python-api/#returns_23","title":"Returns:","text":"<ul> <li><code>str</code>:  pipeline type</li> </ul> <pre><code>def project_name(self)\n</code></pre> <p>Project name the object writes the results to</p>"},{"location":"pipestat/code/python-api/#returns_24","title":"Returns:","text":"<ul> <li><code>str</code>:  project name the object writes the results to</li> </ul> <pre><code>def record_count(self)\n</code></pre> <p>Number of records reported</p>"},{"location":"pipestat/code/python-api/#returns_25","title":"Returns:","text":"<ul> <li><code>int</code>:  number of records reported</li> </ul> <pre><code>def record_identifier(self)\n</code></pre> <p>Pipeline type: \"sample\" or \"project\"</p>"},{"location":"pipestat/code/python-api/#returns_26","title":"Returns:","text":"<ul> <li><code>str</code>:  pipeline type</li> </ul> <pre><code>def remove(self, *args, **kwargs)\n</code></pre> <pre><code>def remove_record(self, *args, **kwargs)\n</code></pre> <pre><code>def report(self, *args, **kwargs)\n</code></pre> <pre><code>def result_schemas(self)\n</code></pre> <p>Result schema mappings</p>"},{"location":"pipestat/code/python-api/#returns_27","title":"Returns:","text":"<ul> <li><code>dict</code>:  schemas that formalize the structure of each resultin a canonical jsonschema way</li> </ul> <pre><code>def retrieve_one(self, *args, **kwargs)\n</code></pre> <pre><code>def schema(self)\n</code></pre> <p>Schema mapping</p>"},{"location":"pipestat/code/python-api/#returns_28","title":"Returns:","text":"<ul> <li><code>ParsedSchema</code>:  schema object that formalizes the results structure</li> </ul> <pre><code>def schema_path(self)\n</code></pre> <p>Schema path</p>"},{"location":"pipestat/code/python-api/#returns_29","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the provided schema</li> </ul> <pre><code>def select_distinct(self, *args, **kwargs)\n</code></pre> <pre><code>def select_records(self, *args, **kwargs)\n</code></pre> <pre><code>def set_status(self, *args, **kwargs)\n</code></pre> <pre><code>def status_schema(self)\n</code></pre> <p>Status schema mapping</p>"},{"location":"pipestat/code/python-api/#returns_30","title":"Returns:","text":"<ul> <li><code>dict</code>:  schema that formalizes the pipeline status structure</li> </ul> <pre><code>def status_schema_source(self)\n</code></pre> <p>Status schema source</p>"},{"location":"pipestat/code/python-api/#returns_31","title":"Returns:","text":"<ul> <li><code>dict</code>:  source of the schema that formalizesthe pipeline status structure</li> </ul> <pre><code>def summarize(self, *args, **kwargs)\n</code></pre> <pre><code>def table(self, *args, **kwargs)\n</code></pre>"},{"location":"pipestat/code/python-api/#class-pipestatboss","title":"Class <code>PipestatBoss</code>","text":"<p>PipestatBoss simply holds Sample or Project Managers that are child classes of PipestatManager. :param list[str] pipeline_list: list that holds pipeline types, e.g. ['sample','project'] :param str record_identifier: record identifier to report for. This creates a weak bound to the record, which can be overridden in this object method calls :param str schema_path: path to the output schema that formalizes the results structure :param str results_file_path: YAML file to report into, if file is used as the object back-end :param bool database_only: whether the reported data should not be stored in the memory, but only in the database :param str | dict config: path to the configuration file or a mapping with the config file content :param str flag_file_dir: path to directory containing flag files :param bool show_db_logs: Defaults to False, toggles showing database logs :param str pipeline_type: \"sample\" or \"project\" :param str result_formatter: function for formatting result :param bool multi_pipelines: allows for running multiple pipelines for one file backend :param str output_dir: target directory for report generation via summarize and table generation via table.</p> <pre><code>def __init__(self, pipeline_list: Optional[list]=None, **kwargs)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"pipestat/code/python-api/#class-pipestatmanager","title":"Class <code>PipestatManager</code>","text":"<p>Pipestat standardizes reporting of pipeline results and pipeline status management. It formalizes a way for pipeline developers and downstream tools developers to communicate -- results produced by a pipeline can easily and reliably become an input for downstream analyses. A PipestatManager object exposes an API for interacting with the results and pipeline status and can be backed by either a YAML-formatted file or a database.</p> <pre><code>def __init__(self, project_name: Optional[str]=None, record_identifier: Optional[str]=None, schema_path: Optional[str]=None, results_file_path: Optional[str]=None, database_only: Optional[bool]=True, config_file: Optional[str]=None, config_dict: Optional[dict]=None, flag_file_dir: Optional[str]=None, show_db_logs: bool=False, pipeline_type: Optional[str]=None, pipeline_name: Optional[str]=None, result_formatter: staticmethod=&lt;function default_formatter at 0x70949b3ae680&gt;, multi_pipelines: bool=False, output_dir: Optional[str]=None)\n</code></pre> <p>Initialize the PipestatManager object</p>"},{"location":"pipestat/code/python-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>record_identifier</code> (<code>str</code>):  record identifier to report for. Thiscreates a weak bound to the record, which can be overridden in this object method calls</li> <li><code>schema_path</code> (<code>str</code>):  path to the output schema that formalizesthe results structure</li> <li><code>results_file_path</code> (<code>str</code>):  YAML file to report into, if file isused as the object back-end</li> <li><code>database_only</code> (<code>bool</code>):  whether the reported data should not bestored in the memory, but only in the database</li> <li><code>config_file</code> (<code>str</code>):  path to the configuration file</li> <li><code>config_dict</code> (<code>dict</code>):   a mapping with the config file content</li> <li><code>flag_file_dir</code> (<code>str</code>):  path to directory containing flag files</li> <li><code>show_db_logs</code> (<code>bool</code>):  Defaults to False, toggles showing database logs</li> <li><code>pipeline_type</code> (<code>str</code>):  \"sample\" or \"project\"</li> <li><code>pipeline_name</code> (<code>str</code>):  name of the current pipeline, defaults to</li> <li><code>result_formatter</code> (<code>str</code>):  function for formatting result</li> <li><code>multi_pipelines</code> (<code>bool</code>):  allows for running multiple pipelines for one file backend</li> <li><code>output_dir</code> (<code>str</code>):  target directory for report generation via summarize and table generation via table.</li> </ul> <pre><code>def check_multi_results(self)\n</code></pre> <pre><code>def clear_status(self, *args, **kwargs)\n</code></pre> <pre><code>def config_path(self)\n</code></pre> <p>Config path. None if the config was not provided or if provided as a mapping of the config contents</p>"},{"location":"pipestat/code/python-api/#returns_32","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the provided config</li> </ul> <pre><code>def count_records(self, *args, **kwargs)\n</code></pre> <pre><code>def data(self)\n</code></pre> <p>Data object</p>"},{"location":"pipestat/code/python-api/#returns_33","title":"Returns:","text":"<ul> <li><code>yacman.YAMLConfigManager</code>:  the object that stores the reported data</li> </ul> <pre><code>def db_url(self)\n</code></pre> <p>Database URL, generated based on config credentials</p>"},{"location":"pipestat/code/python-api/#returns_34","title":"Returns:","text":"<ul> <li><code>str</code>:  database URL</li> </ul>"},{"location":"pipestat/code/python-api/#raises_2","title":"Raises:","text":"<ul> <li><code>PipestatDatabaseError</code>:  if the object is not backed by a database</li> </ul> <pre><code>def file(self)\n</code></pre> <p>File path that the object is reporting the results into</p>"},{"location":"pipestat/code/python-api/#returns_35","title":"Returns:","text":"<ul> <li><code>str</code>:  file path that the object is reporting the results into</li> </ul> <pre><code>def get_status(self, *args, **kwargs)\n</code></pre> <pre><code>def highlighted_results(self)\n</code></pre> <p>Highlighted results</p>"},{"location":"pipestat/code/python-api/#returns_36","title":"Returns:","text":"<ul> <li><code>List[str]</code>:  a collection of highlighted results</li> </ul> <pre><code>def initialize_dbbackend(*args, **kwargs)\n</code></pre> <pre><code>def initialize_filebackend(self, record_identifier, results_file_path, flag_file_dir)\n</code></pre> <pre><code>def link(self, *args, **kwargs)\n</code></pre> <pre><code>def list_recent_results(self, *args, **kwargs)\n</code></pre> <pre><code>def output_dir(self)\n</code></pre> <p>Output directory for report and stats generation</p>"},{"location":"pipestat/code/python-api/#returns_37","title":"Returns:","text":"<ul> <li><code>str</code>:  path to output_dir</li> </ul> <pre><code>def pipeline_name(self)\n</code></pre> <p>Pipeline name</p>"},{"location":"pipestat/code/python-api/#returns_38","title":"Returns:","text":"<ul> <li><code>str</code>:  Pipeline name</li> </ul> <pre><code>def pipeline_type(self)\n</code></pre> <p>Pipeline type: \"sample\" or \"project\"</p>"},{"location":"pipestat/code/python-api/#returns_39","title":"Returns:","text":"<ul> <li><code>str</code>:  pipeline type</li> </ul> <pre><code>def process_schema(self, schema_path)\n</code></pre> <pre><code>def project_name(self)\n</code></pre> <p>Project name the object writes the results to</p>"},{"location":"pipestat/code/python-api/#returns_40","title":"Returns:","text":"<ul> <li><code>str</code>:  project name the object writes the results to</li> </ul> <pre><code>def record_count(self)\n</code></pre> <p>Number of records reported</p>"},{"location":"pipestat/code/python-api/#returns_41","title":"Returns:","text":"<ul> <li><code>int</code>:  number of records reported</li> </ul> <pre><code>def record_identifier(self)\n</code></pre> <p>Pipeline type: \"sample\" or \"project\"</p>"},{"location":"pipestat/code/python-api/#returns_42","title":"Returns:","text":"<ul> <li><code>str</code>:  pipeline type</li> </ul> <pre><code>def remove(self, *args, **kwargs)\n</code></pre> <pre><code>def remove_record(self, *args, **kwargs)\n</code></pre> <pre><code>def report(self, *args, **kwargs)\n</code></pre> <pre><code>def resolve_results_file_path(self, results_file_path)\n</code></pre> <p>Replace {record_identifier} in results_file_path if it exists.</p>"},{"location":"pipestat/code/python-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>results_file_path</code> (<code>str</code>):  YAML file to report into, if file isused as the object back-end</li> </ul> <pre><code>def result_schemas(self)\n</code></pre> <p>Result schema mappings</p>"},{"location":"pipestat/code/python-api/#returns_43","title":"Returns:","text":"<ul> <li><code>dict</code>:  schemas that formalize the structure of each resultin a canonical jsonschema way</li> </ul> <pre><code>def retrieve_history(self, record_identifier: str=None, result_identifier: Union[str, List[str], NoneType]=None) -&gt; Union[Any, Dict[str, Any]]\n</code></pre> <p>Retrieve a single record's history</p>"},{"location":"pipestat/code/python-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>record_identifier</code> (<code>str</code>):  single record_identifier</li> <li><code>result_identifier</code> (<code>str</code>):  single result_identifier or list of result identifiers</li> </ul>"},{"location":"pipestat/code/python-api/#returns_44","title":"Returns:","text":"<ul> <li>``:  a mapping with filtered historical results</li> </ul> <pre><code>def retrieve_many(self, record_identifiers: List[str], result_identifier: Optional[str]=None) -&gt; Union[Any, Dict[str, Any]]\n</code></pre>"},{"location":"pipestat/code/python-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>record_identifiers</code> (``):  list of record identifiers</li> <li><code>result_identifier</code> (<code>str</code>):  single record_identifier</li> </ul>"},{"location":"pipestat/code/python-api/#returns_45","title":"Returns:","text":"<ul> <li>``:  a mapping with filteredresults reported for the record</li> </ul> <pre><code>def retrieve_one(self, *args, **kwargs)\n</code></pre> <pre><code>def schema(self)\n</code></pre> <p>Schema mapping</p>"},{"location":"pipestat/code/python-api/#returns_46","title":"Returns:","text":"<ul> <li><code>ParsedSchema</code>:  schema object that formalizes the results structure</li> </ul> <pre><code>def schema_path(self)\n</code></pre> <p>Schema path</p>"},{"location":"pipestat/code/python-api/#returns_47","title":"Returns:","text":"<ul> <li><code>str</code>:  path to the provided schema</li> </ul> <pre><code>def select_distinct(self, *args, **kwargs)\n</code></pre> <pre><code>def select_records(self, *args, **kwargs)\n</code></pre> <pre><code>def set_status(self, *args, **kwargs)\n</code></pre> <pre><code>def status_schema(self)\n</code></pre> <p>Status schema mapping</p>"},{"location":"pipestat/code/python-api/#returns_48","title":"Returns:","text":"<ul> <li><code>dict</code>:  schema that formalizes the pipeline status structure</li> </ul> <pre><code>def status_schema_source(self)\n</code></pre> <p>Status schema source</p>"},{"location":"pipestat/code/python-api/#returns_49","title":"Returns:","text":"<ul> <li><code>dict</code>:  source of the schema that formalizesthe pipeline status structure</li> </ul> <pre><code>def summarize(self, *args, **kwargs)\n</code></pre> <pre><code>def table(self, *args, **kwargs)\n</code></pre> <p>Version Information: <code>pipestat</code> v0.9.2, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"pipestat/code/python-tutorial/","title":"Pipestat Python API","text":"<p>Pipestat is a Python package for a standardized reporting of pipeline statistics. It formalizes a way to communicate between pipelines and downstream tools that analyze their results so that pipeline results can easily become input for downstream analyses.</p> <p>This tutorial is targeted toward pipeline developers, and shows how to use pipestat to manage pipeline results. This tutorial assumes you're writing your pipeline in Python; if not, there's another tutorial that accomplishes the same thing for any pipeline using the command-line interface.</p>"},{"location":"pipestat/code/python-tutorial/#introduction","title":"Introduction","text":"<p>To make your Python pipeline pipestat-compatible, you first need to initialize pipestat with some important configuration setup:</p> <ol> <li>pipestat schema: a path to a JSON-schema file that defines results reported by this pipeline</li> <li>pipeline_name: defines a unique group name for reported results</li> <li>record_identifier: a unique name for a particular run of the pipeline, typically a sample name</li> <li>backend: where the results should be stored. Either path to a YAML-formatted file or pipestat config with PostgreSQL database login credentials</li> </ol>"},{"location":"pipestat/code/python-tutorial/#back-end-types","title":"Back-end types","text":"<p>Three types of back-ends are currently supported:</p> <ol> <li> <p>a file (pass a file path to the constructor) The changes reported using the <code>report</code> method of <code>PipestatManger</code> will be securely written to the file. Currently only YAML format is supported. </p> </li> <li> <p>a PostgreSQL database (pass a path to the pipestat config to the constructor) This option gives the user the possibility to use a fully fledged database to back <code>PipestatManager</code>. </p> </li> <li> <p>a PEP on PEPhub (pass a pep path to the constructor, e.g. <code>psm = PipestatManager(pephub_path=pephubpath)</code>) This option gives the user the possibility to use PEPhub as a backend for results. </p> </li> </ol>"},{"location":"pipestat/code/python-tutorial/#initializing-a-pipestat-session","title":"Initializing a pipestat session","text":"<p>Start by importing the <code>pipestat</code> package in Python.</p> <pre><code>import pipestat\nfrom jsonschema import ValidationError\n</code></pre> <p>After importing the package, we need to create an <code>PipestatManager</code> object. The object constructor requires a few pieces of information. We'll use a file as the back-end, by passing a file path string to the constructor. Let's create a temporary file first:</p> <pre><code>from tempfile import mkstemp\n\n_, temp_file = mkstemp(suffix=\".yaml\")\nprint(temp_file)\n</code></pre> <pre><code>/tmp/tmpu4r0mojr.yaml\n</code></pre> <p>Now we can create a <code>PipestatManager</code> object that uses this file as the back-end:</p> <pre><code>psm = pipestat.PipestatManager(\n    record_identifier=\"sample1\",\n    results_file_path=temp_file,\n    schema_path=\"../tests/data/sample_output_schema.yaml\",\n)\n</code></pre> <pre><code>Initialize FileBackend\n</code></pre> <p>Note: For schema_path, you will need to point to a sample_output_schema.yaml. An example file can be found here: https://github.com/pepkit/pipestat/blob/master/tests/data/sample_output_schema.yaml</p> <p>You can also put these settings into a config file and just pass that to the <code>config</code> argument, instead of specifying each argument separately. The results will be reported to a \"test\" namespace.</p> <pre><code>psm.pipeline_name\n</code></pre> <pre><code>'default_pipeline_name'\n</code></pre> <p>By default, <code>PipestatManager</code> instance is bound to the record it was initialized with. However, reporting or removing results for a different record can be enforced in the respective methods with <code>sameple_name</code> argument.</p> <pre><code>psm.record_identifier\n</code></pre> <pre><code>'sample1'\n</code></pre> <p>Since we've used a newly created file, nothing has been reported yet:</p> <pre><code>print(psm.retrieve_one(record_identifier='sample1'))\n</code></pre> <p>Using <code>psm.retrieve_one</code> at this stage will return a <code>RecordNotFound</code> exception.</p> <pre><code>psm.data\n</code></pre> <pre><code>default_pipeline_name:\n  project: {}\n  sample: {}\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#reporting-results","title":"Reporting results","text":"<p>To report a result, use a <code>report</code> method. It requires two pieces of information:</p> <ol> <li>record_identifier -- record to report the result for, for example a unique name of the sample (optional if provided at <code>PipestatManager</code> initialization stage)</li> <li>values -- a Python <code>dict</code> of resultID-value pairs to report. The top level keys must correspond to the results identifiers defined in the schema</li> </ol>"},{"location":"pipestat/code/python-tutorial/#available-results-defined-in-schemas","title":"Available results defined in schemas","text":"<p>To learn about the results that the current <code>PipestatManager</code> instance supports check out the <code>schema.result_schemas</code> property:</p> <pre><code>psm.result_schemas\n</code></pre> <pre><code>{'number_of_things': {'type': 'integer', 'description': 'Number of things'},\n 'percentage_of_things': {'type': 'number',\n  'description': 'Percentage of things'},\n 'name_of_something': {'type': 'string', 'description': 'Name of something'},\n 'switch_value': {'type': 'boolean', 'description': 'Is the switch on or off'},\n 'output_file': {'description': 'This a path to the output file',\n  'type': 'object',\n  'object_type': 'file',\n  'properties': {'path': {'type': 'string'}, 'title': {'type': 'string'}},\n  'required': ['path', 'title']},\n 'output_image': {'description': 'This a path to the output image',\n  'type': 'object',\n  'object_type': 'image',\n  'properties': {'path': {'type': 'string'},\n   'thumbnail_path': {'type': 'string'},\n   'title': {'type': 'string'}},\n  'required': ['path', 'thumbnail_path', 'title']},\n 'md5sum': {'type': 'string',\n  'description': 'MD5SUM of an object',\n  'highlight': True}}\n</code></pre> <p>To learn about the actual required attributes of the reported results, like <code>file</code> or <code>image</code> (see: <code>output_file</code> and <code>output_image</code> results) select the <code>output_file</code> from the <code>result_schemas</code> property:</p> <pre><code>psm.result_schemas[\"output_file\"]\n</code></pre> <pre><code>{'description': 'This a path to the output file',\n 'type': 'object',\n 'object_type': 'file',\n 'properties': {'path': {'type': 'string'}, 'title': {'type': 'string'}},\n 'required': ['path', 'title']}\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#results-composition-enforcement","title":"Results composition enforcement","text":"<p>As you can see, to report a <code>output_file</code> result, you need to provide an object with <code>path</code> and <code>title</code> string attributes. If you fail to do so <code>PipestatManager</code> will issue an informative validation error:</p> <pre><code>try:\n    psm.report(record_identifier=\"sample1\", values={\"output_file\": {\"path\": \"/home/user/path.csv\"}})\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p><code>SchemaValidationErrorDuringReport: 'title' is a required property</code></p> <p>Let's report a correct object this time:</p> <pre><code>psm.report(record_identifier=\"sample1\",\n    values={\n        \"output_file\": {\n            \"path\": \"/home/user/path.csv\",\n            \"title\": \"CSV file with some data\",\n        }\n    }\n)\n</code></pre> <pre><code>[\"Reported records for 'sample1' in 'default_pipeline_name' :\\n - output_file: {'path': '/home/user/path.csv', 'title': 'CSV file with some data'}\"]\n</code></pre> <p>Inspect the object's database to verify whether the result has been successfully reported:</p> <pre><code>psm.data\n</code></pre> <pre><code>default_pipeline_name:\n  project: {}\n  sample:\n    sample1:\n      meta:\n        pipestat_modified_time: '2024-04-18 15:04:33'\n        pipestat_created_time: '2024-04-18 15:04:33'\n      output_file:\n        path: /home/user/path.csv\n        title: CSV file with some data\n</code></pre> <p>Or use the retrieve function (required for database backends):</p> <pre><code>psm.retrieve_one('sample1')\n</code></pre> <pre><code>{'output_file': {'path': '/home/user/path.csv',\n  'title': 'CSV file with some data'},\n 'record_identifier': 'sample1'}\n</code></pre> <p>Results are overwritten unless force_overwrite is set to False!</p> <pre><code>psm.report(record_identifier=\"sample1\",\n    values={\n        \"output_file\": {\n            \"path\": \"/home/user/path_new.csv\",\n            \"title\": \"new CSV file with some data\",\n        }\n    }\n)\n</code></pre> <pre><code>These results exist for 'sample1': output_file\nOverwriting existing results: output_file\n\n\n\n\n\n[\"Reported records for 'sample1' in 'default_pipeline_name' :\\n - output_file: {'path': '/home/user/path_new.csv', 'title': 'new CSV file with some data'}\"]\n</code></pre> <pre><code>psm.report(record_identifier=\"sample1\",\n    values={\n        \"output_file\": {\n            \"path\": \"/home/user/path_new.csv\",\n            \"title\": \"new CSV file with some data\",\n        }\n    },\n    force_overwrite=False,\n)\n\npsm.retrieve_one('sample1')\n</code></pre> <pre><code>These results exist for 'sample1': output_file\n\n\n\n\n\n{'output_file': {'path': '/home/user/path_new.csv',\n  'title': 'new CSV file with some data'},\n 'record_identifier': 'sample1'}\n</code></pre> <p>Most importantly, by backing the object by a file, the reported results persist -- another <code>PipestatManager</code> object reads the results when created:</p> <pre><code>psm1 = pipestat.PipestatManager(\n    pipeline_name=\"test\",\n    record_identifier=\"sample1\",\n    results_file_path=temp_file,\n    schema_path=\"../tests/data/sample_output_schema.yaml\",\n)\n</code></pre> <pre><code>Initialize FileBackend\n</code></pre> <pre><code>psm.retrieve_one('sample1')\n</code></pre> <pre><code>{'output_file': {'path': '/home/user/path_new.csv',\n  'title': 'new CSV file with some data'},\n 'record_identifier': 'sample1'}\n</code></pre> <p>That's because the contents are stored in the file we've specified at object creation stage:</p> <pre><code>!echo $temp_file\n!cat $temp_file\n</code></pre> <pre><code>/tmp/tmps01teih1.yaml\ndefault_pipeline_name:\n  project: {}\n  sample:\n    sample1:\n      output_file:\n        path: /home/user/path_new.csv\n        title: new CSV file with some data\n      pipestat_created_time: '2023-11-07 17:30:39'\n      pipestat_modified_time: '2023-11-07 17:30:48'\n</code></pre> <p>Note that two processes can securely report to a single file and single namespace since <code>pipestat</code> supports locks and race-free writes to control multi-user conflicts and prevent data loss.</p>"},{"location":"pipestat/code/python-tutorial/#results-type-enforcement","title":"Results type enforcement","text":"<p>By default <code>PipestatManager</code> raises an exception if a non-compatible result value is reported. </p> <p>This behavior can be changed by setting <code>strict_type</code> to <code>True</code> in <code>PipestatManager.report</code> method. In this case <code>PipestatManager</code> tries to cast the reported results values to the Python classes required by schema. For example, if a result defined as <code>integer</code> is reported and a <code>str</code> value is passed, the eventual value will be <code>int</code>:</p> <pre><code>psm.result_schemas[\"number_of_things\"]\n</code></pre> <pre><code>{'type': 'integer', 'description': 'Number of things'}\n</code></pre> <pre><code>psm.report(record_identifier=\"sample1\",values={\"number_of_things\": \"10\"}, strict_type=False)\n</code></pre> <pre><code>[\"Reported records for 'sample1' in 'default_pipeline_name' :\\n - number_of_things: 10\"]\n</code></pre> <p>The method will attempt to cast the value to a proper Python class and store the converted object. In case of a failure, an error will be raised:</p> <pre><code>try:\n    psm.report(\n        record_identifier=\"sample2\", values={\"number_of_things\": []}, strict_type=False\n    )\nexcept TypeError as e:\n    print(e)\n</code></pre> <pre><code>int() argument must be a string, a bytes-like object or a real number, not 'list'\n</code></pre> <p>Note that in this case we tried to report a result for a different record (<code>sample2</code>), which had to be enforced with <code>record_identifier</code> argument.</p> <pre><code>psm.data\n</code></pre> <pre><code>default_pipeline_name:\n  project: {}\n  sample:\n    sample1:\n      meta:\n        pipestat_modified_time: '2024-04-18 15:06:45'\n        pipestat_created_time: '2024-04-18 15:04:33'\n        history:\n          output_file:\n            '2024-04-18 15:06:04':\n              path: /home/user/path.csv\n              title: CSV file with some data\n      output_file:\n        path: /home/user/path_new.csv\n        title: new CSV file with some data\n      number_of_things: '10'\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#retrieving-results","title":"Retrieving results","text":"<p>Naturally, the reported results can be retrieved. Let's explore all the options the <code>PipestatManager.retrieve</code> method provides:</p> <p>To retrieve a specific result for a record, provide the identifiers:</p> <pre><code>psm.retrieve_one(record_identifier=\"sample1\", result_identifier=\"number_of_things\")\n</code></pre> <pre><code>'10'\n</code></pre> <p>To retrieve all the results for a record, skip the <code>result_identifier</code> argument:</p> <pre><code>psm.retrieve_one(record_identifier=\"sample1\")\n</code></pre> <pre><code>{'output_file': {'path': '/home/user/path_new.csv',\n  'title': 'new CSV file with some data'},\n 'number_of_things': '10',\n 'record_identifier': 'sample1'}\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#retrieving-history","title":"Retrieving History","text":"<p>Pipestat records a history of reported results by default. If results have been overwritten, the historical results can be obtained via:</p> <pre><code>psm.retrieve_history(record_identifier=\"sample1\")\n</code></pre> <pre><code>{'output_file': {'2024-04-18 15:06:04': {'path': '/home/user/path.csv',\n   'title': 'CSV file with some data'}}}\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#removing-results","title":"Removing results","text":"<p><code>PipestatManager</code> object also supports results removal. Call <code>remove</code> method and provide <code>record_identifier</code> and  <code>result_identifier</code> arguments to do so:</p> <pre><code>psm.remove(record_identifier=\"sample1\",result_identifier=\"number_of_things\")\n</code></pre> <pre><code>Removed result 'number_of_things' for record 'sample1' from 'default_pipeline_name' namespace\n\n\n\n\n\nTrue\n</code></pre> <p>The entire record, skip the <code>result_identifier</code> argument:</p> <pre><code>psm.remove()\n</code></pre> <pre><code>Removing 'sample1' record\n\n\n\n\n\nTrue\n</code></pre> <p>Verify that an appropriate entry from the results was deleted:</p> <pre><code>psm.backend._data\n</code></pre> <pre><code>default_pipeline_name:\n  project: {}\n  sample: {}\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#highligting-results","title":"Highligting results","text":"<p>In order to highlight results we need to add an extra property in the pipestat results schema (<code>highlight: true</code>) under the result identifier that we wish to highlight. </p> <pre><code>from tempfile import mkstemp\n\n_, temp_file_highlight = mkstemp(suffix=\".yaml\")\nprint(temp_file_highlight)\n\npsm_highlight = pipestat.PipestatManager(\n    pipeline_name=\"test_highlight\",\n    record_identifier=\"sample1\",\n    results_file_path=temp_file_highlight,\n    schema_path=\"../tests/data/sample_output_schema_highlight.yaml\",\n)\n</code></pre> <pre><code>Initialize PipestatBackend\nInitialize FileBackend\n\n\n/tmp/tmpa9fo3rk7.yaml\n</code></pre> <p>For example, result <code>log</code> is highlighted in this case:</p> <pre><code>psm_highlight.result_schemas['log']\n</code></pre> <pre><code>{'highlight': True,\n 'description': 'The log file of the pipeline run',\n 'type': 'object',\n 'object_type': 'file',\n 'properties': {'path': {'type': 'string'}, 'title': {'type': 'string'}},\n 'required': ['path', 'title']}\n</code></pre> <p>The highlighting feature can be used by pipestat clients to present the highlighted results in a special way.</p> <pre><code>psm_highlight.highlighted_results\n</code></pre> <pre><code>['log', 'profile', 'commands', 'version']\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#pipeline-status-management","title":"Pipeline status management","text":"<p>Pipestat provides a pipeline status management system, which can be used to set and read pipeline status. To maintain the status information between sessions it uses flags or additional DB table if the <code>PipestatManager</code> object is backed with YAML file or PostgreSQL database, respectively.</p> <p>To set pipeline status use <code>set_status</code> method:</p> <pre><code>psm.set_status(record_identifier=\"sample1\", status_identifier=\"running\")\n</code></pre> <p>To get pipeline status use <code>get_status</code> method:</p> <pre><code>psm.get_status(record_identifier=\"sample1\")\n</code></pre> <pre><code>'running'\n</code></pre> <p>Allowable statuses and related metadata are defined in the status schema, which can be accessed via:</p> <pre><code>psm.cfg['_status_schema']\n</code></pre> <pre><code>{'running': {'description': 'the pipeline is running',\n  'color': [30, 144, 255]},\n 'completed': {'description': 'the pipeline has completed',\n  'color': [50, 205, 50]},\n 'failed': {'description': 'the pipeline has failed', 'color': [220, 20, 60]},\n 'waiting': {'description': 'the pipeline is waiting',\n  'color': [240, 230, 140]},\n 'partial': {'description': 'the pipeline stopped before completion point',\n  'color': [169, 169, 169]}}\n</code></pre> <p><code>pipestat</code> Python package ships with a default status schema, so we did not have to provide the schema when constructing the <code>PipestatManager</code> object. Similarly, the flags containg directory is an optional configuration option. </p> <p>Please refer to the Python API documentation (<code>__init__</code> method) to see how to use custom status schema and flags directory.</p>"},{"location":"pipestat/code/python-tutorial/#initializing-pipestatmanager-without-results-schema","title":"Initializing <code>PipestatManager</code> without results schema","text":"<p>Starting with <code>pipestat 0.0.3</code>, it is possible to initialize the <code>PipestatManager</code> object without specifying the results schema file. This feature comes in handy if <code>PipestatManager</code> is created with a sole intent to monitor pipeline status.</p> <p>Here's an example:</p> <pre><code>_, temp_file_no_schema = mkstemp(suffix=\".yaml\")\nprint(temp_file_no_schema)\n\npsm_no_schema = pipestat.PipestatManager(\n    pipeline_name=\"test_no_schema\", results_file_path=temp_file_no_schema\n)\n</code></pre> <pre><code>No schema supplied.\nInitialize PipestatBackend\nInitialize FileBackend\n\n\n/tmp/tmpxpe5w75w.yaml\n</code></pre> <p>As mentioned above, the pipeline status management capabilities are supported with no results schema defined:</p> <pre><code>psm_no_schema.set_status(status_identifier=\"running\", record_identifier=\"sample1\")\npsm_no_schema.get_status(record_identifier=\"sample1\")\n</code></pre> <pre><code>'running'\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#generate-static-html-report-using-the-summarize-command","title":"Generate static HTML Report using the <code>summarize</code> command","text":"<p>You can generate a static browsable html report using the <code>summarize</code> function:</p> <pre><code>psm.summarize()\n</code></pre> <pre><code>Building index page for pipeline: default_pipeline_name\n * Creating sample pages\n * Creating object pages\n\n\n\n\n\n'/tmp/reports/default_pipeline_name/index.html'\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#sample-and-project-level-pipelines","title":"Sample and Project Level Pipelines","text":"<p>All of the examples above assume the user has a sample level pipeline. Pipestat defaults to setting pipeline_type = 'sample'. However, the user can set the pipeline_type = 'project'.</p> <p>Beginning in Pipestat 0.6.0, the user can call SamplePipestatManager() or ProjectPipestatManager() that do everything PipestatManager does but sets the pipeline_type to either 'sample' or 'project' respectively.</p> <pre><code>psm_sample = pipestat. SamplePipestatManager(record_identifier=\"sample1\",\n    results_file_path=temp_file,\n    schema_path=\"../tests/data/sample_output_schema.yaml\",)\n</code></pre> <pre><code>Initialize PipestatBackend\nInitialize FileBackend\nInitialize PipestatMgrSample\n</code></pre> <pre><code>psm_sample.result_schemas[\"output_file\"]\n</code></pre> <pre><code>{'description': 'This a path to the output file',\n 'type': 'object',\n 'object_type': 'file',\n 'properties': {'path': {'type': 'string'}, 'title': {'type': 'string'}},\n 'required': ['path', 'title']}\n</code></pre>"},{"location":"pipestat/code/python-tutorial/#pipestatboss","title":"PipestatBoss","text":"<p>Also in Pipestat 0.6.0, the user can call PipestatBoss with the sample arguments as SamplePipestatManager or ProjectPipestatmanger while also including a list of pipeline_types. This will create and object containing multiple PipestatManager objects.</p> <pre><code>psb = pipestat.PipestatBoss(pipeline_list=['sample', 'project',], \n                   schema_path=\"../tests/data/sample_output_schema.yaml\", results_file_path=temp_file)\n</code></pre> <pre><code>Initialize PipestatBoss\nInitialize PipestatBackend\nInitialize FileBackend\nInitialize PipestatMgrSample\nInitialize PipestatBackend\nInitialize FileBackend\nInitialize PipestatMgrProject\n</code></pre> <pre><code>psb.samplemanager.report(record_identifier=\"sample1\",values={\n        \"output_file\": {\n            \"path\": \"/home/user/path.csv\",\n            \"title\": \"CSV file with some data\",\n        }\n    })\n</code></pre> <pre><code>[\"Reported records for 'sample1' in 'default_pipeline_name' :\\n - output_file: {'path': '/home/user/path.csv', 'title': 'CSV file with some data'}\",\n \"Reported records for 'sample1' in 'default_pipeline_name' :\\n - pipestat_created_time: 2023-11-07 17:31:18\",\n \"Reported records for 'sample1' in 'default_pipeline_name' :\\n - pipestat_modified_time: 2023-11-07 17:31:18\"]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"pipestat/code/reporting-objects/","title":"Reporting objects","text":"<p>This tutorial will show you how pipestat can report not just primitive types, but structured results as objects.</p> <p>First create a <code>pipestat.PipestatManager</code> object with our example schema:</p> <pre><code>import pipestat\npsm = pipestat.PipestatManager(\n    record_identifier=\"sample1\",\n    results_file_path=\"pipestat_results.yaml\",\n    schema_path=\"https://schema.databio.org/pipestat/object_result.yaml\",\n)\n</code></pre> <pre><code>Initialize FileBackend\n</code></pre> <p>Here, we're pointing to a remote <code>schema_path</code>. Let's take a look at the schema object. You can see a preview of it here at the schema server: https://schema.databio.org/?namespace=pipestat&amp;schema=object_result. We can also see what it looks like in Python code:</p> <pre><code>psm.schema\n</code></pre> <pre><code>ParsedSchema (refget)\n Project-level properties:\n - None\n Sample-level properties:\n - value : {'type': 'string', 'description': 'Value of the object referred to by the key'}\n - mydict : {'type': 'object', 'description': 'Can pipestat handle nested objects?'}\n Status properties:\n - None\n</code></pre> <p>This schema defines two sample-level variables: <code>value</code> is a string, and <code>mydict</code> is an object. Let's see how to report a <code>mydict</code>:</p> <pre><code>psm.report({\"mydict\": {\"toplevel\": {\"value\": 456}}}, force_overwrite=True)\n</code></pre> <pre><code>[\"Reported records for 'sample1' in 'refget' :\\n - mydict: {'toplevel': {'value': 456}}\"]\n</code></pre> <p>And now we can retrieve those results (which is returned as a python dict):</p> <pre><code>psm.retrieve_one(\"sample1\", \"mydict\")\n</code></pre> <pre><code>{'toplevel': {'value': 456}}\n</code></pre> <pre><code>psm.retrieve_one(\"sample1\", \"mydict\")['toplevel']\n</code></pre> <pre><code>{'value': 456}\n</code></pre> <pre><code>psm.retrieve_one(\"sample1\", \"mydict\")['toplevel']['value']\n</code></pre> <pre><code>456\n</code></pre>"},{"location":"pypiper/","title":"Pypiper","text":""},{"location":"pypiper/#what-is-pypiper","title":"What is pypiper?","text":"<p><code>Pypiper</code> is a development-oriented pipeline framework. It is a Python package that helps you write robust pipelines directly in Python, handling mundane tasks like restartability, monitoring for time and memory use, monitoring job status, copious log output, robust error handling, easy debugging tools, and guaranteed file output integrity.</p>"},{"location":"pypiper/#what-makes-pypiper-better","title":"What makes pypiper better?","text":"<p>With Pypiper, simplicity is paramount. Prerequisites are few: base Python and 2 common packages (<code>pyyaml</code> and <code>psutil</code>). It should take fewer than 15 minutes to build your first pipeline and only an hour or two to learn the advanced features. Pypiper pipelines are:</p> <ol> <li>written in pure Python, so they do not require learning a new language;</li> <li>easy to modify, so they are simple to update and maintain;</li> <li>simple to understand for an outsider, so they can be approached by others.</li> </ol> <p>These traits make pypiper ideally suited for pipelines under active development. Read more about the pypiper philosophy.</p>"},{"location":"pypiper/#installing","title":"Installing","text":"<p>Releases are posted as GitHub releases, or you can install from PyPI using <code>pip</code>:</p> <p>Global scope for single user: <pre><code>pip install --user --upgrade piper\n</code></pre></p> <p>Within an active virtual environment: <pre><code>pip install --upgrade piper\n</code></pre></p>"},{"location":"pypiper/#quick-start","title":"Quick start","text":"<p>To employ pypiper, you build something like a shell script, but pass the commands through the <code>run</code> method on a <code>PipelineManager</code> object. Build your pipeline in pure python:</p> <pre><code>#!/usr/bin/env python\n\nimport pypiper\noutfolder = \"hello_pypiper_results\" # Choose a folder for your results\n\n# Create a PipelineManager, the workhorse of pypiper\npm = pypiper.PipelineManager(name=\"hello_pypiper\", outfolder=outfolder)\n\n# Timestamps to delineate pipeline sections are easy:\npm.timestamp(\"Hello!\")\n\n# Now build a command and pass it to pm.run()\ntarget_file = \"hello_pypiper_results/output.txt\"\ncommand = \"echo 'Hello, Pypiper!' &gt; \" + target_file\npm.run(command, target_file)\n\npm.stop_pipeline()\n</code></pre> <p>Then invoke your pipeline via the command-line:</p> <pre><code>python my_pipeline.py --help\n</code></pre>"},{"location":"pypiper/#pypiper-strengths","title":"Pypiper strengths","text":"<p>Pypiper differs from existing frameworks in its focus on simplicity. Pypiper requires learning no new language, as pipelines are written in pure python. Pypiper is geared toward developing pipelines that are contained in a single file, easy to update, and easy to understand.</p>"},{"location":"pypiper/advanced-run-method/","title":"Run method options","text":"<p>The <code>PipelineManager.run()</code> function is the core of <code>pypiper</code>. In its simplest case, all you need to provide is a command to run, but it can be much more powerful with additional arguments.</p>"},{"location":"pypiper/advanced-run-method/#the-cmd-argument","title":"The <code>cmd</code> argument","text":"<p>Normally you just pass a string, but you can also pass a list of commands to <code>run</code>, like this:</p> <pre><code>pm.run([cmd1, cmd2, cmd3])\n</code></pre> <p>Pypiper will treat these commands as a group, running each one in turn (and monitoring them individually for time and memory use). The difference in doing it this way, rather than 3 separate calls to <code>run()</code> is that if the series does not complete, the entire series will be re-run. This is therefore useful to piece together commands that must all be run together.</p>"},{"location":"pypiper/advanced-run-method/#the-target-and-lock_name-arguments","title":"The <code>target</code> and <code>lock_name</code> arguments","text":"<p>If you provide a <code>target</code> file, then <code>pypiper</code> will first check to see if that target exists, and only run the <code>command</code> if the <code>target</code> does not exist. To prevent two pipelines from running commands on the same target, <code>pypiper</code> will automatically derive a lock file name from your target file. You can use the <code>lock_name</code> argument to override this default. If you do not provide a <code>target</code>, then you will need to provide a <code>lock_name</code> argument because <code>pypiper</code> will not be able to derive one automatically.</p>"},{"location":"pypiper/advanced-run-method/#the-nofail-argument","title":"The <code>nofail</code> argument","text":"<p>By default, a command that fails will cause the entire pipeline to halt. If you want to provide a command that should not halt the pipeline upon failure, set <code>nofail=True</code>. <code>nofail</code> can be used to implement non-essential parts of the pipeline.</p>"},{"location":"pypiper/advanced-run-method/#the-follow-argument","title":"The <code>follow</code> argument","text":"<p>The <code>PipelineManager.run</code> function has an optional argument named <code>follow</code> that is useful for checking or reporting results from a command. To the <code>follow</code> argument you must pass a python function (which may be either a defined function or a <code>lambda</code> function). These follow functions are then coupled to the command that is run; the follow function will be called by python if and only if the command is run. </p> <p>Why is this useful? The major use cases are QC checks and reporting results. We use a follow function to  run a QC check to make sure processes did what we expect, and then to report that result to the <code>stats</code> file. We only need to check the result and report the statistic once, so it's best to put these kind of checks in a <code>follow</code> function. Often, you'd like to run a function to examine the result of a command, but you only want to run that once, right after the command that produced the result. For example, counting the number of lines in a file after producing it, or counting the number of reads that aligned right after an alignment step. You want the counting process coupled to the alignment process, and don't need to re-run the counting every time you restart the pipeline. Because pypiper is smart, it will not re-run the alignment once it has been run; so there is no need to re-count the result on every pipeline run! </p> <p>Follow functions let you avoid running unnecessary processes repeatedly in the event that you restart your pipeline multiple times (for instance, while debugging later steps in the pipeline).</p>"},{"location":"pypiper/advanced-run-method/#the-container-argument","title":"The <code>container</code> argument","text":"<p>If you specify a string here, <code>pypiper</code> will wrap the command in a <code>docker run</code> call using the given <code>container</code> image name.</p>"},{"location":"pypiper/advanced-run-method/#the-shell-argument-python-subprocess-types","title":"The <code>shell</code> argument: Python subprocess types","text":"<p>Since Pypiper runs all your commands from within python (using the <code>subprocess</code> python module), it's nice to be aware of the two types of processes that <code>subprocess</code> allows: direct processes and shell processes. </p> <p>Direct process: A direct process is executed and managed by Python, so Python retains control over the process completely. This enables Python to monitor the memory use of the subprocess and keep track of it more efficiently. The disadvantage is that you may not use shell-specific operators; for instance, a shell like <code>Bash</code> is what understands an asterisk (<code>*</code>) for wildcard expansion, or a bracket (<code>&gt;</code>) for output redirection, or a pipe (<code>|</code>) to string commands together; these therefore cannot be used in direct subprocesses in Python.</p> <p>Shell process: In a shell process, Python first spawns a shell, and then runs the command in that shell. The spawned shell is the process controlled by Python, but processes in the shell are not. This allows you to use shell operators (e.g. <code>*</code>, <code>&gt;</code>), but at the cost of the ability to monitor each command independently, because Python does not have direct control over subprocesses run inside a subshell.</p> <p>Because we'd like to run direct subprocesses whenever possible, <code>pypiper</code> includes 2 nice provisions that help us deal with shell processes. First, pypiper automatically divides commands with pipes (<code>|</code>) and executes them as direct processes. This enables you to pass a piped shell command, but still get the benefit of a direct process. Each process in the pipe is monitored for return value and for memory use individually, and this information is reported in the pipeline log. Nice! Second, pypiper uses the <code>psutil</code> module to monitor memory of all child processes. That means when you use a shell process, we do monitor the memory use of that process (and any other processes it spawns), which gives us more accurate memory monitoring -- but not from each task individually.</p> <p>You can force Pypiper by specifying <code>shell=True</code> or <code>shell=False</code> to the <code>run</code> function, but really, you shouldn't have to. By default Pypiper will try to guess: if your command contains <code>*</code> or <code>&gt;</code>, it will be run in a shell. If it contains a pipe (<code>|</code>), it will be split and run as direct, piped subprocesses. Anything else will be run as a direct subprocess.</p>"},{"location":"pypiper/best-practices/","title":"Best practices","text":"<p>Here are some guidelines for how you can design the most effective pipelines.</p> <ul> <li> <p>Compartmentalize output into folders.      In your output, keep pipeline steps separate by organizing output into subfolders.</p> </li> <li> <p>Use git for versioning.      If you develop your pipeline in a git repository, Pypiper will automatically record the commit hash when you run a pipeline, making it easy to figure out exactly what code version you ran.</p> </li> <li> <p>Record stats as you go.      In other words, don't do all your stats (<code>report_result()</code>) and QC at the end; do it along the way. This facilitates monitoring and maximizes availability of statistics even when a pipeline fails.</p> </li> <li> <p>Use looper args.      Even if you're not using looper at first, use <code>looper_args</code> and your pipeline will be looper-ready when it comes time to run 500 samples.</p> </li> <li> <p>Use NGSTk early on.      <code>NGSTk</code> has lots of useful functions that you will probably need. We've worked hard to make these robust and universal. For example, using NGSTk, you can easily make your pipeline take flexible input formats (FASTQ or BAM). Right now you may always have the same input type (FASTQ, for example), but later you may want your pipeline to be able to work from <code>bam</code> files. We've already written simple functions to handle single or multiple BAM or FASTQ inputs; just use this infrastructure (in <code>NGSTk</code>) instead of writing your own, and you'll save yourself future headaches.</p> </li> <li> <p>Make some important parameters in the pipeline config, instead of hardcoding them     Pypiper makes it painfully easy to use a config file to make your pipeline configurable. Typically you'll start by hard-coding in those parameters in your pipeline steps. But you can select a few important parameters and make them customizable in the pipeline config. Start from the very beginning by making a <code>yaml</code> pipeline config file. See an example of a pipeline config file.</p> </li> </ul>"},{"location":"pypiper/changelog/","title":"Changelog","text":""},{"location":"pypiper/changelog/#0145-2025-09-22","title":"[0.14.5] -- 2025-09-22","text":""},{"location":"pypiper/changelog/#changed","title":"Changed","text":"<ul> <li>Remove veracitools dependency #233</li> </ul>"},{"location":"pypiper/changelog/#0144-2025-02-25","title":"[0.14.4] -- 2025-02-25","text":""},{"location":"pypiper/changelog/#changed_1","title":"Changed","text":"<ul> <li>Fixed warnings for Python &gt;3.12</li> <li>Updated version of Python to 3.13 in pytests</li> </ul>"},{"location":"pypiper/changelog/#0143-2024-10-02","title":"[0.14.3] -- 2024-10-02","text":""},{"location":"pypiper/changelog/#changed_2","title":"Changed","text":"<ul> <li>bump requirements to require pipestat&gt;=0.11.0</li> </ul>"},{"location":"pypiper/changelog/#0142-2024-05-07","title":"[0.14.2] -- 2024-05-07","text":""},{"location":"pypiper/changelog/#changed_3","title":"Changed","text":"<ul> <li>Addresses #218</li> </ul>"},{"location":"pypiper/changelog/#0141-2024-04-19","title":"[0.14.1] -- 2024-04-19","text":""},{"location":"pypiper/changelog/#changed_4","title":"Changed","text":"<ul> <li>remove pipestat_project_name from PipelineManager parameters</li> <li>refactor pipestat_sample_name to pipestat_record_identifier in PipelineManager parameters</li> <li>update requirements for pipestat 0.9.0, ubiquerg 0.8.0, and yacman 0.9.3</li> <li>set <code>force_overwrite</code> to default to true, Issue #209</li> </ul>"},{"location":"pypiper/changelog/#0140-2023-12-22","title":"[0.14.0] -- 2023-12-22","text":""},{"location":"pypiper/changelog/#changed_5","title":"Changed","text":"<ul> <li>refactor for pipestat v0.6.0 release</li> <li>drop python 2.7</li> <li>updated requirements</li> <li>changed message_raw to be a value_dict when reporting to conform to pipestat</li> </ul>"},{"location":"pypiper/changelog/#fixed","title":"Fixed","text":"<ul> <li>fixed #196 and #197</li> </ul>"},{"location":"pypiper/changelog/#added","title":"Added","text":"<ul> <li>added <code>force_overwrite</code> to <code>report_result</code> and <code>report_object</code></li> <li>added pipestat_pipeline_type, defaulting to sample-level</li> </ul>"},{"location":"pypiper/changelog/#0132-2023-08-02","title":"[0.13.2] -- 2023-08-02","text":""},{"location":"pypiper/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>fixed self.new_start overriding checkpoints.</li> </ul>"},{"location":"pypiper/changelog/#0131-2023-07-14","title":"[0.13.1] -- 2023-07-14","text":""},{"location":"pypiper/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>added _safe_write_to_file back into pypiper for Pepatac backwards compatibility</li> </ul>"},{"location":"pypiper/changelog/#0130-2023-06-29","title":"[0.13.0] -- 2023-06-29","text":""},{"location":"pypiper/changelog/#added_1","title":"Added","text":"<ul> <li>pipestat support </li> </ul>"},{"location":"pypiper/changelog/#0123-2022-01-25","title":"[0.12.3] -- 2022-01-25","text":""},{"location":"pypiper/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>A few bugs with compatibility with Python version 3.9</li> </ul>"},{"location":"pypiper/changelog/#0122-2021-12-20","title":"[0.12.2] -- 2021-12-20","text":""},{"location":"pypiper/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Removed use2to3 for compatibility with setuptools 58</li> </ul>"},{"location":"pypiper/changelog/#0121-2019-08-29","title":"[0.12.1] -- 2019-08-29","text":""},{"location":"pypiper/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Increased requirement for logmuse</li> </ul>"},{"location":"pypiper/changelog/#changed_6","title":"Changed","text":"<ul> <li>Sort argument outputs in logs</li> <li>Fail messages can now be a string (previously required an Exception).</li> </ul>"},{"location":"pypiper/changelog/#0120-2019-08-14","title":"[0.12.0] -- 2019-08-14","text":""},{"location":"pypiper/changelog/#added_2","title":"Added","text":"<ul> <li>Use profile to determine total elapsed time</li> <li><code>logging</code> functions directly on <code>PipelineManager</code></li> <li>Re-export <code>add_logging_options</code> from <code>logmuse</code>, for direct use by a pipeline author.</li> <li><code>logger_via_cli</code> that defaults to the <code>strict=False</code> behavior of the same-named function from <code>logmuse</code></li> <li>Use logging for pypiper-generated output.</li> </ul>"},{"location":"pypiper/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Fix childless processes memory monitoring issue</li> <li>Fix problems with runtime reading from pipeline profile TSV formatted according to two styles</li> <li>Fix problems running containerized executables that would sometimes hang</li> <li>Fix inaccurate elapsed time accumulation </li> <li>Fixed a bug that caused hanging when running in singularity containerized executables</li> <li>Fixed bugs with merging bamfiles using samtools</li> </ul>"},{"location":"pypiper/changelog/#changed_7","title":"Changed","text":"<ul> <li>The hashes in the pipeline profile are produced from the entire original command, even if it is a pipe  </li> <li>Changed output to simplify and improve log readability</li> </ul>"},{"location":"pypiper/changelog/#0113-2019-06-17","title":"[0.11.3] -- 2019-06-17","text":""},{"location":"pypiper/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Fixed a bug that caused an OSError removing lock files for some filesystems.</li> </ul>"},{"location":"pypiper/changelog/#0112-2019-06-06","title":"[0.11.2] -- 2019-06-06","text":""},{"location":"pypiper/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Elevate <code>attmap</code> dependency bound to require inclusion of improved path expansion behavior.</li> </ul>"},{"location":"pypiper/changelog/#0111-2019-05-30","title":"[0.11.1] -- 2019-05-30","text":""},{"location":"pypiper/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Elevate <code>attmap</code> dependency bound to require inclusion of a bugfix there.</li> </ul>"},{"location":"pypiper/changelog/#0110-2019-05-13","title":"[0.11.0] -- 2019-05-13","text":"<ul> <li>Improve python3 handling of integers and strings</li> <li>Fixed a bug with cleanup scripts in <code>dirty</code> mode</li> <li>Restructured profile output with hash and processID, and made lock paths relative</li> <li>Streamlined some logging outputs</li> <li>Allows nested parentheses and braces for piped commands</li> <li>Fixed a bug that would have split a pipe within a braced command</li> <li>Some performance improvements for ngstk functions</li> <li>Allow <code>ngstk.input_to_fastq</code> to yield gzipped fastq files</li> </ul>"},{"location":"pypiper/changelog/#0100-2019-03-22","title":"[0.10.0] -- 2019-03-22","text":"<ul> <li>Fixed a bug that raised exception with empty commands</li> <li>Fixed the pipeline profiling issues</li> <li>Major updates to internal systems: Switch to <code>attmap</code></li> <li>Revamped way of handling child subprocesses which should lead to more efficient memory monitoring of piped subprocesses, and more consistent handling of rogues subprocesses during pipeline failure.</li> <li>Added force mode to ngstk <code>gzip</code> and <code>pigz</code> use.</li> <li>Changed documentation from sphinx to mkdocs.</li> <li>Fixed a bug with python3 output buffering</li> <li>Implement multi-target commands</li> <li>Fixed a bug that had prevented new start mode from working in certain cases.</li> <li>Allow user to change units of memory passed in with default pypiper cli.</li> </ul>"},{"location":"pypiper/changelog/#094-2019-01-31","title":"[0.9.4] -- 2019-01-31","text":"<ul> <li>Point release to PyPI for README rendering.</li> </ul>"},{"location":"pypiper/changelog/#093-2019-01-31","title":"[0.9.3] -- 2019-01-31","text":"<ul> <li>Simple point release update to fix PyPI landing page.</li> </ul>"},{"location":"pypiper/changelog/#092-2019-01-30","title":"[0.9.2] -- 2019-01-30","text":"<ul> <li>Never echo protected-looking attribute request.</li> </ul>"},{"location":"pypiper/changelog/#091-2019-01-29","title":"[0.9.1] -- 2019-01-29","text":"<ul> <li>Fixed a bug in NGSTk that caused errors for read counting functions on  MACOS. MACOS <code>wc</code> returns leading whitespace, which caused these functions to fail.</li> </ul>"},{"location":"pypiper/changelog/#090-2018-11-19","title":"[0.9.0] -- 2018-11-19","text":"<ul> <li>Use <code>psutil</code> to track aggregate memory usage for processes that spawn children. This results in accurate memory records for these processes.</li> <li>Individual commands in a string of commands connected by shell pipes are now treated as individual commands, and and monitored individually for time and memory, and if a single component, fails, the entire string will fail. Previously, only the final return command was recorded, as in <code>bash</code>.</li> <li>Various other small improvements (like waiting checking for dynamic recover flags)</li> </ul>"},{"location":"pypiper/changelog/#081-2018-09-20","title":"[0.8.1] -- 2018-09-20","text":"<ul> <li>Fixed a bug that caused a problem for some pipelines adding groups of pypiper args.</li> <li>Improved the <code>run</code> waiting method to immediately stop upon job   completion, rather than minute-increment polling. This should improve   performance particularly in pipelines with many, medium-runtime steps, and   improve accuracy of timing profiles.</li> </ul>"},{"location":"pypiper/changelog/#080-2018-06-15","title":"[0.8.0] -- 2018-06-15","text":"<ul> <li>Implemented 'new start' mode.</li> <li>Improved error messages and exception handling for missing child software.</li> <li>Clarified the built-in required vs. optional args by allowing pipeline authors to specify which of the pypiper args are required. The command-line help UI now displays these correctly as 'required arguments' instead of incorrectly as 'optional arguments'.</li> <li>Corrected the sort order of added arguments, so they are listed in the help menu more naturally.</li> <li>Fixed a bug that caused an erroneous error message indicating missing pypiper args.</li> <li>Clarified the license is BSD2</li> <li>Fixed a bug that neglected to list pyyaml as a dependency</li> </ul>"},{"location":"pypiper/changelog/#072-2018-06-05","title":"[0.7.2] -- 2018-06-05","text":"<ul> <li>Implemented the 'report object' function.</li> <li>Cleanup files are now relative, so a moved folder could still be cleaned.</li> <li>Fixed a bug that prevented install if pypandoc was not installed</li> <li>Fixed a bug that caused an error in containers where /proc wasn't accessible</li> </ul>"},{"location":"pypiper/changelog/#071-2018-02-27","title":"[0.7.1] -- 2018-02-27","text":"<ul> <li>Package cleanup for Pypi.</li> </ul>"},{"location":"pypiper/changelog/#070-2017-12-12","title":"[0.7.0] -- 2017-12-12","text":"<ul> <li>Standardize <code>NGSTk</code> function naming.</li> <li>Introduce <code>Stage</code> as a model for a logically related set of pipeline processing steps.</li> <li>Introduce <code>Pipeline</code> framework for automated processing phase execution and checkpointing.</li> <li>Add ability to start and/or stop a pipeline at arbitrary checkpoints.</li> <li>Introduce new state for a paused/halted pipeline.</li> <li>Improve spawned process shutdown to avoid zombie processes.</li> </ul>"},{"location":"pypiper/changelog/#060-2017-08-24","title":"[0.6.0] -- 2017-08-24","text":"<ul> <li>Adds 'dynamic recovery' capability. For jobs that are terminated by an interrupt, such as a SIGINT or SIGTERM (as opposed to a failed command), pypiper will now set a dynamic recovery flags. These jobs, when restarted, will automatically pick up where they left off, without requiring any user intervention. Previously, the user would have to specify recover mode (<code>-R</code>). Now, recover mode forces a recover regardless of failure type, but interrupted pipelines will auto-recover.</li> <li>Pypiper now appropriately adds cleanup files intermediate files for failed runs. It adds them to the cleanup script.</li> <li>Improves error messages so only a single exception is raised with a more direct relevance to the user/</li> <li>Pypiper will automatically remove existing flags when the run starts, eliminating the earlier issue of confusion due to multiple flags present on runs that were restarted.</li> <li>Fixes a bug that caused a pipeline to continue if a SIGTERM is given during a process that was marked <code>nofail</code>.</li> <li>Pypiper now can handle multiple SIGTERMs without one canceling the shutdown procedure begun by the other.</li> <li>Major improvements to documentation and tutorials.</li> <li>Adds <code>report_figure</code> function.</li> </ul>"},{"location":"pypiper/changelog/#050-2017-07-21","title":"[0.5.0] -- 2017-07-21","text":"<ul> <li>Adds preliminary support for handling docker containers</li> <li>Updates docs, adds Hello World example</li> <li>Adds 'waiting' flag</li> <li>Eliminates extra spaces in reported results</li> <li>Pypiper module is version aware</li> <li>Updates Success time format to eliminate space</li> <li>Improves efficiency in some ngstk merging functions</li> </ul>"},{"location":"pypiper/changelog/#040-2017-01-23","title":"[0.4.0] -- 2017-01-23","text":"<ul> <li>First major public release!</li> <li>Revamps pypiper args</li> <li>Adds parallel compression/decompression with pigz</li> <li>Various small bug fixes and speed improvements</li> </ul>"},{"location":"pypiper/clean/","title":"Cleaning up intermediate files","text":"<p>Many pipelines produce intermediate files along the way. Should you retain these files or delete them?</p> <p>On the one hand, you may not necessarily want to delete them immediately after creating them, because what if a later pipeline step fails and you need to inspect an intermediate file? On the other hand, you may not want those intermediate files sticking around forever because they waste valuable disk space.</p> <p>Pypiper solves this problem with the concept of a clean list. The clean list is simply a list of files that are flagged for eventual cleanup. A pipeline developer adds to this list using <code>pm.clean_add(filename)</code>. Files on the clean list are not cleaned immediately; instead, they are removed as soon as the pipeline is completed successfully (in other words, after <code>pm.complete_pipeline()</code> is called). The advantage is that intermediate files will always be available as long as a pipeline has not completed successfully.</p> <p>In case a user of a pipeline instead wants to retain these files indefinitely, he or she may simply add <code>--dirty</code> when invoking the pipeline script. This instructs pypiper to not clean the intermediate files, even after a successful pipeline run. In this case, <code>pypiper</code> will produce a shell script (<code>clean.sh</code>), which can be run to remove all flagged files at a later point.</p>"},{"location":"pypiper/cli/","title":"Command-line arguments","text":"<p>Your final pypiper pipeline will be a python script that a pipeline user will invoke on the command-line. You will likely need to allow the user to change some parameters on the command line, and to take full advantage of Pypiper (make your pipeline recoverable, etc.), you will need to add command-line options to your pipeline that change pypiper's settings as well. Pypiper uses the typical Python argparse module to define command-line arguments to your pipeline, and offers a series of built-in functions to help you populate your pipeline's <code>ArgumentParser</code> with pypiper-specific options.</p> <p>You can use an ArgumentParser as usual, adding whatever arguments you like. Then, you add Pypiper args to your parser with the function <code>add_pypiper_args()</code>, and pass command-line options and arguments to your <code>PipelineManager</code>, like this:</p> <pre><code>import pypiper, os, argparse\nparser = ArgumentParser(description='Write a short description here')\n\n# add any custom args here\n# e.g. parser.add_argument('--foo', help='foo help')\n\n# once you've established all your custom arguments, we can add the default\n# pypiper arguments to your parser like this:\n\nparser = pypiper.add_pypiper_args(parser)\n\n# Then, pass the args parsed along to the PipelineManger\n\nargs = parser.parse_args()\n\npipeline = pypiper.PipelineManager(name=\"my_pipeline\", outfolder=\"out\", \\\n                    args=args)\n</code></pre> <p>Once you've added pypiper arguments, your pipeline will then enable a few built-in arguments: <code>--recover</code>, <code>--follow</code>, and <code>--dirty</code>, for example. As a side bonus, all arguments (including any of your custom arguments) will be recorded in the log outputs. </p> <p>That's the basics. But you can customize things for more efficiency using a simple set of pre-built args and groups of args in pypiper:</p>"},{"location":"pypiper/cli/#universal-pypiper-options","title":"Universal pypiper options","text":"<p>With that said, there are a few universal (Pypiper-added) options that are frequently (but not necessarily always) honored by pypiper pipelines. These default pypiper arguments are detailed below:</p> <ul> <li> <p><code>-R, --recover</code>     Recover mode, overwrite locks. This argument will tell pypiper to recover from a failed previous run. Pypiper will execute commands until it encounters a locked file, at which point it will re-execute the failed command and continue from there.</p> </li> <li> <p><code>-F, --follow</code>     Force run follow-functions. By default, follow-functions are only run if their corresponding <code>run</code> command was run; with this option you can force all follow functions to run. This is useful for regenerating QC data on existing output. For more details, see :ref:<code>the follow argument &lt;the_follow_argument&gt;</code>.</p> </li> <li> <p><code>-D, --dirty</code>     Make all cleanups manual. By default, pypiper pipelines will delete any intermediate files. For debugging, you may want to turn this option off -- you can do that by specifying dirty mode.</p> </li> <li> <p><code>-N, --new-start</code>     New start mode. This flag will tell pypiper to start over, and run every command, even if its target output already exists.</p> </li> </ul>"},{"location":"pypiper/cli/#customizing-add_pypiper_args","title":"Customizing <code>add_pypiper_args()</code>","text":"<p>There are two ways to modulate the arguments added by <code>add_pypiper_args()</code> function: the <code>groups</code> argument, which lets you add argument groups; or the <code>args</code> argument, which lets you add arguments individually. By default, <code>add_pypiper_args()</code> add all arguments listed in the <code>pypiper</code> group. You may instead pass a list of one or more of these groups of arguments (to <code>groups</code>) or individual arguments (to <code>args</code>) to customize exactly the set of built-in options your pipeline implements.</p> <p>For example, <code>parser.add_pypiper_args(parser, groups=['pypiper', 'common'])</code> will add all arguments listed under <code>pypiper</code> and <code>common</code> below:</p>"},{"location":"pypiper/cli/#built-in-arguments-accessed-with-add_pypiper_args","title":"Built-in arguments accessed with <code>add_pypiper_args()</code>","text":"<p>Individual arguments that are understood and used by pypiper:</p> <ul> <li><code>-R, --recover</code>: for a failed pipeline run, start off at the last successful step. </li> <li><code>-N, --new-start</code>: Just recreate everything, even if it exists.</li> <li><code>-D, --dirty</code>: Disables automatic cleaning of temporary files, so all intermediate files will still exist after a pipeline run (either successful or failed). Useful for debugging a pipeline even if it succeeds.</li> <li><code>-F, --follow</code>: Runs all <code>follow-functions</code>, regardless of whether the accompanying command is run.</li> <li><code>-C, --config</code>: Pypiper pipeline config yaml file.</li> </ul> <p>Individual arguments just provided for convenience and standardization: - <code>-S, --sample-name</code>: name of the sample - <code>-I, --input</code>: primary input file (e.g. read1) - <code>-I2, --input2</code>: secondary input file (e.g. read2) - <code>-O, --output-parent</code>: parent folder for pipeline results (the pipeline will use this as the parent directory for a folder named <code>sample-name</code>) - <code>-P, --cores</code>: Number of cores to use - <code>-M, --mem</code>: Amount of memory in megabytes - <code>-G, --genome</code>: Reference genome assembly (e.g. <code>hg38</code>) - <code>-Q, --simple-or-paired</code>: For sequencing data, is input single-end or paired-end?</p>"},{"location":"pypiper/cli/#pre-built-collections-of-arguments-added-via-groups","title":"Pre-built collections of arguments added via <code>groups</code>:","text":"<ul> <li>pypiper: <code>recover</code>, <code>new-start</code>, <code>dirty</code>, <code>follow</code></li> <li>common: <code>input</code>, <code>sample-name</code></li> <li>config: <code>config</code></li> <li>resource: <code>mem</code>, <code>cores</code></li> <li>looper: <code>config</code>, <code>output-parent</code>, <code>mem</code>, <code>cores</code></li> <li>ngs: <code>input</code>, <code>sample-name</code>, <code>input2</code>, <code>genome</code>, <code>single-or-paired</code></li> </ul>"},{"location":"pypiper/cli/#specifying-required-built-in-arguments","title":"Specifying required built-in arguments","text":"<p>If you're using the built-in arguments, you may want to module which are required and which are not. That way, you can piggyback on how <code>ArgumentParser</code> handles required arguments very nicely -- if the user does not specify a required argument, the pipeline will automatically prompt with usage instructions.</p> <p>By default, built-in arguments are not flagged as required, but you can pass a list of required built-ins to the <code>required</code> parameter, like <code>add_pypiper_args(parser, args=[\"sample-name\"], required=[\"sample-name\"])</code>.</p>"},{"location":"pypiper/cli/#examples","title":"Examples","text":"<pre><code>import pypiper, os, argparse\nparser = ArgumentParser(description='Write a short description here')\n\n# add just arguments from group `pypiper`\nparser = pypiper.add_pypiper_args(parser, groups=[\"pypiper\"])\n\n# add just arguments from group `common`\nparser = pypiper.add_pypiper_args(parser, groups=[\"common\"])\n\n# add arguments from two groups\nparser = pypiper.add_pypiper_args(parser, groups=[\"common\", \"resources\"],\n                                    required=[\"sample-name\", \"output-parent\"])\n\n# add individual argument\nparser = pypiper.add_pypiper_args(parser, args=[\"genome\"])\n\n# add some groups and some individual arguments\nparser = pypiper.add_pypiper_args(parser, args=[\"genome\"], groups=[\"looper\", \"ngs\"])\n</code></pre>"},{"location":"pypiper/configuration/","title":"Pipeline configuration files","text":"<p>If you write a pipeline config file in <code>yaml</code> format and name it the same thing as the pipeline (but replacing <code>.py</code> with <code>.yaml</code>), pypiper will automatically load and provide access to these configuration options, and make it possible to pass customized config files on the command line. This is very useful for tweaking a pipeline for a similar project with slightly different parameters, without having to re-write the pipeline.</p> <p>It's easy: just load the <code>PipelineManager</code> with <code>args</code> (as described in command-line arguments), and you have access to the config file automatically in in <code>pipeline.config</code>.</p> <p>For example, in <code>myscript.py</code> you write:</p> <pre><code>parser = pypiper.add_pipeline_args(parser, args=[\"config\"])\npipeline = pypiper.PipelineManager(name=\"my_pipeline\", outfolder=outfolder, \\\n                    args = parser)\n</code></pre> <p>And in the same folder, you include <code>myscript.yaml</code>:</p> <pre><code>my_section:\n  setting1: True\n  setting2: 15\n</code></pre> <p>Then you can access these settings automatically in your script using:</p> <pre><code>pipeline.config.my_section.setting1\npipeline.config.my_section.setting2\n</code></pre> <p>This <code>yaml</code> file is useful for any parameters not related to the input Sample (which should be passed on the command-line). By convention, for consistency across pipelines, we use sections called <code>tools</code>, <code>resources</code>, and <code>parameters</code>, but the developer has the freedom to add other sections/variables as needed.</p> <p>Here's a more realist example pipeline configuration file:</p> <pre><code># paths to required tools\ntools:\n  java:  \"/home/user/.local/tools/java\"\n  trimmomatic:  \"/home/user/.local/tools/trimmomatic.jar\"\n  fastqc:  \"fastqc\"\n  samtools:  \"samtools\"\n  bsmap:  \"/home/user/.local/tools/bsmap\"\n  split_reads:  \"/home/user/.local/tools/split_reads.py\"\n\n# paths to reference genomes, adapter files, and other required shared data\nresources:\n  resources: \"/data/groups/lab_bock/shared/resources\"\n  genomes: \"/data/groups/lab_bock/shared/resources/genomes/\"\n  adapters: \"/data/groups/lab_bock/shared/resources/adapters/\"\n\n# parameters passed to bioinformatic tools, subclassed by tool\nparameters:\n  trimmomatic:\n    quality_encoding: \"phred33\"\n    threads: 30\n    illuminaclip:\n      adapter_fasta: \"/home/user/.local/tools/resources/cpgseq_adapter.fa\"\n      seed_mismatches: 2\n      palindrome_clip_threshold: 40\n      simple_clip_threshold: 7\n    slidingwindow:\n      window_size: 4\n      required_quality: 15\n    maxinfo:\n      target_length: 17\n      strictness: 0.5\n    minlen:\n      min_length: 17\n  bsmap:\n    seed_size: 12\n    mismatches_allowed_for_background: 0.10\n    mismatches_allowed_for_left_splitreads: 0.06\n    mismatches_allowed_for_right_splitreads: 0.00\n    equal_best_hits: 100\n    quality_threshold: 15\n    quality_encoding: 33\n    max_number_of_Ns: 3\n    processors: 8\n    random_number_seed: 0\n    map_to_strands: 0\n</code></pre>"},{"location":"pypiper/contributing/","title":"Contributing","text":"<p>We welcome contributions in the form of pull requests.</p> <p>If proposing changes to package source code, please run the test suite in <code>python2</code> and <code>python3</code> by running <code>pytest</code> or <code>python setup.py test</code> from within the repository root.</p> <p>If using <code>pytest</code> directly, we suggest first activating the appropriate Python version's virtual environment and running <code>pip install --upgrade ./</code>. Otherwise, simply specify the appropriate Python version, i.e. <code>python2 setup.py test</code> or <code>python3 setup.py test</code>.</p>"},{"location":"pypiper/faq/","title":"FAQ","text":""},{"location":"pypiper/faq/#how-can-i-run-my-pipeline-on-more-than-1-sample","title":"How can I run my pipeline on more than 1 sample?","text":"<p>Pypiper only handles individual-sample pipelines. To run it on multiple samples, write a loop, or use looper. Dividing multi-sample handling from individual sample handling is a conceptual advantage that allows us to write a nice, universal, generic sample-handler that you only have to learn once.</p>"},{"location":"pypiper/faq/#what-cluster-resources-can-pypiper-use","title":"What cluster resources can pypiper use?","text":"<p>Pypiper is compute-agnostic. You run it wherever you want. If you want a nice way to submit pipelines for samples any cluster manager, check out looper, which can run your pipeline on any compute infrastructure using the divvy python package.</p>"},{"location":"pypiper/faq/#what-does-it-mean-for-a-sample-to-be-in-the-waiting-state","title":"What does it mean for a sample to be in the \"waiting\" state?","text":"<p>Waiting means <code>pypiper</code> encountered a file lock, but no recovery flag. So the pipeline thinks a process (from another run or another process) is currently writing that file. It periodically checks for the lock file to disappear, and assumes that the other process will unlock the file when finished. If you are sure there's not another process writing to that file, you can get <code>pypiper</code> to continue by deleting the corresponding <code>lock</code> file. In the future, you can use <code>pypiper's</code> recover mode (<code>-R</code>) to automatically restart a process when a <code>lock</code> file is found, instead of waiting.</p>"},{"location":"pypiper/faq/#what-is-the-elapsed-time-in-output","title":"What is the 'elapsed time' in output?","text":"<p>The \"elapsed\" time is referring to the amount of time since the preceding timestamp, not since the start of the pipeline. Timestamps are all displayed with a flag: <code>_TIME_</code>. The total cumulative time for the pipeline is displayed only at the end.</p>"},{"location":"pypiper/faq/#how-should-i-run-a-qc-step-to-check-results-of-one-of-my-commands","title":"How should I run a QC step to check results of one of my commands?","text":"<p>Usually, you only want to run a QC step if the result was created in the same pipeline run. There's no need to re-run that step if you have to restart the pipeline due to an error later on. If you use <code>run()</code> for these steps, then they'll need to run each time the pipeline runs. Instead, this is exactly why we created the follow argument This option lets you couple a QC step to a <code>run()</code> call, so it only gets executed when it is required.</p>"},{"location":"pypiper/faq/#how-do-i-solve-installation-errors-involving-psutil-andor-a-compiler-like-gcc-or-clang","title":"How do I solve installation errors involving <code>psutil</code> and/or a compiler like <code>gcc</code> or <code>clang</code>?","text":"<p>If you have trouble with installation and it looks like one of these pieces of software is involved, please check the <code>psutil</code> installation guide.</p>"},{"location":"pypiper/features/","title":"Pypiper features at-a-glance","text":"<p> Simplicity</p> <p>Pipelines are simple both to use and to develop. A pypiper pipeline is nothing more than a python script. You run it on the command line like you would any other python script. The basic documentation is just a few pages. It should only take you 15 minutes to write your first pipeline. </p> <p> Restartability</p> <p>Commands check for their targets and only run if the target needs to be created. This provides computational advantages, and also means the pipeline will pick up where it left off in case it needs to be restarted or extended.</p> <p> File integrity protection</p> <p>Pypiper uses automatic file locks. This ensures that tasks complete, and pipelines never continue with half-finished analysis. It also ensures that multiple pipeline runs will not interfere with one another -even if the steps are identical and produce the same files.</p> <p> Copious logging</p> <p>Pypiper automatically prints output to screen and also stores it in a log file, so all subprocess output is captured permanently. It also provides copious information on versions, compute host, and easy timestamping.</p> <p> Memory use monitoring</p> <p>Processes are polled for memory use, allowing you to more accurately gauge your future memory requirements.</p> <p> Job status monitoring</p> <p>Pypiper automatically creates status flag files, so you can summarize the current state (<code>running</code>, <code>failed</code>, or <code>completed</code>) of hundreds of jobs simultaneously.</p> <p> Easy result reports</p> <p>Pypiper provides functions to put key-value pairs into an easy-to-parse stats file, making it easy to summarize your pipeline results.</p> <p> Robust error handling</p> <p>Pypiper closes pipelines gracefully on interrupt or termination signals, converting the status to <code>failed</code>. By default, a process that returns a nonzero value halts the pipeline, unlike in bash, where by default the pipeline would continue using an incomplete or failed result. This behavior can be overridden as desired with a single parameter.</p> <p> Dynamic recovery</p> <p>If a job is interrupted (with SIGINT or SIGTERM), either from a user or by a cluster resource manager, pypiper will set a <code>dynamic recovery</code> flag. The next time the run is started, it will automatically pick up where it left off. This makes pypiper pipelines <code>automatically pre-emption ready</code>, so they can be immediately deployed on servers where jobs may be pre-emptied.</p>"},{"location":"pypiper/ngstk_intro/","title":"NGSTk - Next Gen Sequencing Toolkit","text":"<p>Pypiper functions are generic; they simply accept command-line commands and run them. You could use this to produce a pipeline in any domain. To add to this, it's helpful to build convenience functions specific to your scientific domain. It's really easy to create your own library of python functions by creating a python package. Then, you just need to import your package in your pipeline script and make use of the common functions. We refer to this type of package as a \"toolkit\".</p> <p>Pypiper includes a built-in toolkit called NGSTk (next-generation sequencing toolkit). NGSTk simply provides some convenient helper functions to create common shell commands, like converting from file formats (e.g. <code>bam_to_fastq()</code>), merging files (e.g. <code>merge_bams()</code>), counting reads, etc. These make it faster to design bioinformatics pipelines in Pypiper, but are entirely optional.</p> <p>Here's how to use <code>NGSTk</code>:</p> <pre><code>import pypiper\npm = pypiper.PipelineManager(..., args = args)\n\n# Create a ngstk object (pass the PipelineManager as an argument)\nngstk = pypiper.NGSTk(pm = pm)\n\n# Now you use use ngstk functions\ncmd = ngstk.index_bam(\"sample.bam\")\npm.run(cmd, target=\"sample.bam\")\n</code></pre> <p>A complete list of functions is in the API or in the source code for NGSTk.</p>"},{"location":"pypiper/outputs/","title":"Outputs explained","text":"<p>Assume you are using a pypiper pipeline named <code>PIPE</code> ( it passes <code>name=\"PIPE\"</code> to the PipelineManager constructor). By default, your <code>PipelineManager</code> will produce the following outputs automatically (in addition to any output created by the actual pipeline commands you run):</p> <ul> <li> <p>PIPE_log.md     The log starts with a bunch of useful information about your run: a starting timestamp, version numbers of the pipeline and pypiper, a declaration of all arguments passed to the pipeline, the compute host, etc. Then, all output sent to screen is automatically logged to this file, providing a complete record of your run.</p> </li> <li> <p>PIPE_status.flag     As the pipeline runs, it produces a flag in the output directory, which can be either <code>PIPE_running.flag</code>, <code>PIPE_failed.flag</code>, or <code>PIPE_completed.flag</code>. These flags make it easy to assess the current state of running pipelines for individual samples, and for many samples in a project simultaneously.</p> </li> <li> <p>stats.yaml     Any results reported by the pipeline are saved as key-value pairs in this file, for easy parsing.</p> </li> <li> <p>PIPE_profile.md     A profile log file that provides, for every process run by the pipeline, 3 items: 1) the process name; 2) the clock time taken by the process; and 3) the memory high water mark used by the process. This file makes it easy to profile pipelines for memory and time resources.</p> </li> <li> <p>PIPE_commands.md     Pypiper produces a log file containing all the commands run by the pipeline, verbatim. These are also included in the main log.</p> </li> </ul> <p>Multiple pipelines can easily be run on the same sample, using the same output folder (and possibly sharing intermediate files), as the result outputs will be identifiable by the <code>PIPE_</code> identifier.</p> <p>These files are markdown making it easy to read either in text format, or to quickly convert to a pretty format like HTML.</p>"},{"location":"pypiper/philosophy/","title":"Pypiper's development philosophy","text":""},{"location":"pypiper/philosophy/#who-should-use-pypiper","title":"Who should use Pypiper?","text":"<p>The target audience for pypiper is an individual who wants to build a basic pipeline, but wants to do better job than just writing a shell script, without learning a new language or system. Many bioinformatics pipelines are simple shell scripts that piece together commands, because that seems the most accessible. Although there has been an explosion of more feature-rich pipeline development frameworks, these often require substantial training and investment to write a pipeline that could be more quickly written as a shell script. Pipelines built using a framework are also harder to understand for users unfamiliar with the framework, and require more experience to develop and modify.  Pypiper tries to give 80% of the benefits of a professional-scale pipelining system while requiring very little additional effort.</p> <p>If you have a shell script that would benefit from a layer of \"handling code\", Pypiper helps you convert that set of shell commands into a production-scale workflow, automatically handling the annoying details (restartablilty, file integrity, logging) to make your pipeline robust and restartable.</p> <p>Pypiper's strength is its simplicity. If all you want is a shell-like script, but now with the power of python, some built-in benefits, and syntactic sugar, then Pypiper is for you.</p>"},{"location":"pypiper/philosophy/#what-pypiper-does-not-do","title":"What Pypiper does NOT do","text":"<p>Pypiper tries to exploit the Pareto principle -- you'll get 80% of the features with only 20% of the work of other pipeline management systems. So, there are a few things Pypiper deliberately doesn't do:</p> <ul> <li> <p>Task dependencies. Pypiper runs sequential pipelines. We view this as an   advantage because it makes the pipeline easier to write, easier to understand,   easier to modify, and easier to debug -- critical things for pipelines that   are still under active development (which is most pipelines in bioinformatics). For   developmental pipelines, the complexity introduced by task dependencies is not   worth the minimal benefit -- read this post on parallelism in   bioinformatics   for an explanation.</p> </li> <li> <p>Cluster submission. Pypiper pipelines are scripts. You can run them on   whatever computing resources you have. We have divided cluster resource   management into a separate project called   looper. Pypiper builds individual,   single-sample pipelines that can be run one sample at a time.   Looper then processes groups of samples,   submitting appropriate pipelines to a cluster or server. The two projects are   independent and can be used separately, keeping things simple and modular.</p> </li> </ul>"},{"location":"pypiper/philosophy/#yet-another-pipeline-system","title":"Yet another pipeline system?","text":"<p>As I began to put together production-scale pipelines, I found a lot of relevant pipelining systems, but was universally disappointed. For my needs, they were all overly complex. I wanted something simple enough to quickly write and maintain a pipeline without having to learn a lot of new functions and conventions, but robust enough to handle requirements like restartability and memory usage monitoring. Everything related was either a pre-packaged pipeline for a defined purpose, or a heavy-duty development environment that was overkill for a simple pipeline. Both of these seemed to be targeted toward ultra- efficient uses, and neither fit my needs: I had a set of commands already in mind -- I just needed a wrapper that could take that code and make it automatically restartable, logged, robust to crashing, easy to debug, and so forth.</p> <p>Pypiper has evolved over the years and gained lots of cool new features. But its core principal has remained the same: simplicity. A pypiper pipeline can be nothing more than a familiar python script that strings together a few shell commands.</p>"},{"location":"pypiper/pipestat/","title":"Pipestat","text":"<p>Starting with pypiper v0.13.0 pipestat is the recommended way of reporting pipeline statistics. You can browse the pipestat documentation to learn more about it, but briefly pipestat is a tool that standardizes reporting of pipeline results. It provides 1) a standard specification for how pipeline outputs should be stored; and 2) an implementation to easily write results to that format from within Python or from the command line.</p>"},{"location":"pypiper/pipestat/#advancements","title":"Advancements","text":"<p>There are a multiple advantages of using pipestat instead of the current pipeline results reporting system:</p> <ol> <li>Database results storage: the results can be stored either in a database or a YAML-formatted results file. This way a pypiper pipeline running in an emphemeral compute environment can report the results to the database and exit. No need to sync the results with a central results storage.</li> <li>Strict and clear results definition: all the results that can be reported by a pipeline run must be pre-defined in a pipestat results schema that in a simplest case just indicates the result's type. This presents pipestat clients with the possibility to reliably gather all the possible results and related metadata.</li> <li>On-the-fly results validation: the schema is used to validate and/or convert the reported result to a strictly determined type, which makes the connection of pypiper with downstream pipeline results processing software seamless.</li> <li>Unified, pipeline-agnostic results interface: other pipelines, possibly created with different pipeline frameworks, can read and write results via Python API or command line interface. This feature significantly incerases your pipeline interoperability.</li> </ol>"},{"location":"pypiper/pipestat/#setup","title":"Setup","text":"<p>In order to start reporting results with pipestat in your pipeline all you need to do is define a pipestat results schema:</p> <pre><code>my_int_result:\n  type: integer\n  description: \"This is my first result\"\nmy_str_result:\n  type: string\n</code></pre> <p>And in the simplest case... that's it! Now you can use <code>pipestat</code> property of the <code>PipelineManager</code> object to report/retrieve results.</p> <p>Pypiper by default will use a YAML-formatted file to store the reported results in the selected <code>outfolder</code> and will look for <code>pipestat_results_schema.yaml</code> file in the pipeline Python script directory.</p>"},{"location":"pypiper/pipestat/#advanced-features","title":"Advanced features","text":"<p>Pypiper-pipestat integration really shines when more advanced features are used. Here's how to set them up.</p>"},{"location":"pypiper/pipestat/#configure-custom-pipestat-options","title":"Configure custom pipestat options","text":"<p>You can configure pipestat by passing arguments with custom values to <code>pypiper.PipelineManager</code> constructor:</p> <pre><code>pm = pypiper.PipelineManager(\n  ...,\n  pipestat_schema=\"custom_results_schema.yaml\",\n  pipestat_results_file=\"custom_results_file.yaml\",\n  pipestat_sample_name=\"my_record\",\n  pipestat_project_name=\"my_namespace\",\n  pipestat_config=\"custom_pipestat_config.yaml\",\n) \n</code></pre>"},{"location":"pypiper/pipestat/#use-a-database-to-store-reported-results","title":"Use a database to store reported results","text":"<p>In order to establish a database connection pipestat requires few pieces of information, which must be provided in a pipestat configuration file passed to the <code>PipelineManager</code> constructor.</p> <p>This is an example of such a file:</p> <pre><code>database:\n  name: pypiper # database name\n  user: pypiper # database user name\n  password: pypiper # database password\n  host: localhost # database host address\n  port: 5433 # port the database is running on\n  dialect: postgresql # type of the database \n  driver: psycopg2 # driver to use to communicate\n</code></pre> <p>For reference, here is a Docker command that would run a PostgreSQL instance that could be used to store the pipeline results when configured with with the configuration file above:</p> <pre><code>docker volume create postgres-data\n\ndocker run -d --name pypiper-postgres \\\n-p 5432:5433 -e POSTGRES_PASSWORD=pypiper \\\n-e POSTGRES_USER=pypiper -e POSTGRES_DB=pypiper \\\n-v postgres-data:/var/lib/postgresql/data postgres\n</code></pre>"},{"location":"pypiper/pipestat/#highlight-results","title":"Highlight results","text":"<p>The pipestat results schema can include any number of additional attributes for results. An example of that is results highlighting. </p> <p>When a <code>highlight: true</code> attribute is included attribute under result identifier in the schema file the highlighted results can be later retrieved by pipestat clients via <code>PipelineManager.pipestat.highlighted_results</code> property, which simply returns a list of result identifiers. to be presented in a special way.</p>"},{"location":"pypiper/pipestat/#usage","title":"Usage","text":"<p>Since a pipeline run-specific <code>PipestatManager</code> instance is attached to the <code>PipelineManager</code> object all the public pipestat API can be used. Please refer to the pipestat API documentation to read about all the currently available features.</p> <p>Here we present the most commonly used features:</p> <ul> <li>results reporting</li> </ul> <p>report a result, convert to schema-defined type and overwrite previously reported result</p> <pre><code>results = {\n  \"my_int_result\": 10,\n  \"my_str_result\": \"test\"\n}\npm.pipestat.report(\n  values=results,\n  strict_type=True,\n  force_overwrite=True\n)\n</code></pre> <ul> <li>results retrieval</li> </ul> <pre><code>pm.pipestat.retrieve(result_identifier=\"my_int_result\")\n</code></pre> <ul> <li>results schema exploration</li> </ul> <pre><code>pm.pipestat.schema\n</code></pre> <ul> <li>exploration of canonical jsonschema representation of result schemas</li> </ul> <pre><code>pm.pipestat.result_schemas\n</code></pre>"},{"location":"pypiper/report/","title":"Reporting statistics","text":"<p>One of the most useful features of pypiper is the <code>report_result</code> function. This function provides a way to record small-scale results, like summary statistics. It standardizes the output so that universal tools can be built to process all the pipeline results from any pipeline, because the results are all reported in the same way.</p> <p>When you call <code>pm.report_result(key, value)</code>, pypiper simply writes the key-value pair to a <code>tsv</code> file (<code>stats.tsv</code>) in the pipeline output folder. These <code>stats.tsv</code> files can then later be read and aggregated systematically by other tools, such as <code>looper summarize</code>.</p>"},{"location":"pypiper/report/#reporting-objects","title":"Reporting objects","text":"<p>Note: Reporting objects will be deprecated in a future release. It is recommended to use <code>report_result</code>.</p> <p>Starting in version 0.8, pypiper now implements a second reporting function, <code>report_object</code>. This is analogous to the <code>report_result</code> function, but instead of reporting simple key-value pairs, it lets you record any produced file as an output. Most commonly, this is used to record figures (PDFs, PNGs, etc.) produced by the pipeline. It can also be used to report other files, like HTML files.</p> <p>Pypiper writes results to <code>objects.tsv</code>, which can then be aggregated for project-level summaries of plots and other pipeline result files.</p>"},{"location":"pypiper/report/#re-using-previously-reported-results","title":"Re-using previously reported results","text":"<p>We frequently want to use the <code>report_result</code> capability in <code>follow</code> functions. It's a convenient place to do something like count or assess the result of a long-running command, and then report some summary statistic on it. One potential hangup with this strategy is dealing with secondary results after a pipeline is interrupted and restarted. By secondary result, I mean one that requires knowing the value of an earlier result. For example, if you want to compute the percentage of reads that aligned, you need to first know the total reads -- but what if your pipeline got interrupted and calculation of total reads happened in an earlier pipeline run?</p> <p>To solve this issue, Pypiper has a neat function called <code>get_stat</code> that lets you retrieve any value you've reported with <code>report_result</code> so you could use it to calculate statistics elsewhere in the pipeline. It will retrieve this either from memory, if the calculation of that result happened during the current pipeline run, or from the <code>stats.tsv</code> file, if the result was reported by an earlier run (or even another pipeline). So you could, in theory, calculate statistics based on results across pipelines.</p> <p>An example for how to use this is how we handle calculating the alignment rate in an NGS pipeline:</p> <pre><code>x = myngstk.count_mapped_reads(bamfile, args.paired_end)\npm.report_result(\"Aligned_reads\", x)\nrr = float(pm.get_stat(\"Raw_reads\"))\npm.report_result(\"Alignment_rate\", round((rr * 100 / float(x), 3))\n</code></pre> <p>Here, we use <code>get_stat</code> to grab a result that we reported previously (with <code>report_result</code>), when we counted the number of <code>Raw_reads</code> (earlier in the pipeline). We need this after the alignment to calculate the alignment rate. Later, now that we've reported <code>Alignment_rate</code>, you could harvest this stat again for use with <code>pm.get_stat(\"Alignment_rate\")</code>. This is useful because you could put this block of code in a <code>follow</code> statement so it may not be executed, but you can still grab a reported result like this even if the execution happened outside of the current pipeline run; you'd only have to do the calculation once.</p>"},{"location":"pypiper/support/","title":"Support","text":"<p>If you find a bug or want request a feature, open an issue at https://github.com/databio/pypiper/issues.</p>"},{"location":"pypiper/code/basic-pipeline/","title":"Your first basic pipeline","text":""},{"location":"pypiper/code/basic-pipeline/#the-basic-functions","title":"The basic functions","text":"<p>Pypiper is simple but powerful. Your pipeline is a python script, let's call it <code>pipeline.py</code>. You really only need to know about 3 commands to get started:</p> <ul> <li><code>PipelineManager()</code>: PipelineManager constructor, initializes the pipeline.</li> <li><code>PipelineManager.run()</code>: The primary workhorse function; runs a command.</li> <li><code>PipelineManager.stop_pipeline()</code>: Terminate the pipeline.</li> </ul> <p>That's all you need to create a powerful pipeline. You can find in-depth reference documentation for each method in the API. In particular, most of Pypiper\u2019s power comes from the <code>run</code> method, which has a series of options outlined in dedicated documentation on the run method.</p> <p>To write your first basic pipeline, first <code>import pypiper</code>, then specify an output folder and create a new <code>PipelineManager</code> object:</p> <pre><code>#!/usr/bin/env python\nimport pypiper, os\noutfolder = \"pipeline_output/\"  # Choose a folder for your results\npm = pypiper.PipelineManager(name=\"my_pipeline\", outfolder=outfolder)\n</code></pre> <p>Creating the <code>PipelineManger</code> object creates your <code>outfolder</code> and places a flag called <code>my_pipeline_running.flag</code> in the folder. It also initializes the log file (<code>my_pipeline_log.md</code>) with statistics such as time of starting, compute node, software versions, command-line parameters, etc.</p> <p>Now, the workhorse of <code>PipelineManager</code> is the <code>run()</code> function. Essentially, you create a shell command as a string in python, and then pass it and its target (a file it creates) to <code>run()</code>. The target is the final output file created by your command. Let's use the built-in <code>shuf</code> command to create some random numbers and put them in a file called <code>outfile.txt</code>:</p> <pre><code># our command will produce this output file\ntarget = os.path.join(outfolder, \"outfile.txt\")\ncommand = \"shuf -i 1-500000000 -n 10000000 &gt; \" + target\npm.run(command, target)\n</code></pre> <p>The command (<code>command</code>) is the only required argument to <code>run()</code>. You can leave <code>target</code> empty (pass <code>None</code>). If you do specify a target, the command will only be run if the target file does not already exist. If you do not specify a target, the command will be run every time the pipeline is run. </p> <p>Now string together whatever commands your pipeline requires! At the end, terminate the pipeline so it gets flagged as successfully completed:</p> <pre><code>pm.stop_pipeline()\n</code></pre> <p>That's it! By running commands through <code>run()</code> instead of directly in bash, you get a robust, logged, restartable pipeline manager for free!</p>"},{"location":"pypiper/code/basic-pipeline/#advanced-functions","title":"Advanced functions","text":"<p>Once you've mastered the basics, add in a few other commands that make debugging and development easier:</p> <ul> <li><code>timestamp()</code>: Print message, time, and time elapsed, perhaps creating checkpoint.</li> <li><code>clean_add()</code>: Add files (or regexs) to a cleanup list, to delete when this pipeline completes successfully.</li> </ul>"},{"location":"pypiper/code/basic-pipeline/#reporting-results","title":"Reporting results","text":"<p><code>Pypiper</code> also has a really nice system for collecting and aggregating result statistics and pipeline outputs. </p> <ul> <li><code>report_result()</code>: Writes a line in the <code>stats.tsv</code> file with a key-value pair of some pipeline result.</li> <li><code>report_object()</code>: Writes a line in the <code>objects.tsv</code> file with a key-value pair pointing to any file (such as an image, HTML report, or other) created by the pipeline.</li> <li><code>get_stat()</code>: Returns a stat that was previously reported.</li> </ul>"},{"location":"pypiper/code/basic-pipeline/#putting-them-together-in-basicpy","title":"Putting them together in <code>basic.py</code>","text":"<p>Now, download basic.py and run it with <code>python basic.py</code> (or, better yet, make it executable (<code>chmod 755 basic.py</code>) and then run it directly with <code>./basic.py</code>). This example is a documented vignette; so just read it and run it to get an idea of how things work. Here are the contents of <code>basic.py</code>:</p> <pre><code># %load ../example_pipelines/basic.py\n#!/usr/bin/env python\n\n\"\"\"Getting Started: A simple sample pipeline built using pypiper.\"\"\"\n\n# This is a runnable example. You can run it to see what the output\n# looks like.\n\n# First, make sure you can import the pypiper package\n\nimport os\nimport pypiper\n\n# Create a PipelineManager instance (don't forget to name it!)\n# This starts the pipeline.\n\npm = pypiper.PipelineManager(name=\"BASIC\",\n    outfolder=\"pipeline_output/\")\n\n# Now just build shell command strings, and use the run function\n# to execute them in order. run needs 2 things: a command, and the\n# target file you are creating.\n\n# First, generate some random data\n\n# specify target file:\ntgt = \"pipeline_output/test.out\"\n\n# build the command\ncmd = \"shuf -i 1-500000000 -n 10000000 &gt; \" + tgt\n\n# and run with run().\npm.run(cmd, target=tgt)\n\n# Now copy the data into a new file.\n# first specify target file and build command:\ntgt = \"pipeline_output/copied.out\"\ncmd = \"cp pipeline_output/test.out \" + tgt\npm.run(cmd, target=tgt)\n\n# You can also string multiple commands together, which will execute\n# in order as a group to create the final target.\ncmd1 = \"sleep 5\"\ncmd2 = \"touch pipeline_output/touched.out\"\npm.run([cmd1, cmd2], target=\"pipeline_output/touched.out\")\n\n# A command without a target will run every time.\n# Find the biggest line\ncmd = \"awk 'n &lt; $0 {n=$0} END{print n}' pipeline_output/test.out\"\npm.run(cmd, \"lock.max\")\n\n# Use checkprint() to get the results of a command, and then use\n# report_result() to print and log key-value pairs in the stats file:\nlast_entry = pm.checkprint(\"tail -n 1 pipeline_output/copied.out\")\npm.report_result(\"last_entry\", last_entry)\n\n\n# Now, stop the pipeline to complete gracefully.\npm.stop_pipeline()\n\n# Observe your outputs in the pipeline_output folder \n# to see what you've created.\n</code></pre>"},{"location":"pypiper/code/basic-pipeline/#building-a-pipeline-to-count-number-of-reads","title":"Building a pipeline to count number of reads","text":"<p>Here we have a more advanced bioinformatics pipeline that adds some new concepts. This is a simple script that takes an input file and returns the file size and the number of sequencing reads in that file. This example uses a function from from the built-in :doc:<code>NGSTk toolkit &lt;ngstk&gt;</code>. In particular, this toolkit contains a few handy functions that make it easy for a pipeline to accept inputs of various types. So, this pipeline can count the number of reads from files in <code>BAM</code> format, or <code>fastq</code> format, or <code>fastq.gz</code> format. You can also use the same functions from NGSTk to develop a pipeline to do more complicated things, and handle input of any of these types.</p> <p>First, grab this pipeline. Download count_reads.py, make it executable (<code>chmod 755 count_reads.py</code>), and then run it with <code>./count_reads.py</code>). </p> <p>You can grab a few small data files in the microtest repository. Run a few of these files like this:</p> <pre><code>./count_reads.py -I ~/code/microtest/data/rrbs_PE_R1.fastq.gz -O $HOME -S sample1\n./count_reads.py -I ~/code/microtest/data/rrbs_PE_fq_R1.fastq -O $HOME -S sample2\n./count_reads.py -I ~/code/microtest/data/atac-seq_SE.bam -O $HOME -S sample3\n</code></pre> <p>This example is a documented vignette; so just read it and run it to get an idea of how things work.</p> <pre><code># %load ../example_pipelines/count_reads.py\n#!/usr/bin/env python\n\n\"\"\"\nCounts reads.\n\"\"\"\n\n__author__ = \"Nathan Sheffield\"\n__email__ = \"nathan@code.databio.org\"\n__license__ = \"GPL3\"\n__version__ = \"0.1\"\n\nfrom argparse import ArgumentParser\nimport os, re\nimport sys\nimport subprocess\nimport yaml\nimport pypiper\n\nparser = ArgumentParser(\n    description=\"A pipeline to count the number of reads and file size. Accepts\"\n    \" BAM, fastq, or fastq.gz files.\")\n\n# First, add standard arguments from Pypiper.\n# groups=\"pypiper\" will add all the arguments that pypiper uses,\n# and adding \"common\" adds arguments for --input and --sample--name\n# and \"output_parent\". You can read more about your options for standard\n# arguments in the pypiper docs (section \"command-line arguments\")\nparser = pypiper.add_pypiper_args(parser, groups=[\"pypiper\", \"common\", \"ngs\"],\n                                    args=[\"output-parent\", \"config\"],\n                                    required=['sample-name', 'output-parent'])\n\n# Add any pipeline-specific arguments if you like here.\n\nargs = parser.parse_args()\n\nif not args.input or not args.output_parent:\n    parser.print_help()\n    raise SystemExit\n\nif args.single_or_paired == \"paired\":\n    args.paired_end = True\nelse:\n    args.paired_end = False\n\n# args for `output_parent` and `sample_name` were added by the standard \n# `add_pypiper_args` function. \n# A good practice is to make an output folder for each sample, housed under\n# the parent output folder, like this:\noutfolder = os.path.abspath(os.path.join(args.output_parent, args.sample_name))\n\n# Create a PipelineManager object and start the pipeline\npm = pypiper.PipelineManager(name=\"count\",\n                             outfolder=outfolder, \n                             args=args)\n\n# NGSTk is a \"toolkit\" that comes with pypiper, providing some functions\n# for dealing with genome sequence data. You can read more about toolkits in the\n# documentation\n\n# Create a ngstk object\nngstk = pypiper.NGSTk(pm=pm)\n\nraw_folder = os.path.join(outfolder, \"raw/\")\nfastq_folder = os.path.join(outfolder, \"fastq/\")\n\n# Merge/Link sample input and Fastq conversion\n# These commands merge (if multiple) or link (if single) input files,\n# then convert (if necessary, for bam, fastq, or gz format) files to fastq.\n\n# We'll start with a timestamp that will provide a division for this section\n# in the log file\npm.timestamp(\"### Merge/link and fastq conversion: \")\n\n# Now we'll rely on 2 NGSTk functions that can handle inputs of various types\n# and convert these to fastq files.\n\nlocal_input_files = ngstk.merge_or_link(\n                        [args.input, args.input2],\n                        raw_folder,\n                        args.sample_name)\n\ncmd, out_fastq_pre, unaligned_fastq = ngstk.input_to_fastq(\n                                            local_input_files,\n                                            args.sample_name,\n                                            args.paired_end,\n                                            fastq_folder)\n\n\n# Now we'll use another NGSTk function to grab the file size from the input files\n#\npm.report_result(\"File_mb\", ngstk.get_file_size(local_input_files))\n\n\n# And then count the number of reads in the file\n\nn_input_files = len(filter(bool, local_input_files))\n\nraw_reads = sum([int(ngstk.count_reads(input_file, args.paired_end)) \n                for input_file in local_input_files]) / n_input_files\n\n# Finally, we use the report_result() function to print the output and \n# log the key-value pair in the standard stats.tsv file\npm.report_result(\"Raw_reads\", str(raw_reads))\n\n# Cleanup\npm.stop_pipeline()\n</code></pre>"},{"location":"pypiper/code/hello-world/","title":"Hello world","text":"<p>This brief tutorial will run your first basic pypiper pipeline to ensure you have everything set up correctly. </p> <p>Just run these 3 lines of code and you're running your first pypiper pipeline!</p>"},{"location":"pypiper/code/hello-world/#install-the-latest-version-of-pypiper","title":"Install the latest version of pypiper","text":"<pre><code>pip install --user piper\n</code></pre>"},{"location":"pypiper/code/hello-world/#download-hello_pypiperpy","title":"Download hello_pypiper.py","text":"<pre><code>wget https://raw.githubusercontent.com/databio/pypiper/master/example_pipelines/hello_pypiper.py\n</code></pre> <p>This is a basic pipeline. Here are the contents:</p> <pre><code>cat ../example_pipelines/hello_pypiper.py\n</code></pre> <pre><code>#!/usr/bin/env python\n\nimport pypiper\noutfolder = \"hello_pypiper_results\" # Choose a folder for your results\n\n# Create a PipelineManager, the workhorse of pypiper\npm = pypiper.PipelineManager(name=\"hello_pypiper\", outfolder=outfolder)\n\n# Timestamps to delineate pipeline sections are easy:\npm.timestamp(\"Hello!\")\n\n# Now build a command-line command however you like, and pass it to pm.run()\ntarget_file = \"hello_pypiper_results/output.txt\"\ncmd = \"echo 'Hello, Pypiper!' &gt; \" + target_file\npm.run(cmd, target_file)\n\npm.stop_pipeline()\n</code></pre>"},{"location":"pypiper/code/hello-world/#run-it","title":"Run it!","text":"<pre><code>python3 ../example_pipelines/hello_pypiper.py\n</code></pre> <pre><code>### [Pipeline run code and environment:]\n\n*              Command:  `../example_pipelines/hello_pypiper.py`\n*         Compute host:  nox\n*          Working dir:  /home/sheffien/code/pypiper/docs_jupyter\n*            Outfolder:  hello_pypiper_results/\n*  Pipeline started at:   (03-16 23:47:37) elapsed: 0.0 _TIME_\n\n### [Version log:]\n\n*       Python version:  3.6.7\n*          Pypiper dir:  `/home/sheffien/.local/lib/python3.6/site-packages/pypiper`\n*      Pypiper version:  0.9.5dev\n*         Pipeline dir:  `/home/sheffien/code/pypiper/example_pipelines`\n*     Pipeline version:  None\n*        Pipeline hash:  b'134e8c8f723da66697ab4f5b204315979b4e1042\\n'\n*      Pipeline branch:  b'* dev\\n'\n*        Pipeline date:  b'2019-03-16 11:41:56 -0400\\n'\n*        Pipeline diff:  b' 1 file changed, 16 insertions(+), 1 deletion(-)\\n'\n\n### [Arguments passed to pipeline:]\n\n\n----------------------------------------\n\n\nChanged status from initializing to running.\nNo config file\nHello! (03-16 23:47:37) elapsed: 0.0 _TIME_\n\nTarget to produce: `hello_pypiper_results/output.txt`\n\n\n&gt; `echo 'Hello, Pypiper!' &gt; hello_pypiper_results/output.txt`\n\n&lt;pre&gt;\n&lt;/pre&gt;\nProcess 128 returned: (0). Elapsed: 0:00:00. Peak memory: (Process: None; Pipeline: 0GB)\n\nChanged status from running to completed.\n\n&gt; `Time`    0:00:00 hello_pypiper   _RES_\n\n&gt; `Success` 03-16-23:47:37  hello_pypiper   _RES_\n\n##### [Epilogue:]\n*   Total elapsed time:  0:00:00\n*     Peak memory used:  0 GB\n* Pipeline completed at:  (03-16 23:47:37) elapsed: 0.0 _TIME_\n\nPypiper terminating spawned child process 114...(tee)\n</code></pre> <p>This output is printed to your screen and also recorded in a log file (called <code>hello_pypiper_log.md</code>). There are a few other outputs from the pipeline as well. All results are placed in a folder called <code>hello_pypiper_results</code>. Navigate to that folder to observe the output of the pipeline, which will include these files:</p> <ul> <li>hello_pypiper_commands.sh</li> <li>hello_pypiper_completed.flag</li> <li>hello_pypiper_log.md</li> <li>hello_pypiper_profile.tsv</li> <li>output.txt</li> <li>stats.tsv</li> </ul> <p>These files are explained in more detail in the reference section outputs explained. </p> <p>What's next? That depends on if you're interested in just running pypiper pipelines, or if you want to develop pypiper pipelines. The next sections are a series of HOW-TO articles that address each of these scenarios.</p>"},{"location":"pypiper/code/ngstk-api/","title":"NGSTk API","text":""},{"location":"pypiper/code/ngstk-api/#package-pypiper-documentation","title":"Package <code>pypiper</code> Documentation","text":""},{"location":"pypiper/code/ngstk-api/#class-ngstk","title":"Class <code>NGSTk</code>","text":"<p>Class to hold functions to build command strings used during pipeline runs. Object can be instantiated with a string of a path to a yaml <code>pipeline config file</code>. Since NGSTk inherits from <code>AttMapEcho</code>, the passed config file and its elements will be accessible through the NGSTk object as attributes under <code>config</code> (e.g. <code>NGSTk.tools.java</code>). In case no <code>config_file</code> argument is passed, all commands will be returned assuming the tool is in the user's $PATH.</p>"},{"location":"pypiper/code/ngstk-api/#parameters","title":"Parameters:","text":"<ul> <li><code>config_file</code> (<code>str</code>):  Path to pipeline yaml config file (optional).</li> <li><code>pm</code> (<code>pypiper.PipelineManager</code>):  A PipelineManager with which to associate this toolkit instance;that is, essentially a source from which to grab paths to tools, resources, etc.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#examples","title":"Examples:","text":"<pre><code>    from pypiper.ngstk import NGSTk as tk\n    tk = NGSTk()\n    tk.samtools_index(\"sample.bam\")\n    # returns: samtools index sample.bam\n\n    # Using a configuration file (custom executable location):\n    from pypiper.ngstk import NGSTk\n    tk = NGSTk(\"pipeline_config_file.yaml\")\n    tk.samtools_index(\"sample.bam\")\n    # returns: /home/.local/samtools/bin/samtools index sample.bam\n</code></pre> <pre><code>def __init__(self, config_file=None, pm=None)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <pre><code>def add_track_to_hub(self, sample_name, track_url, track_hub, colour, five_prime='')\n</code></pre> <pre><code>def bam2fastq(self, input_bam, output_fastq, output_fastq2=None, unpaired_fastq=None)\n</code></pre> <p>Create command to convert BAM(s) to FASTQ(s).</p>"},{"location":"pypiper/code/ngstk-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>input_bam</code> (<code>str</code>):  Path to sequencing reads file to convert</li> <li><code>output_fastq</code> (``):  Path to FASTQ to write</li> <li><code>output_fastq2</code> (``):  Path to (R2) FASTQ to write</li> <li><code>unpaired_fastq</code> (``):  Path to unpaired FASTQ to write</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns","title":"Returns:","text":"<ul> <li><code>str</code>:  Command to convert BAM(s) to FASTQ(s)</li> </ul> <pre><code>def bam_conversions(self, bam_file, depth=True)\n</code></pre> <p>Sort and index bam files for later use.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>depth</code> (<code>bool</code>):  also calculate coverage over each position</li> </ul> <pre><code>def bam_to_bed(self, input_bam, output_bed)\n</code></pre> <pre><code>def bam_to_bigwig(self, input_bam, output_bigwig, genome_sizes, genome, tagmented=False, normalize=False, norm_factor=1000)\n</code></pre> <p>Convert a BAM file to a bigWig file.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>input_bam</code> (<code>str</code>):  path to BAM file to convert</li> <li><code>output_bigwig</code> (<code>str</code>):  path to which to write file in bigwig format</li> <li><code>genome_sizes</code> (<code>str</code>):  path to file with chromosome size information</li> <li><code>genome</code> (<code>str</code>):  name of genomic assembly</li> <li><code>tagmented</code> (<code>bool</code>):  flag related to read-generating protocol</li> <li><code>normalize</code> (<code>bool</code>):  whether to normalize coverage</li> <li><code>norm_factor</code> (<code>int</code>):  number of bases to use for normalization</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_1","title":"Returns:","text":"<ul> <li><code>list[str]</code>:  sequence of commands to execute</li> </ul> <pre><code>def bam_to_fastq(self, bam_file, out_fastq_pre, paired_end)\n</code></pre> <p>Build command to convert BAM file to FASTQ file(s) (R1/R2).</p>"},{"location":"pypiper/code/ngstk-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>bam_file</code> (<code>str</code>):  path to BAM file with sequencing reads</li> <li><code>out_fastq_pre</code> (<code>str</code>):  path prefix for output FASTQ file(s)</li> <li><code>paired_end</code> (<code>bool</code>):  whether the given file contains paired-endor single-end sequencing reads</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_2","title":"Returns:","text":"<ul> <li><code>str</code>:  file conversion command, ready to run</li> </ul> <pre><code>def bam_to_fastq_awk(self, bam_file, out_fastq_pre, paired_end, zipmode=False)\n</code></pre> <p>This converts bam file to fastq files, but using awk. As of 2016, this is much faster than the standard way of doing this using Picard, and also much faster than the bedtools implementation as well; however, it does no sanity checks and assumes the reads (for paired data) are all paired (no singletons), in the correct order.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>zipmode</code> (<code>bool</code>):  Should the output be zipped?</li> </ul> <pre><code>def bam_to_fastq_bedtools(self, bam_file, out_fastq_pre, paired_end)\n</code></pre> <p>Converts bam to fastq; A version using bedtools</p> <pre><code>def bowtie2_map(self, input_fastq1, output_bam, log, metrics, genome_index, max_insert, cpus, input_fastq2=None)\n</code></pre> <pre><code>def calc_frip(self, input_bam, input_bed, threads=4)\n</code></pre> <p>Calculate fraction of reads in peaks.</p> <p>A file of with a pool of sequencing reads and a file with peak call regions define the operation that will be performed. Thread count for samtools can be specified as well.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_6","title":"Parameters:","text":"<ul> <li><code>input_bam</code> (<code>str</code>):  sequencing reads file</li> <li><code>input_bed</code> (<code>str</code>):  file with called peak regions</li> <li><code>threads</code> (<code>int</code>):  number of threads samtools may use</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_3","title":"Returns:","text":"<ul> <li><code>float</code>:  fraction of reads in peaks defined in given peaks file</li> </ul> <pre><code>def calculate_frip(self, input_bam, input_bed, output, cpus=4)\n</code></pre> <pre><code>def center_peaks_on_motifs(self, peak_file, genome, window_width, motif_file, output_bed)\n</code></pre> <pre><code>def check_command(self, command)\n</code></pre> <p>Check if command can be called.</p> <pre><code>def check_fastq(self, input_files, output_files, paired_end)\n</code></pre> <p>Returns a follow sanity-check function to be run after a fastq conversion. Run following a command that will produce the fastq files.</p> <p>This function will make sure any input files have the same number of reads as the output files.</p> <pre><code>def check_trim(self, trimmed_fastq, paired_end, trimmed_fastq_R2=None, fastqc_folder=None)\n</code></pre> <p>Build function to evaluate read trimming, and optionally run fastqc.</p> <p>This is useful to construct an argument for the 'follow' parameter of a PipelineManager's 'run' method.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_7","title":"Parameters:","text":"<ul> <li><code>trimmed_fastq</code> (<code>str</code>):  Path to trimmed reads file.</li> <li><code>paired_end</code> (<code>bool</code>):  Whether the processing is being done withpaired-end sequencing data.</li> <li><code>trimmed_fastq_R2</code> (<code>str</code>):  Path to read 2 file for the paired-end case.</li> <li><code>fastqc_folder</code> (<code>str</code>):  Path to folder within which to place fastqcoutput files; if unspecified, fastqc will not be run.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_4","title":"Returns:","text":"<ul> <li><code>callable</code>:  Function to evaluate read trimming and possibly runfastqc.</li> </ul> <pre><code>def count_concordant(self, aligned_bam)\n</code></pre> <p>Count only reads that \"aligned concordantly exactly 1 time.\"</p>"},{"location":"pypiper/code/ngstk-api/#parameters_8","title":"Parameters:","text":"<ul> <li><code>aligned_bam</code> (<code>str</code>):  File for which to count mapped reads.</li> </ul> <pre><code>def count_fail_reads(self, file_name, paired_end)\n</code></pre> <p>Counts the number of reads that failed platform/vendor quality checks.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_9","title":"Parameters:","text":"<ul> <li><code>paired_end</code> (``):  This parameter is ignored; samtools automatically correctly responds dependingon the data in the bamfile. We leave the option here just for consistency, since all the other counting functions require the parameter. This makes it easier to swap counting functions during pipeline development.</li> </ul> <pre><code>def count_flag_reads(self, file_name, flag, paired_end)\n</code></pre> <p>Counts the number of reads with the specified flag.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_10","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name of reads file</li> <li><code>flag</code> (<code>str</code>):  sam flag value to be read</li> <li><code>paired_end</code> (<code>bool</code>):  This parameter is ignored; samtools automatically correctly responds dependingon the data in the bamfile. We leave the option here just for consistency, since all the other counting functions require the parameter. This makes it easier to swap counting functions during pipeline development.</li> </ul> <pre><code>def count_lines(self, file_name)\n</code></pre> <p>Uses the command-line utility wc to count the number of lines in a file. For MacOS, must strip leading whitespace from wc.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_11","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name of file whose lines are to be counted</li> </ul> <pre><code>def count_lines_zip(self, file_name)\n</code></pre> <p>Uses the command-line utility wc to count the number of lines in a file. For MacOS, must strip leading whitespace from wc. For compressed files.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_12","title":"Parameters:","text":"<ul> <li><code>file</code> (``):  file_name</li> </ul> <pre><code>def count_mapped_reads(self, file_name, paired_end)\n</code></pre> <p>Mapped_reads are not in fastq format, so this one doesn't need to accommodate fastq, and therefore, doesn't require a paired-end parameter because it only uses samtools view. Therefore, it's ok that it has a default parameter, since this is discarded.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_13","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  File for which to count mapped reads.</li> <li><code>paired_end</code> (<code>bool</code>):  This parameter is ignored; samtools automatically correctly responds dependingon the data in the bamfile. We leave the option here just for consistency, since all the other counting functions require the parameter. This makes it easier to swap counting functions during pipeline development.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_5","title":"Returns:","text":"<ul> <li><code>int</code>:  Either return code from samtools view command, or -1 to indicate an error state.</li> </ul> <pre><code>def count_multimapping_reads(self, file_name, paired_end)\n</code></pre> <p>Counts the number of reads that mapped to multiple locations. Warning: currently, if the alignment software includes the reads at multiple locations, this function will count those more than once. This function is for software that randomly assigns, but flags reads as multimappers.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_14","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name of reads file</li> <li><code>paired_end</code> (``):  This parameter is ignored; samtools automatically correctly responds dependingon the data in the bamfile. We leave the option here just for consistency, since all the other counting functions require the parameter. This makes it easier to swap counting functions during pipeline development.</li> </ul> <pre><code>def count_reads(self, file_name, paired_end)\n</code></pre> <p>Count reads in a file.</p> <p>Paired-end reads count as 2 in this function. For paired-end reads, this function assumes that the reads are split into 2 files, so it divides line count by 2 instead of 4. This will thus give an incorrect result if your paired-end fastq files are in only a single file (you must divide by 2 again).</p>"},{"location":"pypiper/code/ngstk-api/#parameters_15","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  Name/path of file whose reads are to be counted.</li> <li><code>paired_end</code> (<code>bool</code>):  Whether the file contains paired-end reads.</li> </ul> <pre><code>def count_unique_mapped_reads(self, file_name, paired_end)\n</code></pre> <p>For a bam or sam file with paired or or single-end reads, returns the number of mapped reads, counting each read only once, even if it appears mapped at multiple locations.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_16","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name of reads file</li> <li><code>paired_end</code> (<code>bool</code>):  True/False paired end data</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_6","title":"Returns:","text":"<ul> <li><code>int</code>:  Number of uniquely mapped reads.</li> </ul> <pre><code>def count_unique_reads(self, file_name, paired_end)\n</code></pre> <p>Sometimes alignment software puts multiple locations for a single read; if you just count those reads, you will get an inaccurate count. This is not the same as multimapping reads, which may or may not be actually duplicated in the bam file (depending on the alignment software). This function counts each read only once. This accounts for paired end or not for free because pairs have the same read name. In this function, a paired-end read would count as 2 reads.</p> <pre><code>def count_uniquelymapping_reads(self, file_name, paired_end)\n</code></pre> <p>Counts the number of reads that mapped to a unique position.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_17","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name of reads file</li> <li><code>paired_end</code> (<code>bool</code>):  This parameter is ignored.</li> </ul> <pre><code>def fastqc(self, file, output_dir)\n</code></pre> <p>Create command to run fastqc on a FASTQ file</p>"},{"location":"pypiper/code/ngstk-api/#parameters_18","title":"Parameters:","text":"<ul> <li><code>file</code> (<code>str</code>):  Path to file with sequencing reads</li> <li><code>output_dir</code> (<code>str</code>):  Path to folder in which to place output</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_7","title":"Returns:","text":"<ul> <li><code>str</code>:  Command with which to run fastqc</li> </ul> <pre><code>def fastqc_rename(self, input_bam, output_dir, sample_name)\n</code></pre> <p>Create pair of commands to run fastqc and organize files.</p> <p>The first command returned is the one that actually runs fastqc when it's executed; the second moves the output files to the output folder for the sample indicated.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_19","title":"Parameters:","text":"<ul> <li><code>input_bam</code> (<code>str</code>):  Path to file for which to run fastqc.</li> <li><code>output_dir</code> (<code>str</code>):  Path to folder in which fastqc output will bewritten, and within which the sample's output folder lives.</li> <li><code>sample_name</code> (<code>str</code>):  Sample name, which determines subfolder withinoutput_dir for the fastqc files.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_8","title":"Returns:","text":"<ul> <li><code>list[str]</code>:  Pair of commands, to run fastqc and then move the files totheir intended destination based on sample name.</li> </ul> <pre><code>def filter_peaks_mappability(self, peaks, alignability, filtered_peaks)\n</code></pre> <pre><code>def filter_reads(self, input_bam, output_bam, metrics_file, paired=False, cpus=16, Q=30)\n</code></pre> <p>Remove duplicates, filter for &gt;Q, remove multiple mapping reads. For paired-end reads, keep only proper pairs.</p> <pre><code>def genome_wide_coverage(self, input_bam, genome_windows, output)\n</code></pre> <pre><code>def get_chrs_from_bam(self, file_name)\n</code></pre> <p>Uses samtools to grab the chromosomes from the header that are contained in this bam file.</p> <pre><code>def get_file_size(self, filenames)\n</code></pre> <p>Get size of all files in string (space-separated) in megabytes (Mb).</p>"},{"location":"pypiper/code/ngstk-api/#parameters_20","title":"Parameters:","text":"<ul> <li><code>filenames</code> (<code>str</code>):  a space-separated string of filenames</li> </ul> <pre><code>def get_fragment_sizes(self, bam_file)\n</code></pre> <pre><code>def get_frip(self, sample)\n</code></pre> <p>Calculates the fraction of reads in peaks for a given sample.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_21","title":"Parameters:","text":"<ul> <li><code>sample</code> (<code>pipelines.Sample</code>):  Sample object with \"peaks\" attribute.</li> </ul> <pre><code>def get_input_ext(self, input_file)\n</code></pre> <p>Get the extension of the input_file. Assumes you're using either .bam or .fastq/.fq or .fastq.gz/.fq.gz.</p> <pre><code>def get_mitochondrial_reads(self, bam_file, output, cpus=4)\n</code></pre> <pre><code>def get_peak_number(self, sample)\n</code></pre> <p>Counts number of peaks from a sample's peak file.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_22","title":"Parameters:","text":"<ul> <li><code>sample</code> (<code>pipelines.Sample</code>):  Sample object with \"peaks\" attribute.</li> </ul> <pre><code>def get_read_type(self, bam_file, n=10)\n</code></pre> <p>Gets the read type (single, paired) and length of bam file.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_23","title":"Parameters:","text":"<ul> <li><code>bam_file</code> (<code>str</code>):  Bam file to determine read attributes.</li> <li><code>n</code> (<code>int</code>):  Number of lines to read from bam file.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_9","title":"Returns:","text":"<ul> <li><code>str, int</code>:  tuple of read type and read length</li> </ul> <pre><code>def homer_annotate_pPeaks(self, peak_file, genome, motif_file, output_bed)\n</code></pre> <pre><code>def homer_find_motifs(self, peak_file, genome, output_dir, size=150, length='8,10,12,14,16', n_motifs=12)\n</code></pre> <pre><code>def htseq_count(self, input_bam, gtf, output)\n</code></pre> <pre><code>def index_bam(self, input_bam)\n</code></pre> <pre><code>def input_to_fastq(self, input_file, sample_name, paired_end, fastq_folder, output_file=None, multiclass=False, zipmode=False)\n</code></pre> <p>Builds a command to convert input file to fastq, for various inputs.</p> <p>Takes either .bam, .fastq.gz, or .fastq input and returns commands that will create the .fastq file, regardless of input type. This is useful to made your pipeline easily accept any of these input types seamlessly, standardizing you to fastq which is still the most common format for adapter trimmers, etc. You can specify you want output either zipped or not. Commands will place the output fastq file in given <code>fastq_folder</code>.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_24","title":"Parameters:","text":"<ul> <li><code>input_file</code> (<code>str</code>):  filename of input you want to convert to fastq</li> <li><code>multiclass</code> (<code>bool</code>):  Are both read1 and read2 included in a singlefile? User should not need to set this; it will be inferred and used in recursive calls, based on number files, and the paired_end arg.</li> <li><code>zipmode</code> (<code>bool</code>):  Should the output be .fastq.gz? Otherwise, just fastq</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_10","title":"Returns:","text":"<ul> <li><code>str</code>:  A command (to be run with PipelineManager) that will ensureyour fastq file exists.</li> </ul> <pre><code>def kallisto(self, input_fastq, output_dir, output_bam, transcriptome_index, cpus, input_fastq2=None, size=180, b=200)\n</code></pre> <pre><code>def link_to_track_hub(self, track_hub_url, file_name, genome)\n</code></pre> <pre><code>def macs2_call_peaks(self, treatment_bams, output_dir, sample_name, genome, control_bams=None, broad=False, paired=False, pvalue=None, qvalue=None, include_significance=None)\n</code></pre> <p>Use MACS2 to call peaks.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_25","title":"Parameters:","text":"<ul> <li><code>treatment_bams</code> (<code>str | Iterable[str]</code>):  Paths to files with data toregard as treatment.</li> <li><code>output_dir</code> (<code>str</code>):  Path to output folder.</li> <li><code>sample_name</code> (<code>str</code>):  Name for the sample involved.</li> <li><code>genome</code> (<code>str</code>):  Name of the genome assembly to use.</li> <li><code>control_bams</code> (<code>str | Iterable[str]</code>):  Paths to files with data toregard as control</li> <li><code>broad</code> (<code>bool</code>):  Whether to do broad peak calling.</li> <li><code>paired</code> (<code>bool</code>):  Whether reads are paired-end</li> <li><code>pvalue</code> (<code>float | NoneType</code>):  Statistical significance measure topass as --pvalue to peak calling with MACS</li> <li><code>qvalue</code> (<code>float | NoneType</code>):  Statistical significance measure topass as --qvalue to peak calling with MACS</li> <li><code>include_significance</code> (<code>bool | NoneType</code>):  Whether to pass astatistical significance argument to peak calling with MACS; if omitted, this will be True if the peak calling is broad or if either p-value or q-value is specified; default significance specification is a p-value of 0.001 if a significance is to be specified but no value is provided for p-value or q-value.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_11","title":"Returns:","text":"<ul> <li><code>str</code>:  Command to run.</li> </ul> <pre><code>def macs2_call_peaks_atacseq(self, treatment_bam, output_dir, sample_name, genome)\n</code></pre> <pre><code>def macs2_plot_model(self, r_peak_model_file, sample_name, output_dir)\n</code></pre> <pre><code>def make_dir(self, path)\n</code></pre> <p>Forge path to directory, creating intermediates as needed.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_26","title":"Parameters:","text":"<ul> <li><code>path</code> (<code>str</code>):  Path to create.</li> </ul> <pre><code>def make_sure_path_exists(self, path)\n</code></pre> <p>Alias for make_dir</p> <pre><code>def mark_duplicates(self, aligned_file, out_file, metrics_file, remove_duplicates='True')\n</code></pre> <pre><code>def merge_bams(self, input_bams, merged_bam, in_sorted='TRUE', tmp_dir=None)\n</code></pre> <p>Combine multiple files into one.</p> <p>The tmp_dir parameter is important because on poorly configured systems, the default can sometimes fill up.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_27","title":"Parameters:","text":"<ul> <li><code>input_bams</code> (<code>Iterable[str]</code>):  Paths to files to combine</li> <li><code>merged_bam</code> (<code>str</code>):  Path to which to write combined result.</li> <li><code>in_sorted</code> (<code>bool | str</code>):  Whether the inputs are sorted</li> <li><code>tmp_dir</code> (<code>str</code>):  Path to temporary directory.</li> </ul> <pre><code>def merge_bams_samtools(self, input_bams, merged_bam)\n</code></pre> <pre><code>def merge_fastq(self, inputs, output, run=False, remove_inputs=False)\n</code></pre> <p>Merge FASTQ files (zipped or not) into one.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_28","title":"Parameters:","text":"<ul> <li><code>inputs</code> (<code>Iterable[str]</code>):  Collection of paths to files to merge.</li> <li><code>output</code> (<code>str</code>):  Path to single output file.</li> <li><code>run</code> (<code>bool</code>):  Whether to run the command.</li> <li><code>remove_inputs</code> (<code>bool</code>):  Whether to keep the original files.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_12","title":"Returns:","text":"<ul> <li><code>NoneType | str</code>:  Null if running the command, otherwise thecommand itself</li> </ul>"},{"location":"pypiper/code/ngstk-api/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>:  Raise ValueError if the call is such thatinputs are to be deleted but command is not run.</li> </ul> <pre><code>def merge_or_link(self, input_args, raw_folder, local_base='sample')\n</code></pre> <p>Standardizes various input possibilities by converting either .bam, .fastq, or .fastq.gz files into a local file; merging those if multiple files given.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_29","title":"Parameters:","text":"<ul> <li><code>input_args</code> (<code>list</code>):  This is a list of arguments, each one is aclass of inputs (which can in turn be a string or a list). Typically, input_args is a list with 2 elements: first a list of read1 files; second an (optional!) list of read2 files.</li> <li><code>raw_folder</code> (<code>str</code>):  Name/path of folder for the merge/link.</li> <li><code>local_base</code> (<code>str</code>):  Usually the sample name. This (plus fileextension) will be the name of the local file linked (or merged) by this function.</li> </ul> <pre><code>def move_file(self, old, new)\n</code></pre> <pre><code>def parse_bowtie_stats(self, stats_file)\n</code></pre> <p>Parses Bowtie2 stats file, returns series with values.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_30","title":"Parameters:","text":"<ul> <li><code>stats_file</code> (<code>str</code>):  Bowtie2 output file with alignment statistics.</li> </ul> <pre><code>def parse_duplicate_stats(self, stats_file)\n</code></pre> <p>Parses sambamba markdup output, returns series with values.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_31","title":"Parameters:","text":"<ul> <li><code>stats_file</code> (<code>str</code>):  sambamba output file with duplicate statistics.</li> </ul> <pre><code>def parse_qc(self, qc_file)\n</code></pre> <p>Parse phantompeakqualtools (spp) QC table and return quality metrics.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_32","title":"Parameters:","text":"<ul> <li><code>qc_file</code> (<code>str</code>):  Path to phantompeakqualtools output file, whichcontains sample quality measurements.</li> </ul> <pre><code>def picard_mark_duplicates(self, input_bam, output_bam, metrics_file, temp_dir='.')\n</code></pre> <pre><code>def plot_atacseq_insert_sizes(self, bam, plot, output_csv, max_insert=1500, smallest_insert=30)\n</code></pre> <p>Heavy inspiration from here: https://github.com/dbrg77/ATAC/blob/master/ATAC_seq_read_length_curve_fitting.ipynb</p> <pre><code>def preseq_coverage(self, bam_file, output_prefix)\n</code></pre> <pre><code>def preseq_curve(self, bam_file, output_prefix)\n</code></pre> <pre><code>def preseq_extrapolate(self, bam_file, output_prefix)\n</code></pre> <pre><code>def remove_file(self, file_name)\n</code></pre> <pre><code>def run_spp(self, input_bam, output, plot, cpus)\n</code></pre> <p>Run the SPP read peak analysis tool.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_33","title":"Parameters:","text":"<ul> <li><code>input_bam</code> (<code>str</code>):  Path to reads file</li> <li><code>output</code> (<code>str</code>):  Path to output file</li> <li><code>plot</code> (<code>str</code>):  Path to plot file</li> <li><code>cpus</code> (<code>int</code>):  Number of processors to use</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_13","title":"Returns:","text":"<ul> <li><code>str</code>:  Command with which to run SPP</li> </ul> <pre><code>def sam_conversions(self, sam_file, depth=True)\n</code></pre> <p>Convert sam files to bam files, then sort and index them for later use.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_34","title":"Parameters:","text":"<ul> <li><code>depth</code> (<code>bool</code>):  also calculate coverage over each position</li> </ul> <pre><code>def sambamba_remove_duplicates(self, input_bam, output_bam, cpus=16)\n</code></pre> <pre><code>def samtools_index(self, bam_file)\n</code></pre> <p>Index a bam file.</p> <pre><code>def samtools_view(self, file_name, param, postpend='')\n</code></pre> <p>Run samtools view, with flexible parameters and post-processing.</p> <p>This is used internally to implement the various count_reads functions.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_35","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  file_name</li> <li><code>param</code> (<code>str</code>):  String of parameters to pass to samtools view</li> <li><code>postpend</code> (<code>str</code>):  String to append to the samtools command;useful to add cut, sort, wc operations to the samtools view output.</li> </ul> <pre><code>def shift_reads(self, input_bam, genome, output_bam)\n</code></pre> <pre><code>def simple_frip(self, input_bam, input_bed, threads=4)\n</code></pre> <pre><code>def skewer(self, input_fastq1, output_prefix, output_fastq1, log, cpus, adapters, input_fastq2=None, output_fastq2=None)\n</code></pre> <p>Create commands with which to run skewer.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_36","title":"Parameters:","text":"<ul> <li><code>input_fastq1</code> (<code>str</code>):  Path to input (read 1) FASTQ file</li> <li><code>output_prefix</code> (<code>str</code>):  Prefix for output FASTQ file names</li> <li><code>output_fastq1</code> (<code>str</code>):  Path to (read 1) output FASTQ file</li> <li><code>log</code> (<code>str</code>):  Path to file to which to write logging information</li> <li><code>cpus</code> (<code>int | str</code>):  Number of processing cores to allow</li> <li><code>adapters</code> (<code>str</code>):  Path to file with sequencing adapters</li> <li><code>input_fastq2</code> (<code>str</code>):  Path to read 2 input FASTQ file</li> <li><code>output_fastq2</code> (<code>str</code>):  Path to read 2 output FASTQ file</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_14","title":"Returns:","text":"<ul> <li><code>list[str]</code>:  Sequence of commands to run to trim reads withskewer and rename files as desired.</li> </ul> <pre><code>def slurm_footer(self)\n</code></pre> <pre><code>def slurm_header(self, job_name, output, queue='shortq', n_tasks=1, time='10:00:00', cpus_per_task=8, mem_per_cpu=2000, nodes=1, user_mail='', mail_type='end')\n</code></pre> <pre><code>def slurm_submit_job(self, job_file)\n</code></pre> <pre><code>def sort_index_bam(self, input_bam, output_bam)\n</code></pre> <pre><code>def spp_call_peaks(self, treatment_bam, control_bam, treatment_name, control_name, output_dir, broad, cpus, qvalue=None)\n</code></pre> <p>Build command for R script to call peaks with SPP.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_37","title":"Parameters:","text":"<ul> <li><code>treatment_bam</code> (<code>str</code>):  Path to file with data for treatment sample.</li> <li><code>control_bam</code> (<code>str</code>):  Path to file with data for control sample.</li> <li><code>treatment_name</code> (<code>str</code>):  Name for the treatment sample.</li> <li><code>control_name</code> (<code>str</code>):  Name for the control sample.</li> <li><code>output_dir</code> (<code>str</code>):  Path to folder for output.</li> <li><code>broad</code> (<code>str | bool</code>):  Whether to specify broad peak calling mode.</li> <li><code>cpus</code> (<code>int</code>):  Number of cores the script may use.</li> <li><code>qvalue</code> (<code>float</code>):  FDR, as decimal value</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_15","title":"Returns:","text":"<ul> <li><code>str</code>:  Command to run.</li> </ul> <pre><code>def topHat_map(self, input_fastq, output_dir, genome, transcriptome, cpus)\n</code></pre> <pre><code>def trimmomatic(self, input_fastq1, output_fastq1, cpus, adapters, log, input_fastq2=None, output_fastq1_unpaired=None, output_fastq2=None, output_fastq2_unpaired=None)\n</code></pre> <pre><code>def validate_bam(self, input_bam)\n</code></pre> <p>Wrapper for Picard's ValidateSamFile.</p>"},{"location":"pypiper/code/ngstk-api/#parameters_38","title":"Parameters:","text":"<ul> <li><code>input_bam</code> (<code>str</code>):  Path to file to validate.</li> </ul>"},{"location":"pypiper/code/ngstk-api/#returns_16","title":"Returns:","text":"<ul> <li><code>str</code>:  Command to run for the validation.</li> </ul> <pre><code>def zinba_call_peaks(self, treatment_bed, control_bed, cpus, tagmented=False)\n</code></pre> <pre><code>def ziptool(self)\n</code></pre> <p>Returns the command to use for compressing/decompressing.</p>"},{"location":"pypiper/code/ngstk-api/#returns_17","title":"Returns:","text":"<ul> <li><code>str</code>:  Either 'gzip' or 'pigz' if installed and multiple cores</li> </ul> <p>Version Information: <code>pypiper</code> v0.14.1, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"pypiper/code/ngstk/","title":"Ngstk","text":""},{"location":"pypiper/code/ngstk/#package-ngstk-documentation","title":"Package <code>ngstk</code> Documentation","text":"<p>Package exports </p>"},{"location":"pypiper/code/ngstk/#class-unsupportedfiletypeexception","title":"Class <code>UnsupportedFiletypeException</code>","text":"<p>Restrict domain of file types.</p> <pre><code>def count_fail_reads(file_name, paired_end, prog_path)\n</code></pre> <p>Count the number of reads that failed platform/vendor quality checks.</p>"},{"location":"pypiper/code/ngstk/#parameters","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name/path to file to examine</li> <li><code>paired_end</code> (``):  this parameter is ignored; samtools automaticallycorrectly responds depending on the data in the bamfile; we leave the option here just for consistency, since all the other counting functions require the parameter; this makes it easier to swap counting functions during pipeline development.</li> <li><code>prog_path</code> (<code>str</code>):  path to main program/tool to use for the counting</li> </ul>"},{"location":"pypiper/code/ngstk/#returns","title":"Returns:","text":"<ul> <li><code>int</code>:  count of failed reads</li> </ul> <pre><code>def count_flag_reads(file_name, flag, paired_end, prog_path)\n</code></pre> <p>Counts the number of reads with the specified flag.</p>"},{"location":"pypiper/code/ngstk/#parameters_1","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name/path to file to examine</li> <li><code>flag</code> (<code>str int |</code>):  SAM flag value to be read</li> <li><code>paired_end</code> (``):  this parameter is ignored; samtools automaticallycorrectly responds depending on the data in the bamfile; we leave the option here just for consistency, since all the other counting functions require the parameter; this makes it easier to swap counting functions during pipeline development.</li> <li><code>prog_path</code> (<code>str</code>):  path to main program/tool to use for the counting</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_1","title":"Returns:","text":"<ul> <li><code>str</code>:  terminal-like text output</li> </ul> <pre><code>def count_lines(file_name)\n</code></pre> <p>Uses the command-line utility wc to count the number of lines in a file.</p> <p>For MacOS, must strip leading whitespace from wc.</p>"},{"location":"pypiper/code/ngstk/#parameters_2","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name of file whose lines are to be counted</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_2","title":"Returns:","text":"<ul> <li><code>str</code>:  terminal-like text output</li> </ul> <pre><code>def count_lines_zip(file_name)\n</code></pre> <p>Count number of lines in a zipped file.</p> <p>This function eses the command-line utility wc to count the number of lines in a file. For MacOS, strip leading whitespace from wc.</p>"},{"location":"pypiper/code/ngstk/#parameters_3","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  path to file in which to count lines</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_3","title":"Returns:","text":"<ul> <li><code>str</code>:  terminal-like text output</li> </ul> <pre><code>def count_reads(file_name, paired_end, prog_path)\n</code></pre> <p>Count reads in a file.</p> <p>Paired-end reads count as 2 in this function. For paired-end reads, this function assumes that the reads are split into 2 files, so it divides line count by 2 instead of 4. This will thus give an incorrect result if your paired-end fastq files are in only a single file (you must divide by 2 again).</p>"},{"location":"pypiper/code/ngstk/#parameters_4","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  Name/path of file whose reads are to be counted.</li> <li><code>paired_end</code> (<code>bool</code>):  Whether the file contains paired-end reads.</li> <li><code>prog_path</code> (<code>str</code>):  path to main program/tool to use for the counting</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_4","title":"Returns:","text":"<ul> <li><code>str</code>:  terminal-like text output (if input is SAM/BAM), or actualcount value (if input isn't SAM/BAM)</li> </ul> <pre><code>def get_file_size(filename)\n</code></pre> <p>Get size of all files in gigabytes (Gb).</p>"},{"location":"pypiper/code/ngstk/#parameters_5","title":"Parameters:","text":"<ul> <li><code>filename</code> (<code>str | collections.Iterable[str]</code>):  A space-separatedstring or list of space-separated strings of absolute file paths.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_5","title":"Returns:","text":"<ul> <li><code>float</code>:  size of file(s), in gigabytes.</li> </ul> <pre><code>def get_input_ext(input_file)\n</code></pre> <p>Get the extension of the input_file.</p> <p>This function assumes you're using .bam, .fastq/.fq, or .fastq.gz/.fq.gz.</p>"},{"location":"pypiper/code/ngstk/#parameters_6","title":"Parameters:","text":"<ul> <li><code>input_file</code> (<code>str</code>):  name/path of file for which to get extension</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_6","title":"Returns:","text":"<ul> <li><code>str</code>:  standardized extension</li> </ul>"},{"location":"pypiper/code/ngstk/#raises","title":"Raises:","text":"<ul> <li><code>ubiquerg.ngs.UnsupportedFiletypeException</code>:  if the given file nameor path has an extension that's not supported</li> </ul> <pre><code>def is_fastq(file_name)\n</code></pre> <p>Determine whether indicated file appears to be in FASTQ format.</p>"},{"location":"pypiper/code/ngstk/#parameters_7","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  Name/path of file to check as FASTQ.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_7","title":"Returns:","text":"<ul> <li><code>bool</code>:  Whether indicated file appears to be in FASTQ format, zippedor unzipped.</li> </ul> <pre><code>def is_gzipped_fastq(file_name)\n</code></pre> <p>Determine whether indicated file appears to be a gzipped FASTQ.</p>"},{"location":"pypiper/code/ngstk/#parameters_8","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  Name/path of file to check as gzipped FASTQ.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_8","title":"Returns:","text":"<ul> <li><code>bool</code>:  Whether indicated file appears to be in gzipped FASTQ format.</li> </ul> <pre><code>def is_sam_or_bam(file_name)\n</code></pre> <p>Determine whether a file appears to be in a SAM format.</p>"},{"location":"pypiper/code/ngstk/#parameters_9","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  Name/path of file to check as SAM-formatted.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_9","title":"Returns:","text":"<ul> <li><code>bool</code>:  Whether file appears to be SAM-formatted</li> </ul> <pre><code>def is_unzipped_fastq(file_name)\n</code></pre> <p>Determine whether indicated file appears to be an unzipped FASTQ.</p>"},{"location":"pypiper/code/ngstk/#parameters_10","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  Name/path of file to check as unzipped FASTQ.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_10","title":"Returns:","text":"<ul> <li><code>bool</code>:  Whether indicated file appears to be in unzipped FASTQ format.</li> </ul> <pre><code>def parse_ftype(input_file)\n</code></pre> <p>Checks determine filetype from extension.</p>"},{"location":"pypiper/code/ngstk/#parameters_11","title":"Parameters:","text":"<ul> <li><code>input_file</code> (<code>str</code>):  String to check.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_11","title":"Returns:","text":"<ul> <li><code>str</code>:  filetype (extension without dot prefix)</li> </ul>"},{"location":"pypiper/code/ngstk/#raises_1","title":"Raises:","text":"<ul> <li><code>UnsupportedFiletypeException</code>:  if file does not appear of asupported type</li> </ul> <pre><code>def peek_read_lengths_and_paired_counts_from_bam(bam, sample_size)\n</code></pre> <p>Counting read lengths and paired reads in a sample from a BAM.</p>"},{"location":"pypiper/code/ngstk/#parameters_12","title":"Parameters:","text":"<ul> <li><code>bam</code> (<code>str</code>):  path to BAM file to examine</li> <li><code>sample_size</code> (<code>int</code>):  number of reads to look at for estimation</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_12","title":"Returns:","text":"<ul> <li><code>defaultdict[int, int], int</code>:  read length observation counts, andnumber of paired reads observed</li> </ul>"},{"location":"pypiper/code/ngstk/#raises_2","title":"Raises:","text":"<ul> <li><code>OSError</code>: </li> </ul> <pre><code>def samtools_view(file_name, param, prog_path, postpend='')\n</code></pre> <p>Run samtools view, with flexible parameters and post-processing.</p> <p>This is used to implement the various read counting functions.</p>"},{"location":"pypiper/code/ngstk/#parameters_13","title":"Parameters:","text":"<ul> <li><code>file_name</code> (<code>str</code>):  name/path of reads tile to use</li> <li><code>param</code> (<code>str</code>):  String of parameters to pass to samtools view</li> <li><code>prog_path</code> (<code>str</code>):  path to the samtools program</li> <li><code>postpend</code> (<code>str</code>):  String to append to the samtools command;useful to add cut, sort, wc operations to the samtools view output.</li> </ul>"},{"location":"pypiper/code/ngstk/#returns_13","title":"Returns:","text":"<ul> <li><code>str</code>:  terminal-like text output</li> </ul> <p>Version Information: <code>ngstk</code> v0.0.1pre2, generated by <code>lucidoc</code> v0.4.3</p>"},{"location":"pypiper/code/python-api/","title":"Pypiper API","text":""},{"location":"pypiper/code/python-api/#package-pypiper-documentation","title":"Package <code>pypiper</code> Documentation","text":""},{"location":"pypiper/code/python-api/#class-pipelinemanager","title":"Class <code>PipelineManager</code>","text":"<p>Base class for instantiating a PipelineManager object, the main class of Pypiper.</p>"},{"location":"pypiper/code/python-api/#parameters","title":"Parameters:","text":"<ul> <li><code>name</code> (<code>str</code>):  Choose a name for your pipeline;it's used to name the output files, flags, etc.</li> <li><code>outfolder</code> (<code>str</code>):  Folder in which to store the results.</li> <li><code>args</code> (<code>argparse.Namespace</code>):  Optional args object from ArgumentParser;Pypiper will simply record these arguments from your script</li> <li><code>multi</code> (<code>bool</code>):  Enables running multiple pipelines in one scriptor for interactive use. It simply disables the tee of the output, so you won't get output logged to a file.</li> <li><code>dirty</code> (<code>bool</code>):  Overrides the pipeline's clean_add()manual parameters, to never clean up intermediate files automatically. Useful for debugging; all cleanup files are added to manual cleanup script.</li> <li><code>recover</code> (<code>bool</code>):  Specify recover mode, to overwrite lock files.If pypiper encounters a locked target, it will ignore the lock and recompute this step. Useful to restart a failed pipeline.</li> <li><code>new_start</code> (<code>bool</code>):  start over and run every command even if output exists</li> <li><code>force_follow</code> (<code>bool</code>):  Force run all follow functionseven if  the preceding command is not run. By default, following functions  are only run if the preceding command is run.</li> <li><code>cores</code> (<code>int</code>):  number of processors to use, default 1</li> <li><code>mem</code> (<code>str</code>):  amount of memory to use. Default units are megabytes unlessspecified using the suffix [K|M|G|T].\"</li> <li><code>config_file</code> (<code>str</code>):  path to pipeline configuration file, optional</li> <li><code>output_parent</code> (<code>str</code>):  path to folder in which output folder will live</li> <li><code>overwrite_checkpoints</code> (<code>bool</code>):  Whether to override the stage-skippinglogic provided by the checkpointing system. This is useful if the calls to this manager's run() method will be coming from a class that implements pypiper.Pipeline, as such a class will handle checkpointing logic automatically, and will set this to True to protect from a case in which a restart begins upstream of a stage for which a checkpoint file already exists, but that depends on the upstream stage and thus should be rerun if it's \"parent\" is rerun.</li> <li><code>pipestat_record_identifier</code> (<code>str</code>):  record_identifier to report results via pipestat</li> <li><code>pipestat_schema</code> (<code>str</code>):  output schema used by pipestat to report results</li> <li><code>pipestat_results_file</code> (<code>str</code>):  path to file backend for reporting results</li> <li><code>pipestat_config_file</code> (<code>str</code>):  path to pipestat configuration file</li> <li><code>pipestat_pipeline_type</code> (<code>str</code>):  Sample or Project level pipeline</li> <li><code>pipestat_result_formatter</code> (``):  function used to style reported results, defaults to result_formatter_markdown</li> </ul>"},{"location":"pypiper/code/python-api/#raises","title":"Raises:","text":"<ul> <li><code>TypeError</code>:  if start or stop point(s) are provided both directly andvia args namespace, or if both stopping types (exclusive/prospective and inclusive/retrospective) are provided.</li> </ul> <pre><code>def __init__(self, name, outfolder, version=None, args=None, multi=False, dirty=False, recover=False, new_start=False, force_follow=False, cores=1, mem='1000M', config_file=None, output_parent=None, overwrite_checkpoints=False, logger_kwargs=None, pipestat_record_identifier=None, pipestat_schema=None, pipestat_results_file=None, pipestat_config=None, pipestat_pipeline_type=None, pipestat_result_formatter=None, **kwargs)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <pre><code>def callprint(self, cmd, shell=None, lock_file=None, nofail=False, container=None)\n</code></pre> <p>Prints the command, and then executes it, then prints the memory use and return code of the command.</p> <p>Uses python's subprocess.Popen() to execute the given command. The shell argument is simply passed along to Popen(). You should use shell=False (default) where possible, because this enables memory profiling. You should use shell=True if you require shell functions like redirects (&gt;) or stars (*), but this will prevent the script from monitoring memory use. The pipes (|) will be used to split the command into subprocesses run within python, so the memory profiling is possible. cmd can also be a series (a dict object) of multiple commands, which will be run in succession.</p>"},{"location":"pypiper/code/python-api/#parameters_1","title":"Parameters:","text":"<ul> <li><code>cmd</code> (<code>str | Iterable[str]</code>):  Bash command(s) to be run.</li> <li><code>lock_file</code> (<code>str</code>):  a lock file name</li> <li><code>nofail</code> (<code>bool</code>):  FalseNofail can be used to implement non-essential parts of the pipeline; if these processes fail, they will not cause the pipeline to bail out.</li> <li><code>shell</code> (<code>bool</code>):  None (will tryto determine based on the command)</li> <li><code>container</code> (``):  Named Docker container in which to execute.</li> <li><code>container</code> (``):  str</li> </ul> <pre><code>def checkprint(self, cmd, shell=None, nofail=False)\n</code></pre> <p>Just like callprint, but checks output -- so you can get a variable in python corresponding to the return value of the command you call. This is equivalent to running subprocess.check_output() instead of subprocess.call().</p>"},{"location":"pypiper/code/python-api/#parameters_2","title":"Parameters:","text":"<ul> <li><code>cmd</code> (<code>str | Iterable[str]</code>):  Bash command(s) to be run.</li> <li><code>shell</code> (<code>bool | str</code>):  If command requires should be run in its own shell. Optional.Default: \"guess\" -- <code>run()</code> will try to guess if the command should be run in a shell (based on the presence of a pipe (|) or redirect (&gt;), To force a process to run as a direct subprocess, set <code>shell</code> to False; to force a shell, set True.</li> <li><code>nofail</code> (<code>bool</code>):  FalseNofail can be used to implement non-essential parts of the pipeline; if these processes fail, they will not cause the pipeline to bail out.</li> </ul>"},{"location":"pypiper/code/python-api/#returns","title":"Returns:","text":"<ul> <li><code>str</code>:  text output by the executed subprocess (check_output)</li> </ul> <pre><code>def clean_add(self, regex, conditional=False, manual=False)\n</code></pre> <p>Add files (or regexs) to a cleanup list, to delete when this pipeline completes successfully. When making a call with run that produces intermediate files that should be deleted after the pipeline completes, you flag these files for deletion with this command. Files added with clean_add will only be deleted upon success of the pipeline.</p>"},{"location":"pypiper/code/python-api/#parameters_3","title":"Parameters:","text":"<ul> <li><code>regex</code> (<code>str</code>):   A unix-style regular expression that matches files to delete(can also be a file name).</li> <li><code>conditional</code> (<code>bool</code>):  True means the files will only be deleted if no otherpipelines are currently running; otherwise they are added to a manual cleanup script called {pipeline_name}_cleanup.sh</li> <li><code>manual</code> (<code>bool</code>):  True means the files will just be added to a manual cleanup script.</li> </ul> <pre><code>def complete(self)\n</code></pre> <p>Stop a completely finished pipeline.</p> <pre><code>def critical(self, msg, *args, **kwargs)\n</code></pre> <pre><code>def debug(self, msg, *args, **kwargs)\n</code></pre> <pre><code>def error(self, msg, *args, **kwargs)\n</code></pre> <pre><code>def fail_pipeline(self, exc: Exception, dynamic_recover: bool=False)\n</code></pre> <p>If the pipeline does not complete, this function will stop the pipeline gracefully. It sets the status flag to failed and skips the normal success completion procedure.</p>"},{"location":"pypiper/code/python-api/#parameters_4","title":"Parameters:","text":"<ul> <li><code>exc</code> (<code>Exception</code>):  Exception to raise.</li> <li><code>dynamic_recover</code> (<code>bool</code>):  Whether to recover e.g. for job termination.</li> </ul> <pre><code>def fatal(self, msg, *args, **kwargs)\n</code></pre> <pre><code>def get_container(self, image, mounts)\n</code></pre> <pre><code>def get_elapsed_time(self)\n</code></pre> <p>Parse the pipeline profile file, collect the unique and last duplicated runtimes and sum them up. In case the profile is not found, an estimate is calculated (which is correct only in case the pipeline was not rerun)</p>"},{"location":"pypiper/code/python-api/#returns_1","title":"Returns:","text":"<ul> <li><code>int</code>:  sum of runtimes in seconds</li> </ul> <pre><code>def get_stat(self, key)\n</code></pre> <p>Returns a stat that was previously reported. This is necessary for reporting new stats that are derived from two stats, one of which may have been reported by an earlier run. For example, if you first use report_result to report (number of trimmed reads), and then in a later stage want to report alignment rate, then this second stat (alignment rate) will require knowing the first stat (number of trimmed reads); however, that may not have been calculated in the current pipeline run, so we must retrieve it from the stats.yaml output file. This command will retrieve such previously reported stats if they were not already calculated in the current pipeline run.</p>"},{"location":"pypiper/code/python-api/#parameters_5","title":"Parameters:","text":"<ul> <li><code>key</code> (``):  key of stat to retrieve</li> </ul> <pre><code>def halt(self, checkpoint=None, finished=False, raise_error=True)\n</code></pre> <p>Stop the pipeline before completion point.</p>"},{"location":"pypiper/code/python-api/#parameters_6","title":"Parameters:","text":"<ul> <li><code>checkpoint</code> (<code>str</code>):  Name of stage just reached or just completed.</li> <li><code>finished</code> (<code>bool</code>):  Whether the indicated stage was just finished(True), or just reached (False)</li> <li><code>raise_error</code> (<code>bool</code>):  Whether to raise an exception to trulyhalt execution.</li> </ul> <pre><code>def halted(self)\n</code></pre> <p>Is the managed pipeline in a paused/halted state?</p>"},{"location":"pypiper/code/python-api/#returns_2","title":"Returns:","text":"<ul> <li><code>bool</code>:  Whether the managed pipeline is in a paused/halted state.</li> </ul> <pre><code>def info(self, msg, *args, **kwargs)\n</code></pre> <pre><code>def make_sure_path_exists(path)\n</code></pre> <p>Creates all directories in a path if it does not exist.</p>"},{"location":"pypiper/code/python-api/#parameters_7","title":"Parameters:","text":"<ul> <li><code>path</code> (<code>str</code>):  Path to create.</li> </ul>"},{"location":"pypiper/code/python-api/#raises_1","title":"Raises:","text":"<ul> <li><code>Exception</code>:  if the path creation attempt hits an error with code indicating a cause other than pre-existence.</li> </ul> <pre><code>def pipestat(self)\n</code></pre> <p><code>pipestat.PipestatManager</code> object to use for pipeline results reporting and status management</p> <p>Depending on the object configuration it can report to a YAML-formatted file or PostgreSQL database. Please refer to pipestat documentation for more details: http://pipestat.databio.org/</p>"},{"location":"pypiper/code/python-api/#returns_3","title":"Returns:","text":"<ul> <li><code>pipestat.PipestatManager</code>:  object to use for results reporting</li> </ul> <pre><code>def process_counter(self)\n</code></pre> <p>Increments process counter with regard to the follow state: if currently executed command is a follow function of another one, the counter is not incremented.</p>"},{"location":"pypiper/code/python-api/#returns_4","title":"Returns:","text":"<ul> <li><code>str | int</code>:  current counter state, a number if the counter has been incremented or a number of the previous process plus \"f\" otherwise</li> </ul> <pre><code>def remove_container(self, container)\n</code></pre> <pre><code>def report_object(self, key, filename, anchor_text=None, anchor_image=None, annotation=None, nolog=False, result_formatter=None, force_overwrite=True)\n</code></pre> <p>Writes a key:value pair to self.pipeline_stats_file. Note: this function will be deprecated. Using report_result is recommended.</p>"},{"location":"pypiper/code/python-api/#parameters_8","title":"Parameters:","text":"<ul> <li><code>key</code> (<code>str</code>):  name (key) of the object</li> <li><code>filename</code> (<code>str</code>):  relative path to the file (relative to parentoutput dir)</li> <li><code>anchor_text</code> (<code>str</code>):  text used as the link anchor test or caption torefer to the object. If not provided, defaults to the key.</li> <li><code>anchor_image</code> (<code>str</code>):  a path to an HTML-displayable image thumbnail(so, .png or .jpg, for example). If a path, the path should be relative to the parent output dir.</li> <li><code>annotation</code> (<code>str</code>):  By default, the figures will be annotated withthe pipeline name, so you can tell which pipeline records which figures. If you want, you can change this.</li> <li><code>nolog</code> (<code>bool</code>):  Turn on this flag to NOT print this result in thelogfile. Use sparingly in case you will be printing the result in a different format.</li> <li><code>result_formatter</code> (<code>str</code>):  function for formatting via pipestat backend</li> <li><code>force_overwrite</code> (<code>bool</code>):  overwrite results if they already exist?</li> </ul>"},{"location":"pypiper/code/python-api/#returns_5","title":"Returns:","text":"<ul> <li><code>str reported_result</code>:  the reported result is returned as a list of formatted strings.</li> </ul> <pre><code>def report_result(self, key, value, nolog=False, result_formatter=None, force_overwrite=True)\n</code></pre> <p>Writes a key:value pair to self.pipeline_stats_file.</p>"},{"location":"pypiper/code/python-api/#parameters_9","title":"Parameters:","text":"<ul> <li><code>key</code> (<code>str</code>):  name (key) of the stat</li> <li><code>value</code> (<code>dict</code>):  value of the stat to report.</li> <li><code>nolog</code> (<code>bool</code>):  Turn on this flag to NOT print this result in thelogfile. Use sparingly in case you will be printing the result in a different format.</li> <li><code>result_formatter</code> (<code>str</code>):  function for formatting via pipestat backend</li> <li><code>force_overwrite</code> (<code>bool</code>):  overwrite results if they already exist?</li> </ul>"},{"location":"pypiper/code/python-api/#returns_6","title":"Returns:","text":"<ul> <li><code>str reported_result</code>:  the reported result is returned as a list of formatted strings.</li> </ul> <pre><code>def run(self, cmd, target=None, lock_name=None, shell=None, nofail=False, clean=False, follow=None, container=None, default_return_code=0)\n</code></pre> <p>The primary workhorse function of PipelineManager, this runs a command.</p> <p>This is the command  execution function, which enforces race-free file-locking, enables restartability, and multiple pipelines can produce/use the same files. The function will wait for the file lock if it exists, and not produce new output (by default) if the target output file already exists. If the output is to be created, it will first create a lock file to prevent other calls to run (for example, in parallel pipelines) from touching the file while it is being created. It also records the memory of the process and provides some logging output.</p>"},{"location":"pypiper/code/python-api/#parameters_10","title":"Parameters:","text":"<ul> <li><code>cmd</code> (<code>str | list[str]</code>):  Shell command(s) to be run.</li> <li><code>target</code> (<code>str | Sequence[str]</code>):  Output file(s) to produce, optional.If all target files exist, the command will not be run. If no target is given, a lock_name must be provided.</li> <li><code>lock_name</code> (<code>str</code>):  Name of lock file. Optional.</li> <li><code>shell</code> (<code>bool</code>):  If command requires should be run in its own shell.Optional. Default: None --will try to determine whether the command requires a shell.</li> <li><code>nofail</code> (<code>bool</code>):  Whether the pipeline proceed past a nonzero return from process, default False; nofail can be used to implement non-essential parts of the pipeline; if a 'nofail' command fails, the pipeline is free to continue execution.</li> <li><code>clean</code> (<code>bool</code>):  True means the target file will be automatically addedto an auto cleanup list. Optional.</li> <li><code>follow</code> (<code>callable</code>):  Function to call after executing (each) command.</li> <li><code>container</code> (<code>str</code>):  Name for Docker container in which to run commands.</li> <li><code>default_return_code</code> (<code>Any</code>):  Return code to use, might be used to discriminatebetween runs that did not execute any commands and runs that did.</li> </ul>"},{"location":"pypiper/code/python-api/#returns_7","title":"Returns:","text":"<ul> <li><code>int</code>:  Return code of process. If a list of commands is passed,this is the maximum of all return codes for all commands.</li> </ul> <pre><code>def start_pipeline(self, args=None, multi=False)\n</code></pre> <p>Initialize setup. Do some setup, like tee output, print some diagnostics, create temp files. You provide only the output directory (used for pipeline stats, log, and status flag files).</p> <pre><code>def stop_pipeline(self, status='completed')\n</code></pre> <p>Terminate the pipeline.</p> <p>This is the \"healthy\" pipeline completion function. The normal pipeline completion function, to be run by the pipeline at the end of the script. It sets status flag to completed and records some time and memory statistics to the log file.</p> <pre><code>def time_elapsed(time_since)\n</code></pre> <p>Returns the number of seconds that have elapsed since the time_since parameter.</p>"},{"location":"pypiper/code/python-api/#parameters_11","title":"Parameters:","text":"<ul> <li><code>time_since</code> (<code>float</code>):  Time as a float given by time.time().</li> </ul> <pre><code>def timestamp(self, message='', checkpoint=None, finished=False, raise_error=True)\n</code></pre> <p>Print message, time, and time elapsed, perhaps creating checkpoint.</p> <p>This prints your given message, along with the current time, and time elapsed since the previous timestamp() call.  If you specify a HEADING by beginning the message with \"###\", it surrounds the message with newlines for easier readability in the log file. If a checkpoint is designated, an empty file is created corresponding to the name given. Depending on how this manager's been configured, the value of the checkpoint, and whether this timestamp indicates initiation or completion of a group of pipeline steps, this call may stop the pipeline's execution.</p>"},{"location":"pypiper/code/python-api/#parameters_12","title":"Parameters:","text":"<ul> <li><code>message</code> (<code>str</code>):  Message to timestamp.</li> <li><code>checkpoint</code> (<code>str</code>):  Name of checkpoint; this tends to be somethingthat reflects the processing logic about to be or having just been completed. Provision of an argument to this parameter means that a checkpoint file will be created, facilitating arbitrary starting and stopping point for the pipeline as desired.</li> <li><code>finished</code> (<code>bool</code>):  Whether this call represents the completion of aconceptual unit of a pipeline's processing</li> <li><code>raise_error</code> (``):  Whether to raise exception ifcheckpoint or current state indicates that a halt should occur.</li> </ul> <pre><code>def warning(self, msg, *args, **kwargs)\n</code></pre> <p>Version Information: <code>pypiper</code> v0.14.1, generated by <code>lucidoc</code> v0.4.4</p>"},{"location":"spec/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format. </p>"},{"location":"spec/changelog/#210-2022-05-03","title":"[2.1.0] - 2022-05-03","text":"<p>Version 2.1.0 relaxes a few constraints and adds a few new features. It is backwards-compatible with 2.1.0 so updated parsers should continue to work with old sample tables. No existing features were changed, only a few new options were added.</p>"},{"location":"spec/changelog/#added","title":"Added","text":"<ul> <li>CSV-only PEPs: A <code>YAML</code> project configuration file is no longer required. A PEP may now be specified with only a CSV sample table file.</li> <li>Multi-row samples. Originally, PEP required each row to correspond to a single sample, and attributes with multiple values for a sample were specified using subsample tables. Furthermore, samples tables required  <code>sample_name</code> as the index column. In PEP <code>2.1.0</code>, multi-value attributes may now be accomplished without subsample tables by specifying multiple rows with the same identifier. These rows will be automatically merged, and unique attribute values will be retained. </li> <li>Sample index and subsample index columns may now be specified using new project attributes: <code>sample_table_index</code> and <code>subsample_table_index</code>.</li> </ul>"},{"location":"spec/changelog/#200-2020-05-26","title":"[2.0.0] - 2020-05-26","text":""},{"location":"spec/changelog/#added_1","title":"Added","text":"<ul> <li>Imports section allows linking to external PEP config files.</li> <li>It is now possible to specify more than one amendment (which replaced subprojects).</li> <li>Support for multiple subsample tables</li> </ul>"},{"location":"spec/changelog/#changed","title":"Changed","text":"<ul> <li>The structure changed (no more metadata section, new sections for sample and project modifiers, config, etc).</li> <li>Added a sample_modifiers section to house 5 modifiers: remove, append, duplicate, imply, and derive</li> <li>Added a project_modifiers section to house amend and import</li> <li>The structure for 'imply' now uses an if-then format instead of the previous format that was not readily interpretable</li> <li>Subprojects are renamed amendments</li> <li>Tool-specific sections for looper have been removed.</li> </ul>"},{"location":"spec/changelog/#100-legacy-version","title":"[1.0.0] - Legacy version","text":"<p>The original PEP specification was used from 2014 through 2020 with some minor, backwards-compatible updates. It was not versioned.</p>"},{"location":"spec/howto-amendments/","title":"How to store many projects in one file","text":"<p>The <code>amendments</code> section of the config file allows you to include multiple variations of a project within one file. When a PEP is parsed, you may specify one or more included amendments, which will amend the values in the processed PEP. This is a powerful function that can be used for many purposes, such as on the fly tweaks or embedding multiple subprojects within a parent project.</p>"},{"location":"spec/howto-amendments/#example","title":"Example","text":"<pre><code>sample_table: annotation.csv\nproject_modifiers:\n  amend:\n    my_project2:\n      sample_table: annotation2.csv\n    my_project3:\n      sample_table: annotation3.csv\n...\n</code></pre> <p>If you load this configuration file, it will by default use the <code>annotation.csv</code> file specified in the <code>sample_table</code> attribute, as you would expect. If you don't activate any amendments, they are ignored. But if you choose, you may activate one of the two amendments, which are called <code>my_project2</code> and <code>my_project3</code>. If you activate <code>my_project2</code>, by passing <code>amendments=my_project2</code> when parsing the PEP, the resulting object will use the <code>annotation2.csv</code> sample_table instead of the default <code>annotation.csv</code>. All other project settings will be the same as if no amendment was activated because there are no other values specified in the <code>my_project2</code> amendment.</p> <p>Practically what happens under the scenes is that the primary project is first loaded, and then, if an amendment is activated, it overrides any attributes with those specified in the amendment.</p>"},{"location":"spec/howto-amendments/#rationale","title":"Rationale","text":"<p>At times you will want to create two projects that are very similar, but differ just in one or two attributes. For example, you may define a project with one set of samples, and then want an identical project but using a different sample annotation sheet. Or, you may define a project to run on a particular reference genome, and want to define a second project that is identical, but uses a different reference genome.</p> <p>You could simply define 2 complete PEPs, but this would duplicate information and make it harder to maintain. Instead, you can use amendments, which allow you to encode additional similar projects all within the original <code>project_config.yaml</code> file. Amendments are like mini embedded <code>project_config.yaml</code> files that can be activated by software. </p>"},{"location":"spec/howto-amendments/#how-do-you-activate-an-amendment","title":"How do you activate an amendment?","text":"<p>Activating an amendment depends on what software you're using to load your PEP. A PEP-compliant implementation must define some way to activate amendments. For example, you can activate amendments in <code>peppy</code> or <code>pepr</code> by passing an argument, <code>amendments=my_project2</code>, when you construct the <code>Project</code> object.</p>"},{"location":"spec/howto-amendments/#can-you-activate-multiple-amendments","title":"Can you activate multiple amendments?","text":"<p>Yes! Amendments are passed and parsed as a priority list; so the first amendment is processed, and then the next one, and so on. So, the final amendment in the list has the highest priority.</p>"},{"location":"spec/howto-automatic-groups/","title":"How to create automatic sample groups","text":"<p>The <code>imply</code> modifier lets you set global, group-level attributes at the project level instead of duplicating that information for every sample that belongs to a group of samples. This makes your project more portable and does a better job conceptually with separating sample attributes from project attributes.</p>"},{"location":"spec/howto-automatic-groups/#example","title":"Example","text":"<pre><code>sample_modifiers:\n  imply:\n    - if:\n        genome: [\"hg18\", \"hg19\", \"hg38\"]\n      then:\n        organism: \"human\"\n    - if:\n        genome: [\"mm9\", \"mm10\"]\n      then:\n        organism: \"mouse\"  \n    - if:\n        organism: [\"human\", \"mouse\", \"Mouse\"]\n      then:\n        family: \"mammal\"\n    - if:\n        organism: [\"bird\", \"jay\", \"cardinal\"]\n      then:\n        family: \"aves\"\n</code></pre> <p>In this example we expect the samples will have existing <code>genome</code> attributes. The first and second implications will set an <code>organism</code> attribute to either <code>human</code> or <code>mouse</code>, depending on the value in the <code>genome</code> attribute. The next implication then adds a new variable, <code>family</code>, with the value <code>mammal</code> for any samples with <code>human</code> or <code>mouse</code> as the <code>organism</code>.</p> <p>We've therefore used implied attributes to automatically create sample groups on the basis of existing attributes. We could then use the value in <code>family</code>, for example, to do a differential comparison on these samples. Any new samples added to the table in the original format will immediately work with any downstream tools.</p>"},{"location":"spec/howto-eliminate-paths/","title":"How to eliminate paths from a sample table","text":"<p>Sample tables often need to point to one or more input files for each sample. Of course, you could just add a column with the file path, like <code>/path/to/input/file.fastq.gz</code>. For example:</p> <pre><code>\"sample_name\", \"library\", \"organism\", \"time\", \"file_path\"\n\"pig_0h\", \"RRBS\", \"pig\", \"0\", \"/data/lab/project/pig_0h.fastq\"\n\"pig_1h\", \"RRBS\", \"pig\", \"1\", \"/data/lab/project/pig_1h.fastq\"\n\"frog_0h\", \"RRBS\", \"frog\", \"0\", \"/data/lab/project/frog_0h.fastq\"\n\"frog_1h\", \"RRBS\", \"frog\", \"1\", \"/data/lab/project/frog_1h.fastq\"\n</code></pre> <p>This is common, but what if the data get moved, the filesystem changes, or you switch servers? Will this data still be there in 2 years? Do you want long file paths cluttering your sample table? What if you have 2 or 3 input files? Do you want to manually manage these unwieldy absolute paths?</p>"},{"location":"spec/howto-eliminate-paths/#using-a-derived-column","title":"Using a derived column","text":"<p>The PEP specification makes it really easy to do better with the <code>derive</code> sample modifier. The <code>derive</code> sample modifier adds new sample attributes that are derived from existing sample attributes. What was originally <code>/long/path/to/sample.fastq.gz</code> would instead contain just a key, like <code>source1</code>, and then a systematic formula for how to construct the path programmatically is included in the project config file. Here's an example of the same sample table using a <code>derived attribute</code> for <code>file_path</code>:</p> <pre><code>\"sample_name\", \"library\", \"organism\", \"time\", \"file_path\"\n\"pig_0h\", \"RRBS\", \"pig\", \"0\", \"source1\"\n\"pig_1h\", \"RRBS\", \"pig\", \"1\", \"source1\"\n\"frog_0h\", \"RRBS\", \"frog\", \"0\", \"source1\"\n\"frog_1h\", \"RRBS\", \"frog\", \"1\", \"source1\"\n</code></pre> <p>And here's the project config file that will derive that path correctly:</p> <pre><code>sample_modifiers:\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: /data/lab/project/{sample_name}.fastq\n      source2: /path/from/collaborator/weirdNamingScheme_{external_id}.fastq\n</code></pre> <p>In this example, the samples' <code>file_path</code> attribute will be derived with one of two different paths, depending on the original value of the attribute (<code>source1</code> or <code>source2</code>). To do this, your project config file must specify two things: First, which attributes are derived (in this case, <code>file_path</code>); and second, a <code>derived_sources</code> section mapping keys to strings that will construct your path from other sample or project attributes.</p> <p>That's it! The source string can use other sample attributes (columns) using braces, as in <code>{sample_name}</code>. The attributes will be automatically populated separately for each sample. To take this a step further, you'd get the same result with this config file, which substitutes <code>{sample_name}</code> for other sample attributes, <code>{organism}</code> and <code>{time}</code>:</p> <pre><code>sample_modifiers:\n  derive:\n    attributes: [file_path]\n    sources:\n      source1: /data/lab/project/{organism}_{time}h.fastq\n      source2: /path/from/collaborator/weirdNamingScheme_{external_id}.fastq\n</code></pre> <p>As long as your file naming system is systematic, you can easily deal with any external naming scheme. You can specify as many derived columns as you want.</p>"},{"location":"spec/howto-eliminate-paths/#tips","title":"Tips","text":"<p>Don't put absolute paths to files in your sample table. Instead, specify a source and then provide a template in the config file. This way if your data changes locations (which happens more often than we would like), or you change servers, or you want to share or publish the project, you just have to change the config file and not update paths in the sample table; this makes the sample table universal across environments, users, publication, etc. The whole project is now portable.</p> <p>Think of each sample as belonging to a certain type (for simple experiments, the type will be the same); then define the location of these samples in the project configuration file. As a side bonus, you can easily include samples from different locations, and you can also share the same sample sample table on different environments (i.e. servers or users) by having multiple project config files (or, better yet, by defining a subproject for each environment). The only thing you have to change is the project-level expression describing the location, not any sample attributes (plus, you get to eliminate those annoying long/path/arguments/in/your/sample/annotation/sheet).</p>"},{"location":"spec/howto-genome-id/","title":"How to remove genome from a sample table","text":"<p>Many sample tables include identifiers like a genome or transcriptome assembly (e.g. <code>hg38</code>) that really are an aspect of an analysis, rather than an attribute of a particular sample. If you store these attributes within the sample table, you reduce its portability because those attributes only apply to that particular analysis. If samples are only used for as single analysis, that's fine, but the point of PEP is to encourage re-use of data, so we'd like our sample tables to be as portable as possible. Instead, if you store these variables in the project configuration file, the sample table could be re-used across projects with different analysis settings. </p> <p>One way to solve this is to use an <code>append</code> modifier to add a <code>genome</code> attribute to each sample from the project config file.</p> <pre><code>sample_modifiers:\n  append:\n    genome: \"hg38\"\n</code></pre> <p>This way, we've moved the 'genome' attribute out of the sample table. Another analysis that could run on this same set of input data could now use the sample table without issue. In fact, we could even include these two analyses in the same project config file using an amendment:</p> <pre><code>sample_modifiers:\n  append:\n    genome: \"hg38\"\n\nproject_modifiers:\n  amend:\n    hg19_alignment:\n      sample_modifiers:\n        append:\n          genome: \"hg19\"\n</code></pre> <p>Now, when loading this project, if you run <code>amendments=hg19_alignment</code>, the project will align to hg19 instead of hg38.</p>"},{"location":"spec/howto-genome-id/#example-with-multiple-species","title":"Example with multiple species","text":"<p>The <code>append</code> modifier will add the same value to all samples; if your project requires that some samples be aligned to different assemblies, you'll need more power. The <code>imply</code> sample modifier allows you to create new sample attributes whose value depends on the value of another attribute. Here's an example that adds a genome attribute with a value that depends on another attribute:</p> <pre><code>sample_modifiers:\n  imply:\n    - if:\n        organism: \"human\"\n      then:\n        genome: \"hg38\"\n        macs_genome_size: \"hs\"\n    - if:\n        organism: \"mouse\"\n      then:\n        genome: \"mm10\"\n        macs_genome_size: \"mm\"\n</code></pre> <p>In this example, if my <code>organism</code> attribute is <code>human</code>, this implies a few other project-specific attributes. For one project, I want to set <code>genome</code> to <code>hg38</code> and <code>macs_genome_size</code> to <code>hs</code>. Of course, I could just define columns called <code>genome</code> and <code>macs_genome_size</code>, but this has several disadvantages: First, changing the aligned genome would require changing every sample in the sample table. Second, the genome is now tied to the sample table, so it could not be used in a different project that used a different genome. A better way would be handle these attributes at the project level using <code>imply</code>.</p> <p>Instead of hard-coding <code>genome</code> and <code>macs_genome_size</code> in the sample table, you can simply specify that the attribute <code>organism</code> implies additional attribute-value pairs (which may vary by sample based on the value of the <code>organism</code> attribute). This lets you specify the genome, transcriptome, genome size, and other similar variables all in your project configuration file. After all, a reference genome assembly is really not an inherent property of a sample, but of a sample in respect to a particular project or alignment.</p>"},{"location":"spec/howto-geofetch/","title":"How to create a PEP from GEO or SRA","text":"<p><code>geofetch</code> is a command-line utility that converts GEO or SRA accessions into PEP projects. You provide an accession (or a spreadsheet with a list of accessions), and <code>geofetch</code> with produce the PEP (both project config and sample annotation). <code>geofetch</code> can also download the data from SRA, so your project will be ready for direct input into any PEP-compatible tool.</p> <p>For more information, see <code>geofetch</code> user documentation and vignettes</p>"},{"location":"spec/howto-integrate/","title":"How to integrate imports and amendments","text":"<p>Imports and amendments can be combined to make really powerful analysis integration. One use case is to keep track of a bunch of pipelines and analyses.</p> <p>First, make a PEP that specifies your list of pipelines, each as an amendment with a key you choose. Save this as <code>my_pipelines.yaml</code> <pre><code>pep_version: 2.0.0\nproject_modifiers:\n  amend:\n    pipeline1:\n      looper:\n        pipeline_interface: http://piface.databio.org/peppro.yaml\n    pipeline2:\n      looper:\n        pipeline_interface: http://piface.databio.org/pepatac.yaml\n</code></pre></p> <p>Now create a PEP that lists a bunch of separate sample collections under amendments. Save this file as <code>my_data.yaml</code>:</p> <pre><code>pep_version: 2.0.0\nimports: my_pipelines.yaml\nproject_modifiers:\n  amend:\n    data1:\n      sample_table: my_samples.csv\n    data2:\n      sample_table: samples2.csv\n</code></pre> <p>Since this second file imports the first, all of the amendments will be available. Now you can mix and match data and analysis with:</p> <pre><code>cmd ... --pep=my_data.yaml --amend=pipeline1,data1\ncmd ... --pep=my_data.yaml --amend=pipeline1,data2\ncmd ... --pep=my_data.yaml --amend=pipeline2,data1\ncmd ... --pep=my_data.yaml --amend=pipeline2,data2\n</code></pre> <p>You could import the same pipelines file in a different data description file to keep your pipelines consistent across analysis.</p>"},{"location":"spec/howto-mixmatch/","title":"How to mix and match amendments on the fly","text":"<p>At times you will want to create two projects that are very similar, but differ just in one or two attributes. For example, you may define a project with one set of samples, and then want an identical project but using a different sample annotation sheet. Or, you may define a project to run on a particular reference genome, and want to define a second project that is identical, but uses a different reference genome.</p> <p>You could simply define 2 complete PEPs, but this would duplicate information and make it harder to maintain. Instead, you can use amendments, which allow you to encode additional similar projects all within the original <code>project_config.yaml</code> file. Amendments are like mini embedded <code>project_config.yaml</code> files that can be activated by software. When a PEP is parsed, you may specify one or more included amendments, which will amend the values in the processed PEP. This is a powerful function that can be used for many purposes, such as on the fly tweaks or embedding multiple subprojects within a parent project.</p>"},{"location":"spec/howto-mixmatch/#example","title":"Example","text":"<pre><code>sample_table: annotation.csv\ngenome: hg38\nproject_modifiers:\n  amend:\n    my_project2:\n      sample_table: annotation2.csv\n    my_project3:\n      sample_table: annotation3.csv\n    hg19_analysis:\n      genome: hg19\n...\n</code></pre> <p>If you load this configuration file, it will by default use the <code>annotation.csv</code> file, and the <code>genome</code> attribute will be set to \"hg38\". There are 3 amendments available: <code>my_project2</code>, <code>my_project3</code>,a and <code>hg19_analysis</code>. If you activate <code>my_project2</code>, by passing <code>amendments=my_project2</code> when parsing the PEP, the resulting object will use the <code>annotation2.csv</code> sample_table instead of the default <code>annotation.csv</code> -- still run on \"hg38\". To run two amendments, you could issue <code>amendments=my_project2,hg19_analysis</code>, which will result in this config file:</p> <pre><code>sample_table: annotation2.csv\ngenome: hg19\nproject_modifiers:\n  amend:\n    ...\n</code></pre> <p>Practically what happens under the scenes is that the primary project is first loaded, and then, if an amendment is activated, it overrides any attributes with those specified in the amendment.</p>"},{"location":"spec/howto-mixmatch/#how-do-you-activate-an-amendment","title":"How do you activate an amendment?","text":"<p>Activating an amendment depends on what software you're using to load your PEP. A PEP-compliant implementation must define some way to activate amendments. For example, you can activate amendments in <code>peppy</code> or <code>pepr</code> by passing an argument, <code>amendments=my_project2</code>, when you construct the <code>Project</code> object.</p>"},{"location":"spec/howto-mixmatch/#can-you-activate-multiple-amendments","title":"Can you activate multiple amendments?","text":"<p>Yes! Amendments are passed and parsed as a priority list; so the first amendment is processed, and then the next one, and so on. So, the final amendment in the list has the highest priority.</p>"},{"location":"spec/howto-multi-value-attributes/","title":"How to specify multiple input files","text":"<p>Occasionally, a sample needs to have more than one value for an attribute. For example, you may have multiple input files for one sample, such as a single library that was spread across multiple sequencing lanes. This doesn't fit naturally into a tabular data format because it requires a one-to-many relationship. For these kinds of attributes, the PEP specification provides three possibilities:</p> <ol> <li>Use shell expansion characters (like <code>*</code> or <code>[]</code>) in a <code>derived.source</code> definition (good for situations where the multi-valued attributes are file paths)</li> <li>Specify a subsample table (infinitely customizable for more complicated merges).</li> <li>Use a multiple rows per sample (useful when a single sample table is required).</li> </ol> <p>To explain how this works, we'll use the most common example case of needing it: a single sample with an attribute that points to a file, but there are multiple input files for that attribute.</p>"},{"location":"spec/howto-multi-value-attributes/#option-1-wildcards","title":"Option 1: wildcards","text":"<p>To use wildcards, just use asterisks in your data source specifications, like this:</p> <pre><code>derive:\n  sources:\n    data_R1: \"${DATA}/{id}_S{nexseq_num}_L00*_R1_001.fastq.gz\"\n    data_R2: \"${DATA}/{id}_S{nexseq_num}_L00*_R2_001.fastq.gz\"\n</code></pre> <p>PEP will automatically glob these to whatever files are on disk, and use those as the multiple values for the attribute. This only works if:</p> <ol> <li>the attribute with multiple values is a file,</li> <li>the file paths are systematic, computable with a shell wildcard, and </li> <li>the file system is local to the PEP and can be computed at runtime.</li> </ol>"},{"location":"spec/howto-multi-value-attributes/#option-2-the-subsample-table","title":"Option 2: the subsample table","text":"<p>A subsample table is a table with one row per attribute -- so a single sample appears multiple times in the table. Just provide a subsample table in your project config:</p> <pre><code>sample_table: annotation.csv\nsubsample_table: subsample_table.csv\n</code></pre> <p>Make sure the <code>sample_name</code> column of this table match the <code>sample_name</code> column in your sample_table, and then include any columns that require multiple values. <code>PEP</code> will automatically include all of these values as appropriate. </p> <p>Here's a simple example of a PEP that uses subsamples. If you define <code>annotation.csv</code> like this:</p> <pre><code>sample_name,library\nfrog_1,anySampleType\nfrog_2,anySampleType\n</code></pre> <p>Then point <code>subsample_table.csv</code> to the following, which maps <code>sample_name</code> to a new column called <code>file</code></p> <pre><code>sample_name,file\nfrog_1,data/frog1a_data.txt\nfrog_1,data/frog1b_data.txt\nfrog_1,data/frog1c_data.txt\nfrog_2,data/frog2a_data.txt\nfrog_2,data/frog2b_data.txt\n</code></pre> <p>This sets up a simple relational database that maps multiple files to each sample. You can also combine these multi-value attributes with derived columns; columns will first be derived, and then merged. Let's now look at a slightly more complex example that has two multi-value attributes (such as the case with a paired-end sequencing experiment with multiple files for each R1 and R2):</p> <p>The sample table is:</p> <pre><code>sample_name library\nfrog_1  anySampleType\nfrog_2  anySampleType\n</code></pre> <p>And the subsample table is:</p> <pre><code>sample_name,read1,read2\nfrog_1,data/frog1a_R1.txt,data/frog1a_R2.txt\nfrog_1,data/frog1b_R1.txt,data/frog1b_R2.txt\nfrog_1,data/frog1c_R1.txt,data/frog1c_R2.txt\nfrog_2,data/frog2a_R1.txt,data/frog2a_R2.txt\nfrog_2,data/frog2b_R1.txt,data/frog2b_R2.txt\n</code></pre> <p>This yields 2 samples, each with a single-valued attribute of <code>library</code>, and multi-valued attributes of <code>read1</code> and <code>read2</code>.</p>"},{"location":"spec/howto-multi-value-attributes/#option-3-multiple-rows-per-sample","title":"Option 3: Multiple rows per sample","text":"<p>In PEP v2.1.0, we relaxed the constraint that each sample must correspond to exactly one row in the sample table, and introduce the possibility of encoding multi-valued samples with multiple rows. The same data as the two-table approach above can be represented like this:</p> <pre><code>sample_name,library,read1,read2\nfrog_1,anySampleType,data/frog1a_R1.txt,data/frog1a_R2.txt\nfrog_1,,data/frog1b_R1.txt,data/frog1b_R2.txt\nfrog_1,,data/frog1c_R1.txt,data/frog1c_R2.txt\nfrog_2,anySampleType,data/frog2a_R1.txt,data/frog2a_R2.txt\nfrog_2,,data/frog2b_R1.txt,data/frog2b_R2.txt\n</code></pre> <p>Now, this is the <code>sample_table</code>, but the <code>sample_name</code> column is not unique -- the first row has values for any single-valued attributes (<code>library</code> in the example), and then other columns can be specified in multiple rows. The PEP processor will integrate these samples in correctly represent this as two samples.</p>"},{"location":"spec/howto-multi-value-attributes/#further-thoughts","title":"Further thoughts","text":"<ul> <li> <p>There are pros and cons to each of these approaches. Representing a single sample with multiple rows in the sample table can cause conceptual and analytical challenges. There is value in the simplicity of being able to see each row as a separate sample, which is preserved in the subsample table approach. However, this leads to the requirement of an additional file to represent samples, which can be problematic in other settings. One advantage of PEP is that either approach can be used, so you can use the approach that fits your situation best.</p> </li> <li> <p>Subsample tables are intended to handle multiple values of the same type. To handle different classes of input files, like read1 and read2, these are not the same type, and are therefore not put into a subsample table. Instead, these should be handled as different columns in the main sample annotation sheet (and therefore different arguments to the pipeline). It is possible that you will want to have read1 and read2, and then each of these could have multiple inputs, which would then be placed in the subsample table.</p> </li> <li> <p>If your project has some samples with subsample entries, but others without, then you only need to include samples in the subsample table if they have subsample attributes. Other samples can just be included in the primary annotation table. However, this means you'll need to make sure you provide the correct columns in the primary <code>sample_table</code> sheet; the simple example above assumes every sample has subsample attributes, so it doesn't need to define <code>file</code> in the <code>sample_table</code>. If you had samples without attributes specified in the subsample table, you'd need to specify that column in the primary sheet.</p> </li> <li> <p>In practice, we've found that most projects can be solved using wildcards, and subsample tables are not necessary. If you start to think about how to use subsample tables, first double-check that you can't solve the problem using a simple wildcard; that makes it much easier to think about, and it should be possible as long as the files are named systematically.</p> </li> <li> <p>Don't combine these approaches.  Using both wildcards and a subsample table simultaneously for the same sample can lead to recursion. Using a subsample table requires your sample table to have unique sample names, which precludes the possibility of multi-row samples. So, just pick an approach and stick to it.</p> </li> <li> <p>Keep in mind: subsample tables is for files of the same type. A paired-end sequencing experiment, for example, yields two input files for each sample (read1 and read2). These are not equivalent, so you do not use subsample tables to put read1 and read2 files into the same attribute; instead, you would list in a subsample table all files (perhaps from different lanes) for read1, and then, separately, all files for read2. This is two separate attributes, which each </p> </li> </ul>"},{"location":"spec/howto-validate/","title":"How to validate a PEP","text":"The eido tool validates any PEP against any schema"},{"location":"spec/howto-validate/#introduction","title":"Introduction","text":"<p>The base PEP specification has few requirements, so PEPs can represent very different data types. A PEP that satisfies the base specification will work with general PEP tools, but a more specialized tool may need to impose additional requirements. To extend the PEP specification to specialized use cases, PEP provides a powerful validation framework. This allows tool authors to define a schema that specifies which attributes are required, which are optional, what their types are, which attributes point to input files, and so on.</p>"},{"location":"spec/howto-validate/#validating-a-generic-pep","title":"Validating a generic PEP","text":"<p>The base PEP specification is hosted at https://schema.databio.org/pep/2.0.0.yaml. You can validate a PEP against a PEP schema using the eido Python package like this:</p> <pre><code>eido validate path/to/your/PEP_config.yaml -s https://schema.databio.org/pep/2.0.0.yaml\n</code></pre> <p>This command will ensure that your metadata follows basic PEP format. That may be all you need, or you may need to validate it against a more specialized schema for a particular analysis.</p>"},{"location":"spec/howto-validate/#validating-a-pep-for-a-specific-tool","title":"Validating a PEP for a specific tool","text":"<p>Most tools will require more attributes than a base PEP provides. For example, a tool may require a <code>genome</code> attribute for each sample and we need to validate a PEP against a stricter schema. You would do this in the same way, just using the more specialized schema:</p> <pre><code>eido validate path/to/project_config.yaml -s SCHEMA\n</code></pre> <p>Where SCHEMA is a URL or local file. The author of the tool you are using should provide this schema so that you can make sure you are providing the correct metadata for the tool.</p>"},{"location":"spec/howto-validate/#writing-a-schema","title":"Writing a schema","text":"<p>If you developing a PEP-compatible tool, we recommend you write a PEP schema that describes what sample and project attributes are required for your tool to work. A detailed guide for writing your own schema can be found in the eido documentation.</p>"},{"location":"spec/peppy/","title":"Python package: peppy","text":"<p><code>peppy</code> is a Python package that loads PEPs. It instantiates an in-memory representation of metadata for your project and all of its samples, for any downstream purpose. <code>peppy</code> is useful for software developers or data analysts who use Python.</p>"},{"location":"spec/peppy/#code-and-documentation","title":"Code and documentation","text":"<ul> <li>User documentation and vignettes</li> <li>Source code at GitHub</li> </ul>"},{"location":"spec/peppy/#quick-start","title":"Quick start","text":"<p>Peppy is on pypi. Install with: <pre><code>pip install peppy\n</code></pre></p> <p>Or use <code>pip install --user --upgrade peppy</code> to install a local copy. Then you can load your project into Python with this code:</p> <pre><code>import peppy\n\nmy_project = peppy.Project(\"path/to/project_config.yaml\")\nmy_samples = my_project.samples\nsample_table = my_project.sample_table\n</code></pre>"},{"location":"spec/pepr/","title":"R package: pepr","text":"<p><code>pepr</code> is an R package for reading Portable Encapsulated Projects. It will read PEP projects, loading all project and sample metadata into R with a single line of code. <code>pepr</code> is currently in alpha mode and should not be used production projects.</p>"},{"location":"spec/pepr/#code-and-documentation","title":"Code and documentation","text":"<ul> <li>User documentation and vignettes</li> <li><code>pepr</code> API</li> <li>Source code at Github</li> </ul>"},{"location":"spec/pepr/#quick-start","title":"Quick start","text":"<p>Install from CRAN:</p> <pre><code>install.packages(\"pepr\")\n</code></pre> <p>Load a project and explore metadata like this:</p> <pre><code>library(\"pepr\")\ncfgPath = system.file(\n    \"extdata\",\n    paste0(\"example_peps-master\"),\n    \"example_basic\",\n    \"project_config.yaml\",\n    package = \"pepr\"\n  )\np = Project(file = cfgPath)\n\nsampleTable(p)\nconfig(p)\n</code></pre>"},{"location":"spec/rationale/","title":"Rationale","text":"PEP specifies a universal format <p>In a data analysis project, we frequently want to run many different tools on the same input data. Too often, this requires structuring the data uniquely for each tool. This makes it difficult to test multiple tools because each connection structure must be defined manually.</p> <p>To alleviate this challenge of linking data to tools, Portable Encapsulated Projects (PEP) standardizes the description of data collections, enabling both data providers and data users to communicate through a common interface. This link operates around a simple, standard, extensible definition of a project.</p> <p>PEP makes it easy to:</p> <ol> <li>use one metadata structure for all your projects</li> <li>work collaboratively</li> <li>share your project with others</li> <li>use multiple tools without restructuring metadata</li> <li>analyze data in both R and Python</li> </ol> <p>This web page outlines the PEP specification. Once you have a PEP, you will be able to process that metadata using tools in pepkit. You can find tools to use at PEP statistics.</p>"},{"location":"spec/rationale/#user-stories","title":"User stories","text":"<p>Here are a few example scenarios that describe the type of users and use cases for which PEP can be useful. </p>"},{"location":"spec/rationale/#sharing-datasets-among-collaborators","title":"Sharing datasets among collaborators","text":""},{"location":"spec/rationale/#the-situation","title":"The situation","text":"<p>Vijay is a computational biologist who collaborates with a group that has produced genomic data from 24 human blood samples. Vijay put together some software tools to analyze this data and came up with a novel way of looking at the samples. When he presents his results, Artur, from a lab down the hall, thinks Vijay's method would be really cool to apply on his samples as well. Artur's project has 50 genomic patient blood samples from a different disease with a similar underpinning. Artur would like to just send his data to Vijay and have him run it through Vijay's tool to see what he finds. Will Vijay be stuck with organizing and reformatting Artur's poorly-organized data?</p>"},{"location":"spec/rationale/#how-pepkit-solves-the-problem","title":"How pepkit solves the problem","text":"<p>Luckily, Artur's group is enlightened, and they always organize their projects in PEP format because they use a PEP-compatible pipeline for their data processing. Similarly, Vijay has designed his python package to rely on peppy for project structure. As a result, all Artur has to do is send Vijay his PEP, and Vijay runs it right on the samples. Nobody has to duplicate, rename, reorganize, and re-annotate Artur's samples to get Vijay's tool to run.</p>"},{"location":"spec/rationale/#running-multiple-pipelines-on-my-dataset","title":"Running multiple pipelines on my dataset","text":""},{"location":"spec/rationale/#the-situation_1","title":"The situation","text":"<p>Agnes is a biologist interested in understanding how chromatin accessibility differs among patients with Ewing sarcoma. Agnes processed 15 human patient samples with ATAC-seq. She isn't sure how to analyze the data, but she found PEPATAC, a pipeline that processes ATAC-seq data. PEPATAC requires that she organize her 15 samples in PEP format, which she does. She runs the data through the pipeline and is analyzing the results when a new pipeline called ATACMASTER is published, which is particularly suited for her particular biological system. She'd love to run her samples through this pipeline to see if it changes things. But wait...it took her a week to get her samples in the right format and structure and filenames on the cluster for the first pipeline. Will she need to re-organize her project, re-name her files, and change her annotation structure to fit the new pipeline?</p>"},{"location":"spec/rationale/#how-pepkit-solves-the-problem_1","title":"How pepkit solves the problem","text":"<p>Because ATACMASTER is also PEP-compatible, Agnes doesn't have to reorganize anything; all she does is change her project to point at the new pipeline, and it runs right through. If ATACMASTER was built in the traditional way, though, it would specify it's own structure, and Agnes would have a few painful days of data munging and troubleshooting before she got ATACMASTER to run. Then, if ULTIMATEPIPELINE comes out, Agnes could be burned out of data restructuring, and may decide to forget it, missing potential discovery.</p>"},{"location":"spec/rationale/#running-my-pipeline-on-a-public-dataset-from-geo","title":"Running my pipeline on a public dataset from GEO","text":""},{"location":"spec/rationale/#the-situation_2","title":"The situation","text":"<p>Jane has been working on a project studying DNA methylation in kidneys from a population of mice. She has sequencing data from 200 mouse samples, which are divided into 3 different treatment groups, and wants to know how the DNA methylation differs among groups. She has put together a pipeline to process her raw <code>fastq</code> sequencing data into methylation calls and has identified some interesting differences. But now she'd like to analyze this is the context of a recently published cohort of 75 mouse kidneys from a different population. The original study put the raw data on GEO, but the processed data was run on the <code>mm9</code> reference assembly instead of <code>mm10</code>, so she can't immediately compare. Shortly thereafter, this kidney work has become really hot, and 2, then 3 more studies with mouse kidney samples are published. How can Jane organize all that recently published data in a way that will work with her project?</p>"},{"location":"spec/rationale/#how-pepkit-solves-the-problem_2","title":"How pepkit solves the problem","text":"<p>If she organized her project as a PEP and her pipeline used pepkit, she could use geofetch to download the public GEO data and create a PEP from it in a single line of code. In another line or two, she could use <code>looper</code> to run her custom DNA methylation pipeline on that data. Finally, she can use <code>pepr</code> or <code>BiocProject</code> to load that processed public data into R, and immediately compare it with her in-house data. If she was using an independent method for organizing her samples, she'd have to repeat that for each new dataset that is published.</p>"},{"location":"spec/rationale/#testing-my-tool-on-many-different-data-sources","title":"Testing my tool on many different data sources","text":""},{"location":"spec/rationale/#the-situation_3","title":"The situation","text":"<p>Carlos has downloaded some public transcription factor binding microarray data and been trying to think of new ways to visualize things. He made a nice new tool that can load up this data type into R and display a shiny app for interactive visualization. He would like to use it to test a bunch of other public datasets, and eventually realizes that others might want to use it to look at their data as well. </p>"},{"location":"spec/rationale/#how-pepkit-solves-the-problem_3","title":"How pepkit solves the problem","text":"<p>Since Carlos' package imports <code>pepr</code>, his tool can immediately work on any PEP-compatible project. He can therefor use <code>geofetch</code> to download the public GEO data and create a PEP, which can load right into his tool Any of his colleagues that want to use his tool can, as well. By testing a bunch of different datasets, Carlos has made his tool much more useful.</p>"},{"location":"spec/simple-example/","title":"PEP specification: A simple example","text":"<p>How do I create my own PEP? </p> <p></p> <p>To use any PEP-compatible tool, you first need a PEP. A PEP describes a collection of data with its metadata. To create a PEP to represent your dataset, you create 2 files:</p> <ol> <li>Project config file - a <code>yaml</code> file with project settings</li> <li>Sample table - a <code>csv</code> file with 1 row per sample</li> </ol> <p>In the simplest case, project_config.yaml is just a few lines of yaml, a simple and widely-used hierarchical markup language used to store key-value pairs; you can read more about yaml here.  Here's a minimal example project_config.yaml:</p> <pre><code>pep_version: 2.0.0\nsample_table: \"path/to/sample_table.csv\"\n</code></pre> <p>The <code>sample_table</code> key points to the second part of a PEP, a comma-separated value (<code>csv</code>) file annotating samples in the project. Here's a small example of sample_table.csv:</p> <pre><code>\"sample_name\", \"protocol\", \"file\"\n\"frog_1\", \"RNA-seq\", \"frog1.fq.gz\"\n\"frog_2\", \"RNA-seq\", \"frog2.fq.gz\"\n\"frog_3\", \"RNA-seq\", \"frog3.fq.gz\"\n\"frog_4\", \"RNA-seq\", \"frog4.fq.gz\"\n</code></pre> <p>With those two simple files, you are ready to use the pepkit tools! With a single line of code, you could load this into R using pepr, into python using peppy, or run each sample through an arbitrary command-line pipeline using looper. You can use this formulation to run a workflow written in CWL or using SnakeMake. If you make a habit of describing all your projects like this, you'll never parse another sample annotation sheet again. You'll never write another pipeline submission loop.</p> <p>This simple example presents a minimal functioning PEP. In practice, there are many advanced features of PEP structure. For instance, you can add additional sections to tailor your project for specific tools. But at its core, PEP is simple and generic; this way, you can start with the basics, and only add more complexity as you need it.</p> <p>More advanced features are described in the complete PEP specification.</p>"},{"location":"spec/specification/","title":"PEP specification version 2.1.0","text":"PEP specification version 2.1.0 <p>Table of contents:</p> <ul> <li>Introduction and motivation</li> <li>How PEP improves sample annotation portability</li> <li>Definitions of terms and components of a PEP</li> <li>Validating a PEP</li> <li>Project config file specification<ul> <li>Project attribute: pep_version</li> <li>Project attribute: sample_table</li> <li>Project attribute: subsample_table</li> <li>Project attribute: sample_modifiers<ul> <li>Sample modifier: remove</li> <li>Sample modifier: append</li> <li>Sample modifier: duplicate</li> <li>Sample modifier: imply</li> <li>Sample modifier: derive</li> </ul> </li> <li>Project attribute: project_modifiers<ul> <li>Project modifier: import</li> <li>Project modifier: amend</li> </ul> </li> </ul> </li> <li>Sample table specification<ul> <li>Sample table index specification for sample identification</li> </ul> </li> <li>Subsample table specification<ul> <li>Subsample table index</li> </ul> </li> </ul>"},{"location":"spec/specification/#introduction-and-motivation","title":"Introduction and motivation","text":"<p>Bioinformatics projects often start from a sample table, a spreadsheet of samples as rows with attributes of those samples in columns. For example, the some attributes may include file paths to raw data, sample annotation like organism or treatment, and other experimental details. Unfortunately, each project is usually done differently -- there is little standardization of these file formats and column names across projects. The downstream processing tools that consume the sample table typically expect specific way of formatting the table, such as requiring certain columns, expecting certain file formats, and so on. These assumptions are often inherent in the tools, but rarely explained. And even where they are explained, they tend to be unique for each tool. There is no standard way to represent metadata that spans projects and tools. This restricts the portability and reusability of annotated datasets and software that processes them.</p> <p>Portable Encapsulated Projects (PEP for short) seeks to make datasets and related software more portable and reusable. PEP does this by providing metadata standardization, metadata validation, and portability modifiers.</p>"},{"location":"spec/specification/#how-pep-improves-sample-annotation-portability","title":"How PEP improves sample annotation portability","text":"<p>PEP provides 3 features to improve portability: </p> <ol> <li>A standardized metadata structure. PEP standardizes sample metadata formats. This allows tools and pipelines to read data from different sources more easily.</li> <li>A validation framework. PEP provides formal validation schemas. This allows us to confirm that a PEP complies with a requirements for an arbitrary tool. </li> <li>Project and sample modifiers. PEP provides a powerful framework to programmatically modify sample- and project-level metadata. This allows us to systematize metadata so one input source can span multiple tools.</li> </ol>"},{"location":"spec/specification/#definitions-of-terms-and-components-of-a-pep","title":"Definitions of terms and components of a PEP","text":"<p>A PEP can be made from any collection of metadata represented in tabular form. Typically, a PEP represents a data-intensive bioinformatics project with many samples, like individuals or cell lines. The key terms are:</p> <ul> <li>Project: a collection of metadata that annotates a set of samples.</li> <li>Sample: loosely defined; a unit that can be collected into a project, usually with one or more data files.</li> <li>PEP specification: the way to organize project and sample metadata in files using a <code>yaml</code> + <code>csv</code> format.</li> <li>PEP: a project that follows the PEP specification.</li> </ul> A PEP consists of 3 components <p>The PEP specification divides metadata into components: sample metadata, which can vary by sample, and project metadata, which applies to all samples. These components are stored in separate files. A complete PEP consists of up to 3 files:</p> <ul> <li>Project config file - RECOMMENDED. a <code>yaml</code> file containing project-level metadata</li> <li>Sample table - RECOMMENDED. a <code>csv</code> file of sample metadata, with 1 row per sample</li> <li>Subsample table - OPTIONAL. A <code>csv</code> file of sample  with multiple rows for each sample, used to specify sample attributes with multiple values (e.g. used to point to inputs in sequencing experiments when split across multiple files).</li> </ul> <p>This document describes each of these 3 files in detail.</p>"},{"location":"spec/specification/#validating-a-pep","title":"Validating a PEP","text":"<p>PEP uses an extended JSON Schema vocabulary with novel sample metadata features. The formal PEP spec is described as a schema at schema.databio.org/pep/2.1.0.yaml. You can validate a PEP against any PEP schema using the eido Python package like this:</p> <pre><code>eido validate path/to/your/PEP_config.yaml -s https://schema.databio.org/pep/2.1.0.yaml\n</code></pre> <p>The generic schema may be easily extended into a more specific schema that adds new requirements or optional attributes, requires input files, and so forth. You can find more detail about how to extend and use these schemas in the how-to guide for PEP validation.</p>"},{"location":"spec/specification/#project-config-file-specification","title":"Project config file specification","text":"<p>The project config file is the source of project-level information. It is the only required file and must be in <code>yaml</code> format. The config file includes five recognized project attributes, most being optional:</p> <ul> <li><code>pep_version</code> - REQUIRED</li> <li><code>sample_table</code>- RECOMMENDED</li> <li><code>subsample_table</code>- OPTIONAL</li> <li><code>sample_modifiers</code> - OPTIONAL</li> <li><code>project_modifiers</code> - OPTIONAL</li> </ul> <p>These attributes may appear in any order.</p> <p>Example</p> <pre><code>pep_version: 2.1.0\nsample_table: \"path/to/sample_table.csv\"\nsubsample_table: [\"path/to/subsample_table.csv\", \"path/to/subsample_table2.csv\"]\nsample_modifiers:\n  append:\n    attribute1: value\n    attr2: val2 \n  duplicate:\n    oldattr: newattr\n  imply:\n    - if:\n        genome: [\"hg18\", \"hg19\", \"hg38\"]\n      then:\n        organism: \"human\"\n  derive:\n    attributes: [read1, read2, other_attr]\n    sources:\n      key1: \"path/to/derived/value/{attribute1}\"\n      key2: \"path/to/derived/value/{attr2}\"\nproject_modifiers:\n  amend:\n    variant1:\n      sample_table: \"path/to/alternative_table.csv\"\n  import:\n    - external_pep.yaml\n    - http://url.com/pep.yaml\n</code></pre>"},{"location":"spec/specification/#project-attribute-pep_version","title":"Project attribute: <code>pep_version</code>","text":"<p>The only required project attribute, which documents the version of the PEP specification this PEP complies with. For PEP version 2.1.0, this must be the string <code>\"2.1.0\"</code>.</p>"},{"location":"spec/specification/#project-attribute-sample_table","title":"Project attribute: <code>sample_table</code>","text":"<p>The <code>sample_table</code> is a path (string) to the sample csv file. It can be absolute or relative; relative paths are assumed relative to the location of the <code>project_config.yaml</code> file. The target file is expected to comply with the PEP specification for the sample table, described later.</p>"},{"location":"spec/specification/#project-attribute-subsample_table","title":"Project attribute: <code>subsample_table</code>","text":"<p>The <code>subsample_table</code> is a path (string) to the subsample csv file or, in case the subsamples are dispersed across multiple annotation sheets, a collection of paths (array of strings). Like with the sample_table attribute, relative paths are assumed relative to the location of the <code>project_config.yaml</code> file. The target file is expected to comply with the PEP specification for the subsample table.</p>"},{"location":"spec/specification/#project-attribute-sample_modifiers","title":"Project attribute: <code>sample_modifiers</code>","text":"Sample modifiers are project settings that modify samples. <p>The sample modifiers allows you to modify sample attributes from within the project configuration file. You can use this to add new attributes to samples in a variety of ways, including attributes whose value varies depending on values of existing attributes, or whose values are composed of existing attribute values. This is a key feature of PEP that allows you to make the sample tables more portable. There are 5 subsections corresponding to 5 types of sample modifier: <code>remove</code>, <code>append</code>, <code>duplicate</code>, <code>imply</code>, and <code>derive</code>; and the samples will be modified in that order. Within each modifier, samples will be modified in the order in which the commands are listed.</p>"},{"location":"spec/specification/#sample-modifier-remove","title":"Sample modifier: remove","text":"Remove eliminates attribute from all samples. <p>The <code>remove</code> modifier elimiantes one or more sample attributes. </p> <p>Example:</p> <pre><code>sample_modifiers:\n  remove: \n    - read_type\n    - organism\n</code></pre> <p>This example eliminates <code>read_type</code> and <code>organism</code> attributes from each sample. This modifier is useful when one is in need to override an attribute with another on-the-fly. This allows that without editing the annotation sheet by hand.</p>"},{"location":"spec/specification/#sample-modifier-append","title":"Sample modifier: append","text":"Append adds a constant attribute to all samples. <p>The <code>append</code> modifier adds additional sample attributes with a constant value across all samples. </p> <p>Example:</p> <pre><code>sample_modifiers:\n  append:\n    read_type: SINGLE\n</code></pre> <p>This example adds a <code>read_type</code> attribute to each sample, with the value <code>SINGLE</code> for all samples. This modifier is useful on its own to add constant attributes, and can also be  combined with <code>derive</code> and/or <code>imply</code>.</p>"},{"location":"spec/specification/#sample-modifier-duplicate","title":"Sample modifier: duplicate","text":"Duplicate copies an attribute to a new name. <p>The <code>duplicate</code> modifier copies an existing sample attribute to a new attribute with a different name. This can be useful if you need to tweak a PEP to work under a different tool that specifies a different schema for the same data.</p> <p>Example:</p> <pre><code>sample_modifiers:\n  duplicate:\n    old_attribute_name: new_attribute_name\n</code></pre> <p>This example would copy the value of <code>old_attribute_name</code> to a new attribute called <code>new_attribute_name</code>.</p>"},{"location":"spec/specification/#sample-modifier-imply","title":"Sample modifier: imply","text":"Imply depends on other attribute values. <p>The <code>imply</code> modifier adds sample attributes with values that depends on the value of an existing attribute. Under the <code>imply</code> keyword is a list of items. Each item has an <code>if</code> section and a <code>then</code> section. The <code>if</code> section defines one or more attributes, each with one or more possible values. If all attributes listed have any of the values in the list for that attribute, then the sample passes the conditional and the implied attributes will be added. One or more attributes to imply are listed under the <code>then</code> section as <code>key: value</code> pairs.</p> <p>Example:</p> <pre><code>sample_modifiers:\n  imply:\n    - if:\n        organism: \"human\"\n      then:\n        genome: \"hg38\"\n        macs_genome_size: \"hs\"\n</code></pre> <p>This example will take any sample with <code>organism</code> attribute set to the string \"human\" and add attributes of <code>genome</code> (with value \"hg38\") and <code>macs_genome_size</code> (with value \"hs\"). This example shows only 1 implication, but you can include as many as you like.</p> <p>Implied attributes can be useful for pipeline arguments. For instance, it may be that one sample attribute implies several more. Rather than encoding these each as separate columns in the annotation sheet for a particular pipeline, you may simply indicate in the <code>project_config.yaml</code> that samples of a certain type should automatically inherit additional attributes. For more details, see how to eliminate project-level attributes from a sample table.</p>"},{"location":"spec/specification/#sample-modifier-derive","title":"Sample modifier: derive","text":"Derive builds new attributes from existing values. <p>The <code>derive</code> sample modifier converts existing sample attribute values into new values derived from other existing sample attribute values. It contains two sections; in <code>attributes</code> is a list of existing attributes that should be derived; in <code>sources</code> is a mapping of key-value pairs that defines the templates used to derive the new attribute values. The <code>sources</code> templates are available for all  entries under <code>attributes</code> .</p> <p>Example:</p> <pre><code>sample_modifiers:\n  derive:\n    attributes: [read1, read2, data_1]\n    sources:\n      key1: \"/path/to/{sample_name}_{sample_type}.bam\"\n      key2: \"/from/collaborator/weirdNamingScheme_{ext_id}.fastq\"\n      key3: \"${HOME}/{test_id}.fastq\"\n</code></pre> <p>In this example, the samples should already have attributes named <code>read1</code>, <code>read2</code>, and <code>data_1</code>, which are flagged as attributes to derive. These attribute values should originally be set to one of the keys in the <code>sources</code> section: <code>key1</code>, <code>key2</code>, or <code>key3</code>. The <code>derive</code> modifier will replace any samples set as <code>key1</code> with the specified string (<code>\"/path/to/{sample_name}_{sample_type}.bam\"</code>), but with variables like <code>{sample_name}</code> populated with the values of other sample attributes. The variables in the file paths are formatted as <code>{variable}</code>, and are populated by sample attributes (columns in the sample annotation sheet). For example, your files may be stored in <code>/path/to/{sample_name}.fastq</code>, where <code>{sample_name}</code> will be populated individually for each sample in your PEP. You can also use shell environment variables (like <code>${HOME}</code>) or wildcards (like <code>*</code>).</p> <p>Using <code>derive</code> is a powerful and flexible way to point to data files on disk. This enables you to point to more than one input file for each sample. For more details and a complete example, see how to eliminate paths from the sample table.</p>"},{"location":"spec/specification/#project-attribute-project_modifiers","title":"Project attribute: <code>project_modifiers</code>","text":"<p>The project modifiers allows you to modify project-level attributes from within the project configuration file. There are 2 subsections corresponding to 2 types of project modifier: <code>import</code> and <code>amend</code>. Imports run first, followed by amendments.</p>"},{"location":"spec/specification/#project-modifier-import","title":"Project modifier: import","text":"Imports include external PEP config files. <p>The <code>import</code> project modifier allows the config file to import other external PEP config files. The values in the imported files will be overridden by the corresponding entries in the current config file. Imports are recursive, so an imported file that imports another file is allowed; the imports are resolved in cascading order with the most distant imports happening first, so the closest configuration options override the more distant ones.</p> <p>Example:</p> <pre><code>project_modifiers:\n  import:\n    - path/to/parent_project_config.yaml\n</code></pre> <p>Imports can be used to record and manage complex analysis relationships among analysis components. In a sense, imports are the opposite of amendments, because they allow combining multiple PEP files into one. When used in combination with amendments, they make it possible to orchestrate very powerful analysis. For more information, see how to integrate imports and amendments.</p>"},{"location":"spec/specification/#project-modifier-amend","title":"Project modifier: amend","text":"Amendments specify project variations within one file. <p>The <code>amend</code> project modifier specifies multiple variations of a project within one file. When a PEP is parsed, you may select one or more included amendments, which will amend the values in the processed PEP. Unlike all other sample or project modifiers, amendments are optional and must be activated individually when the PEP is loaded.</p> <p>For example:</p> <pre><code>sample_table: annotation.csv\nproject_modifiers:\n  amend:\n    my_project2:\n      sample_table: annotation2.csv\n    my_project3:\n      sample_table: annotation3.csv\n...\n</code></pre> <p>If you load this configuration file, by default it sets <code>sample_table</code> to <code>annotation.csv</code>. If you don't activate any amendments, they are ignored. But if you choose, you may activate one of the two amendments, which are called <code>my_project2</code> and <code>my_project3</code>. If you activate <code>my_project2</code>, by passing <code>amendments=my_project2</code> when parsing the PEP, the resulting object will use the <code>annotation2.csv</code> sample_table instead of the default <code>annotation.csv</code>. All other project settings will be the same as if no amendment was activated because there are no other values specified in the <code>my_project2</code> amendment.</p> <p>Amendments are useful to define multiple similar projects within a single project config file. Under the amendments key, you specify names of amendments, and then underneath these you specify any project config variables that you want to override for that particular amendment. It is also possible to activate more than one amendment in priority order, which allows you to combine different project features on-the-fly. For more details, see how to mix and match amendments.</p>"},{"location":"spec/specification/#sample-table-specification","title":"Sample table specification","text":"<p>The <code>sample_table</code> is a <code>.csv</code> file containing information about all samples (or pieces of data) in a project. A sample table may contain any number of columns with any column names. Each column corresponds to an attribute of a sample. For this reason, we sometimes use the word <code>column</code> and <code>attribute</code> interchangeably. </p>"},{"location":"spec/specification/#sample-table-index-specification-for-sample-identification","title":"Sample table index specification for sample identification","text":"<p>Samples tables must include an identifier attribute, or index, which specifies unique strings identifying each sample. This should be a string without whitespace. By default, PEP uses <code>sample_name</code> column as the index for the sample table, but this can be changed in the project configuration. The sample table index selection priority order is:</p> <ol> <li>Value specified in Project constructor</li> <li>Value specified in Config with <code>sample_table_index</code> attribute</li> <li>Default value (<code>sample_name</code>)</li> </ol> <p>Typically, one row corresponds to one sample, so the sample_name attribute would be unique in the table; however, PEP v2.1.0 allows multiple rows per sample as a way to specify multi-value attributes. Here are some examples of both approaches: First, here is a table with one row per sample:</p> <pre><code>\"sample_name\",\"protocol\",\"organism\",\"flowcell\",\"lane\", \"data_source\"\n\"albt_0h\",\"RRBS\",\"albatross\",\"BSFX0190\",\"1\",\"bsf_sample\"\n\"albt_1h\",\"RRBS\",\"albatross\",\"BSFX0190\",\"1\",\"bsf_sample\"\n\"albt_2h\",\"RRBS\",\"albatross\",\"BSFX0190\",\"1\",\"bsf_sample\"\n\"albt_3h\",\"RRBS\",\"albatross\",\"BSFX0190\",\"1\",\"bsf_sample\"\n\"frog_0h\",\"RRBS\",\"frog\",\"\",\"\",\"frog_data\"\n\"frog_1h\",\"RRBS\",\"frog\",\"\",\"\",\"frog_data\"\n\"frog_2h\",\"RRBS\",\"frog\",\"\",\"\",\"frog_data\"\n\"frog_3h\",\"RRBS\",\"frog\",\"\",\"\",\"frog_data\"\n</code></pre> <p>In a table with duplicate values in the index column, the rows with the same identifier will be merged into a single sample, with potentially many values for other attributes. Here's an example where the <code>sample_name</code> column has a duplicated <code>albt_1h</code> value. </p> <pre><code>\"sample_name\",\"organism\",\"flowcell\",\"lane\"\n\"albt_0h\",\"albatross\",\"BSFX0190\",\"1\"\n\"albt_1h\",\"albatross\",\"BSFX0190\",\"1\"\n\"albt_1h\",\"albatross\",\"BSFX0190\",\"2\"\n\"albt_2h\",\"albatross\",\"BSFX0190\",\"1\"\n</code></pre> <p>This table has 4 rows, but the processed PEP has only 3 samples. The <code>albt_1h</code> sample has the following attributes:</p> <pre><code>organism: `albatross`\nflowcell: `BSFX0190`\nlane: [`1`, `2`]\n</code></pre> <p>A sample table with no attributes satisfies the generic PEP requirement, but it isn't really useful. Therefore, tools that use PEPs should make use of the PEP validation framework to specify further requirements. For more details, see the how-to guide for PEP validation.</p>"},{"location":"spec/specification/#subsample-table-specification","title":"Subsample table specification","text":"<p>For users who prefer to keep to one-row-per-sample, PEP can accommodate multi-value attributes with a <code>subsample_table</code>, a second <code>.csv</code> file. This approach keeps the multi-layered data structure out of the sample table, keeping it cleaner and simpler at the cost of an additional csv file. In the subsample table, multiple values for an attribute are specified as multiple rows with the same sample name. The subsample table contains an index column that maps to the index column in the sample table. This may be configured with the <code>subsample_table_index</code> value in the project configuration.</p> <p>One common use case for subsample tables is for when samples have multiple input files of the same type. For example, in a sequencing experiment, it's common to split samples across multiple sequencing lanes, which each yield a separate file. Subsample tables are one way to associate many files to a single sample attribute.</p> <p>Here's a simple example. If you define the <code>sample_table</code> like this:</p> <pre><code>sample_name,library\nfrog_1,anySampleType\nfrog_2,anySampleType\n</code></pre> <p>Then point <code>subsample_table</code> to the following, which maps <code>sample_name</code> to a new column called <code>file</code></p> <pre><code>sample_name,file\nfrog_1,data/frog1a_data.txt\nfrog_1,data/frog1b_data.txt\nfrog_1,data/frog1c_data.txt\nfrog_2,data/frog2a_data.txt\nfrog_2,data/frog2b_data.txt\n</code></pre> <p>This sets up a simple relational database that maps multiple files to each sample. You can also combine a subsample table with derived attributes; attributes will first be derived and then merged, leading to a very flexible way to point to many files of a given type for single sample.</p>"},{"location":"spec/specification/#subsample-table-index","title":"Subsample table index","text":"<p>By default, PEP uses <code>subsample_name</code> and <code>sample_name</code> columns as the indexes for the subsample table. However, it is possible to use a custom column as the sample table index, which can be specified with <code>subsample_table_index</code> attribute or on-the-fly at the project creation stage.</p> <p>This is the subsample table index selection priority order:</p> <ol> <li>Value specified in Project constructor</li> <li>Value specified in Config</li> <li>Default value (<code>subsample_name</code> and <code>sample_name</code>)</li> </ol>"},{"location":"spec/team/","title":"The PEP team","text":"<ul> <li>Nathan Sheffield, Department of Genome Sciences, University of Virginia</li> <li>Andre Rendeiro, Englander Institute for Precision Medicine, Cornell University</li> <li>Vince Reuter, University of Pennsylvania</li> <li>Michal Stolarczyk, Department of Genome Sciences, University of Virginia</li> </ul>"},{"location":"spec/team/#board-of-advisors","title":"Board of advisors","text":"<ul> <li>Christoph Bock, Center for Molecular Medicine, Vienna, Austria</li> <li>Johannes K\u00f6ster, University of Duisburg-Essen, Germany</li> </ul>"},{"location":"spec/team/#funding","title":"Funding","text":"<p>The PEP spec and toolkit are funded by the National Institutes of Health (NIH), Institute for General Medical Sciences (NIGMS), MIRA award R35GM128636 to Nathan C. Sheffield, which runs from 1-AUG-2018 to 31-JUL-2023.</p>"},{"location":"spec/team/#contributing","title":"Contributing","text":"<p>We welcome more participants! If you are interested in contributing to ongoing development of either the PEP structure format or the software toolkit, please feel free to reach out via e-mail or via raising issues or pull requests on GitHub.</p>"}]}